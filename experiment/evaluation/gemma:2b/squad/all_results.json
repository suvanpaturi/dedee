{
  "results": [
    {
      "query": "Who was Cixi's son?",
      "predicted_response": "Tongzhi",
      "method": "exact-match",
      "actual_response": "Tongzhi",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.160000778734684e-05,
        "tree_retrieval": 0.3706806730479002,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.795194375095889,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.7948391437530518,
      "score": "5"
    },
    {
      "query": "In kilometers, how long is the Staten Island Ferry route?",
      "predicted_response": "8.4",
      "method": "exact-match",
      "actual_response": "8.4",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.45009970664978e-05,
        "tree_retrieval": 0.08597874792758375,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.24306404194794595,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9597485661506653,
      "score": "5"
    },
    {
      "query": "What was the name of the miner's leader that was blamed for the strike?",
      "predicted_response": "Arthur Scargill",
      "method": "exact-match",
      "actual_response": "Arthur Scargill",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.880010288208723e-05,
        "tree_retrieval": 0.08717045199591666,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.24359004199504852,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9373068809509277,
      "score": "5"
    },
    {
      "query": "How much was allocated?",
      "predicted_response": "\u20b9 52.7 million",
      "method": "exact-match",
      "actual_response": "\u20b9 52.7 million",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9499991796910763e-05,
        "tree_retrieval": 0.08262893895152956,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.23937779199332,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0584653615951538,
      "score": "5"
    },
    {
      "query": "When did Virginia adopt The Principle of Partus Sequitur Ventrem?",
      "predicted_response": "1662",
      "method": "exact-match",
      "actual_response": "1662",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9000104404985905e-05,
        "tree_retrieval": 0.08410844299942255,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.2410400421358645,
      "bert_score": 0.9999996423721313,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8003727197647095,
      "score": "5"
    },
    {
      "query": "When did the Borders Books open up in Ann Arbor?",
      "predicted_response": "1971",
      "method": "exact-match",
      "actual_response": "1971",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0099996365606785e-05,
        "tree_retrieval": 0.09082916309125721,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.24723504204303026,
      "bert_score": 1.0000005960464478,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8586664795875549,
      "score": "5"
    },
    {
      "query": "What has the research that has been conducted for the influence of genetics made many people think?",
      "predicted_response": "that biology and environment factors play a complex role in forming it",
      "method": "exact-match",
      "actual_response": "that biology and environment factors play a complex role in forming it",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.620008308440447e-05,
        "tree_retrieval": 0.08565444801934063,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.3051822080742568,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0501559972763062,
      "score": "5"
    },
    {
      "query": "How big was the crew of a bomber?",
      "predicted_response": "four to five crewmen",
      "method": "exact-match",
      "actual_response": "four to five crewmen",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.889998722821474e-05,
        "tree_retrieval": 0.08870195702183992,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.3044887091964483,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.1010231971740723,
      "score": "5"
    },
    {
      "query": "What, typically, is the speed of the second movement in a classical piece?",
      "predicted_response": "slow",
      "method": "exact-match",
      "actual_response": "slow",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.1200040616095066e-05,
        "tree_retrieval": 0.08753285300917923,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.24819866591133177,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8838905096054077,
      "score": "5"
    },
    {
      "query": "Along with the Marianas, where do typhoons that begin in the Marshalls sometimes terminate?",
      "predicted_response": "the Philippines",
      "method": "exact-match",
      "actual_response": "the Philippines",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6700086891651154e-05,
        "tree_retrieval": 0.08783045399468392,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.24152350006625056,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9783362150192261,
      "score": "5"
    },
    {
      "query": "How much of Nigeria's male population can read?",
      "predicted_response": "75.7%",
      "method": "exact-match",
      "actual_response": "75.7%",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9499991796910763e-05,
        "tree_retrieval": 0.08891575690358877,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.24104408407583833,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0238531827926636,
      "score": "5"
    },
    {
      "query": "The true diversity within the insect species remains what?",
      "predicted_response": "uncertain",
      "method": "exact-match",
      "actual_response": "uncertain",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.719997428357601e-05,
        "tree_retrieval": 0.08891995693556964,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.24118758318945765,
      "bert_score": 1.0000008344650269,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9613901972770691,
      "score": "5"
    },
    {
      "query": "What school does not include the Mahayava scriptures in its canon?",
      "predicted_response": "the Theravada",
      "method": "exact-match",
      "actual_response": "the Theravada",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.170004442334175e-05,
        "tree_retrieval": 0.08606384799350053,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.23911862494423985,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9828547239303589,
      "score": "5"
    },
    {
      "query": "How much rain does the Sahel savannah area get per year?",
      "predicted_response": "less than 500 millimetres (20 in)",
      "method": "exact-match",
      "actual_response": "less than 500 millimetres (20 in)",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.1400006264448166e-05,
        "tree_retrieval": 0.08465184399392456,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.24183320812880993,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.1119412183761597,
      "score": "5"
    },
    {
      "query": "What are the financial districts of London known as?",
      "predicted_response": "the City of London and Canary Wharf",
      "method": "exact-match",
      "actual_response": "the City of London and Canary Wharf",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.210005186498165e-05,
        "tree_retrieval": 0.08631934900768101,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.24052370782010257,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0287798643112183,
      "score": "5"
    },
    {
      "query": "Approximately how many old masters works are included in the V&A collection?",
      "predicted_response": "2,000",
      "method": "exact-match",
      "actual_response": "2,000",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.489995677024126e-05,
        "tree_retrieval": 0.08813185407780111,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.24092158302664757,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0497397184371948,
      "score": "5"
    },
    {
      "query": "By coping the Arsenal team kit exactly, what nickname was given to Sporting Clube de Braga?",
      "predicted_response": "Os Arsenalistas",
      "method": "exact-match",
      "actual_response": "Os Arsenalistas",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.059991963207722e-05,
        "tree_retrieval": 0.08379844203591347,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.2373236669227481,
      "bert_score": 0.9999996423721313,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9606010913848877,
      "score": "5"
    },
    {
      "query": "Which year was royal assent last withheld in the UK?",
      "predicted_response": "1708",
      "method": "exact-match",
      "actual_response": "1708",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.800102811306715e-05,
        "tree_retrieval": 0.08750075194984674,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.2417622501961887,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.7942100167274475,
      "score": "5"
    },
    {
      "query": "Children who have experienced sever diarrhea are more likely to have what effect?",
      "predicted_response": "significantly lower scores on a series of tests of intelligence",
      "method": "exact-match",
      "actual_response": "significantly lower scores on a series of tests of intelligence",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.759997732937336e-05,
        "tree_retrieval": 0.086301248986274,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.3000429579988122,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0820969343185425,
      "score": "5"
    },
    {
      "query": "How is the recognized official language of Burma displayed ? ",
      "predicted_response": "It is written in a script consisting of circular and semi-circular letters",
      "method": "exact-match",
      "actual_response": "It is written in a script consisting of circular and semi-circular letters",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.03010456264019e-05,
        "tree_retrieval": 0.084832843975164,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.2985972920432687,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0656276941299438,
      "score": "5"
    },
    {
      "query": "What dis the new term, post-punk, cover?",
      "predicted_response": "groups moving beyond punk's sonic template into disparate areas",
      "method": "exact-match",
      "actual_response": "groups moving beyond punk's sonic template into disparate areas",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9399991035461426e-05,
        "tree_retrieval": 0.0856683470774442,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.23877216689288616,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0577574968338013,
      "score": "5"
    },
    {
      "query": "What did a study done by Elizabeth Loftus and John palmer show?",
      "predicted_response": "that when people were provided with misleading information they tended to misremember, a phenomenon known as the misinformation effect.",
      "method": "exact-match",
      "actual_response": "that when people were provided with misleading information they tended to misremember, a phenomenon known as the misinformation effect.",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7999980375170708e-05,
        "tree_retrieval": 0.08766915299929678,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.26027374994009733,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0310109853744507,
      "score": "5"
    },
    {
      "query": "What well-known archeologist believed the Amazon didn't have many inhabitants?",
      "predicted_response": "Betty Meggers",
      "method": "exact-match",
      "actual_response": "Betty Meggers",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.230004899203777e-05,
        "tree_retrieval": 0.08485014399047941,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.237197290873155,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9067932367324829,
      "score": "5"
    },
    {
      "query": "What are there no longer limitations on since 1990?",
      "predicted_response": "residency registration",
      "method": "exact-match",
      "actual_response": "residency registration",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7699978090822697e-05,
        "tree_retrieval": 0.08808125404175371,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.2636754580307752,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9806365966796875,
      "score": "5"
    },
    {
      "query": "What happens if the mode is activated on the fire-recall floor?",
      "predicted_response": "the elevator will have an alternate floor to recall to",
      "method": "exact-match",
      "actual_response": "the elevator will have an alternate floor to recall to",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.120000474154949e-05,
        "tree_retrieval": 0.08568374696187675,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.30167383304797113,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0705066919326782,
      "score": "5"
    },
    {
      "query": "What does the Fermat primality test depend upon?",
      "predicted_response": "np\u2261n (mod p)",
      "method": "exact-match",
      "actual_response": "np\u2261n (mod p)",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.0400965847074986e-05,
        "tree_retrieval": 0.08647104899864644,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.23915745806880295,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9960482120513916,
      "score": "5"
    },
    {
      "query": "What year was the PlayStation 3 released?",
      "predicted_response": "2006",
      "method": "exact-match",
      "actual_response": "2006",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9499991796910763e-05,
        "tree_retrieval": 0.08382824202999473,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.26292058313265443,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8033151626586914,
      "score": "5"
    },
    {
      "query": "Who is the current captian of the cricket team?",
      "predicted_response": "Rakep Patel",
      "method": "exact-match",
      "actual_response": "Rakep Patel",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8799986466765404e-05,
        "tree_retrieval": 0.08865655597764999,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.30456299986690283,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9145548343658447,
      "score": "5"
    },
    {
      "query": "Who helped with the Free Elections?",
      "predicted_response": "UN Office of Electoral Affairs",
      "method": "exact-match",
      "actual_response": "UN Office of Electoral Affairs",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0099996365606785e-05,
        "tree_retrieval": 0.08795565401669592,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.24462341680191457,
      "bert_score": 0.9999995827674866,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.051126480102539,
      "score": "5"
    },
    {
      "query": "Where is the Fylde coast?",
      "predicted_response": "Blackpool Urban Area",
      "method": "exact-match",
      "actual_response": "Blackpool Urban Area",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.4900073185563087e-05,
        "tree_retrieval": 0.08604754798579961,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.23881654092110693,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0105761289596558,
      "score": "5"
    },
    {
      "query": "Where is the Nea Moni Monastery located?",
      "predicted_response": "Chios",
      "method": "exact-match",
      "actual_response": "Chios",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9199989512562752e-05,
        "tree_retrieval": 0.08512394491117448,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.23868054198101163,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8480039238929749,
      "score": "5"
    },
    {
      "query": "What was the first carrier-launched airstrike?",
      "predicted_response": "the Tondern Raid in July 1918",
      "method": "exact-match",
      "actual_response": "the Tondern Raid in July 1918",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8199981898069382e-05,
        "tree_retrieval": 0.08310304000042379,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.23755399999208748,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0573056936264038,
      "score": "5"
    },
    {
      "query": "What is the county's payroll?",
      "predicted_response": "greater than $733 million.",
      "method": "exact-match",
      "actual_response": "greater than $733 million.",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9300106689333916e-05,
        "tree_retrieval": 0.08663905004505068,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.2530683330260217,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0997694730758667,
      "score": "5"
    },
    {
      "query": "What do forces have with regard to additive quantities?",
      "predicted_response": "magnitude and direction",
      "method": "exact-match",
      "actual_response": "magnitude and direction",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9099988751113415e-05,
        "tree_retrieval": 0.086594149004668,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.24168654112145305,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0465612411499023,
      "score": "5"
    },
    {
      "query": "What percent decrease did audio CD sales experience?",
      "predicted_response": "50%",
      "method": "exact-match",
      "actual_response": "50%",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.120000474154949e-05,
        "tree_retrieval": 0.08512474503368139,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.3262506250757724,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9902071952819824,
      "score": "5"
    },
    {
      "query": "Who was influenced by Western European and Islamic tendencies?",
      "predicted_response": "Greek masters working in Sicily",
      "method": "exact-match",
      "actual_response": "Greek masters working in Sicily",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0099996365606785e-05,
        "tree_retrieval": 0.08560724603012204,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.24162512505427003,
      "bert_score": 0.9999997019767761,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.057360053062439,
      "score": "5"
    },
    {
      "query": "Who said in 2012 that the fight would change from military to law enforcement?",
      "predicted_response": "Jeh Johnson",
      "method": "exact-match",
      "actual_response": "Jeh Johnson",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9199989512562752e-05,
        "tree_retrieval": 0.08566734695341438,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.2403317920397967,
      "bert_score": 1.0000003576278687,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9000398516654968,
      "score": "5"
    },
    {
      "query": "Which company made Spectre?",
      "predicted_response": "Eon Productions",
      "method": "exact-match",
      "actual_response": "Eon Productions",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0199997127056122e-05,
        "tree_retrieval": 0.08644734893459827,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.24203999992460012,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8703005313873291,
      "score": "5"
    },
    {
      "query": "What sports activity is featured in The Times on Mondays?",
      "predicted_response": "football",
      "method": "exact-match",
      "actual_response": "football",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0099996365606785e-05,
        "tree_retrieval": 0.08267963794060051,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.2633992920164019,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8423523902893066,
      "score": "5"
    },
    {
      "query": "What area differs from other areas in the United Kingdom regarding education?",
      "predicted_response": "Wales",
      "method": "exact-match",
      "actual_response": "Wales",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9199989512562752e-05,
        "tree_retrieval": 0.08528974500950426,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.29149979213252664,
      "bert_score": 0.9999992251396179,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8903079032897949,
      "score": "5"
    },
    {
      "query": "What type of paper is The Siasat Daily?",
      "predicted_response": "major Urdu papers",
      "method": "exact-match",
      "actual_response": "major Urdu papers",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6600086130201817e-05,
        "tree_retrieval": 0.08471914404071867,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.2392657920718193,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0010067224502563,
      "score": "5"
    },
    {
      "query": "Which Soviet countries did the RSFSR border on the west?",
      "predicted_response": "the Ukrainian, Belarusian, Estonian, Latvian and Lithuanian SSRs",
      "method": "exact-match",
      "actual_response": "the Ukrainian, Belarusian, Estonian, Latvian and Lithuanian SSRs",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.120000474154949e-05,
        "tree_retrieval": 0.08539424603804946,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.2415165000129491,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0337799787521362,
      "score": "5"
    },
    {
      "query": "This type of problem happens in any organization where the interests of the people who financially support it conflicts with what?",
      "predicted_response": "the primary purpose of the institution",
      "method": "exact-match",
      "actual_response": "the primary purpose of the institution",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.389994475990534e-05,
        "tree_retrieval": 0.11290392698720098,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.3016736248973757,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0828161239624023,
      "score": "5"
    },
    {
      "query": "What is the Windows 8 Pro price?",
      "predicted_response": "$199.99",
      "method": "exact-match",
      "actual_response": "$199.99",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.3600058890879154e-05,
        "tree_retrieval": 0.08607105002738535,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.2948815419804305,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0298289060592651,
      "score": "5"
    },
    {
      "query": "What online service did the National Archives decide to use to showcase its photographic holdings?",
      "predicted_response": "Flickr",
      "method": "exact-match",
      "actual_response": "Flickr",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.090000245720148e-05,
        "tree_retrieval": 0.08590014895889908,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.2413733338471502,
      "bert_score": 0.9999997019767761,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8906532526016235,
      "score": "5"
    },
    {
      "query": "What is a broad, modern working definition of a gene?",
      "predicted_response": "any discrete locus of heritable, genomic sequence which affect an organism's traits by being expressed as a functional product",
      "method": "exact-match",
      "actual_response": "any discrete locus of heritable, genomic sequence which affect an organism's traits by being expressed as a functional product",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.850010059773922e-05,
        "tree_retrieval": 0.08804805506952107,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.24314299994148314,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0265333652496338,
      "score": "5"
    },
    {
      "query": "Why is tort law hard to standardize?",
      "predicted_response": "immense size and diversity",
      "method": "exact-match",
      "actual_response": "immense size and diversity",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6099965907633305e-05,
        "tree_retrieval": 0.095630077063106,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.2507509170100093,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0751700401306152,
      "score": "5"
    },
    {
      "query": "What had a hold over social life in the Ottoman Empire as well as other cultures?",
      "predicted_response": "religion",
      "method": "exact-match",
      "actual_response": "religion",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.589996438473463e-05,
        "tree_retrieval": 0.08328394091222435,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.24158575013279915,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.848621129989624,
      "score": "5"
    },
    {
      "query": "Who does the evaluations for drugs in the United Kingdom?",
      "predicted_response": "European Medicines Agency",
      "method": "exact-match",
      "actual_response": "European Medicines Agency",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9099988751113415e-05,
        "tree_retrieval": 0.08681325195357203,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.2431107920128852,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0507718324661255,
      "score": "5"
    },
    {
      "query": "In what year was the God's House Tower built?",
      "predicted_response": "1417",
      "method": "exact-match",
      "actual_response": "1417",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.859998494386673e-05,
        "tree_retrieval": 0.08295374095905572,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.24698833306320012,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8043615818023682,
      "score": "5"
    },
    {
      "query": "What is a PoP?",
      "predicted_response": "point of presence",
      "method": "exact-match",
      "actual_response": "point of presence",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9099988751113415e-05,
        "tree_retrieval": 0.08388104301411659,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.23814558400772512,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0720758438110352,
      "score": "5"
    },
    {
      "query": "What tree leaves did Androsthenes describe?",
      "predicted_response": "tamarind",
      "method": "exact-match",
      "actual_response": "tamarind",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9699993319809437e-05,
        "tree_retrieval": 0.0863020500401035,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.2405126669909805,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8776722550392151,
      "score": "5"
    },
    {
      "query": "Where is the North Hills Shopping center?",
      "predicted_response": "Midtown Raleigh",
      "method": "exact-match",
      "actual_response": "Midtown Raleigh",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.120000474154949e-05,
        "tree_retrieval": 0.08293894096277654,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.23800150002352893,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0296317338943481,
      "score": "5"
    },
    {
      "query": "What does law enforcement in some countries use to profile suspects?",
      "predicted_response": "race",
      "method": "exact-match",
      "actual_response": "race",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8699985705316067e-05,
        "tree_retrieval": 0.08436434506438673,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.2776514170691371,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8122368454933167,
      "score": "5"
    },
    {
      "query": "What is it easier to do regarding God?",
      "predicted_response": "state what God is not",
      "method": "exact-match",
      "actual_response": "state what God is not",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.1800980903208256e-05,
        "tree_retrieval": 0.08346834196709096,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.2401656659785658,
      "bert_score": 0.9999997019767761,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0840314626693726,
      "score": "5"
    },
    {
      "query": " What is the second most populous of Switzerland's cantons?",
      "predicted_response": "Canton of Bern",
      "method": "exact-match",
      "actual_response": "Canton of Bern",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8501031920313835e-05,
        "tree_retrieval": 0.0850142459385097,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.2565864578355104,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.992042064666748,
      "score": "5"
    },
    {
      "query": "When did the Reconstruction Era end?",
      "predicted_response": "1877",
      "method": "exact-match",
      "actual_response": "1877",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.090000245720148e-05,
        "tree_retrieval": 0.08348384196870029,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.2372809590306133,
      "bert_score": 0.9999998211860657,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8476759195327759,
      "score": "5"
    },
    {
      "query": "808s & Heartbreak was released by what company? ",
      "predicted_response": "Island Def Jam",
      "method": "exact-match",
      "actual_response": "Island Def Jam",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.1100003980100155e-05,
        "tree_retrieval": 0.08292444108519703,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.34977225004695356,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9238510727882385,
      "score": "5"
    },
    {
      "query": "What type of stimuli causes pain?",
      "predicted_response": "intense or damaging",
      "method": "exact-match",
      "actual_response": "intense or damaging",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.130000550299883e-05,
        "tree_retrieval": 0.0852321470156312,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.29569400008767843,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0752239227294922,
      "score": "5"
    },
    {
      "query": "When did season 14 premiere?",
      "predicted_response": "January 7, 2015",
      "method": "exact-match",
      "actual_response": "January 7, 2015",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.520100239664316e-05,
        "tree_retrieval": 0.08629224996548146,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.24178208294324577,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.1207845211029053,
      "score": "5"
    },
    {
      "query": "Maududi was trained as a lawyer, but chose what professional for himself instead?",
      "predicted_response": "journalism",
      "method": "exact-match",
      "actual_response": "journalism",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.889998722821474e-05,
        "tree_retrieval": 0.08487974607851356,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.24075233307667077,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8349429965019226,
      "score": "5"
    },
    {
      "query": "Who is the professional head of the British Armed Forces?",
      "predicted_response": "Chief of the Defence Staff",
      "method": "exact-match",
      "actual_response": "Chief of the Defence Staff",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.889998722821474e-05,
        "tree_retrieval": 0.08359574305359274,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.2873411658219993,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0919629335403442,
      "score": "5"
    },
    {
      "query": "War was started with the Ottomans when?",
      "predicted_response": "17 March 1821",
      "method": "exact-match",
      "actual_response": "17 March 1821",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.130097270011902e-05,
        "tree_retrieval": 0.08500494598411024,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.23949666693806648,
      "bert_score": 1.0000005960464478,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0445047616958618,
      "score": "5"
    },
    {
      "query": "What was published the same years?",
      "predicted_response": "Annus Mirabilis Papers",
      "method": "exact-match",
      "actual_response": "Annus Mirabilis Papers",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0499999411404133e-05,
        "tree_retrieval": 0.08730965305585414,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.24230629205703735,
      "bert_score": 1.0000004768371582,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9631952047348022,
      "score": "5"
    },
    {
      "query": "What can tamers do in order to give their partners advantages?",
      "predicted_response": "slide game cards through their \"Digivices",
      "method": "exact-match",
      "actual_response": "slide game cards through their \"Digivices",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6099965907633305e-05,
        "tree_retrieval": 0.08364474202971905,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.30578408297151327,
      "bert_score": 0.9999998211860657,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0654115676879883,
      "score": "5"
    },
    {
      "query": "What does Stephen Jay Gould call philosophy that deals in the supernatural?",
      "predicted_response": "non-overlapping magisteria\" (NOMA)",
      "method": "exact-match",
      "actual_response": "non-overlapping magisteria\" (NOMA)",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8099981136620045e-05,
        "tree_retrieval": 0.08491464692633599,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.29866141616366804,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.036368727684021,
      "score": "5"
    },
    {
      "query": "The path was important for the expansion of which road?",
      "predicted_response": "El Camino Real de Tierra Adentro",
      "method": "exact-match",
      "actual_response": "El Camino Real de Tierra Adentro",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.779997445642948e-05,
        "tree_retrieval": 0.08424234495032579,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.2868310830090195,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9958230257034302,
      "score": "5"
    },
    {
      "query": "What did Burke think a social hierarchy should be based on?",
      "predicted_response": "property",
      "method": "exact-match",
      "actual_response": "property",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9799994081258774e-05,
        "tree_retrieval": 0.0829933409113437,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.2958973329514265,
      "bert_score": 1.0000003576278687,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9001156091690063,
      "score": "5"
    },
    {
      "query": "What is the term given to 122nd Street by Wizard in Taxi Driver indicating the area is majority black?",
      "predicted_response": "\"Mau Mau Land\"",
      "method": "exact-match",
      "actual_response": "\"Mau Mau Land\"",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.889998722821474e-05,
        "tree_retrieval": 0.08334714209195226,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.25777775002643466,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.011643886566162,
      "score": "5"
    },
    {
      "query": "What were the only proteins necessary to the circadian timekeeper experiment?",
      "predicted_response": "KaiA, KaiB, KaiC",
      "method": "exact-match",
      "actual_response": "KaiA, KaiB, KaiC",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9799994081258774e-05,
        "tree_retrieval": 0.0847369460389018,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.23685362515971065,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9800236821174622,
      "score": "5"
    },
    {
      "query": "How many delegates passed the motion?",
      "predicted_response": "129",
      "method": "exact-match",
      "actual_response": "129",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.029999788850546e-05,
        "tree_retrieval": 0.08613655006047338,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.2419093339703977,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8013512492179871,
      "score": "5"
    },
    {
      "query": "How are incomes distributed in Sweden?",
      "predicted_response": "more equally",
      "method": "exact-match",
      "actual_response": "more equally",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.4200063459575176e-05,
        "tree_retrieval": 0.09266246808692813,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.24453366687521338,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9076771140098572,
      "score": "5"
    },
    {
      "query": "What group declared independence as Asawad?",
      "predicted_response": "MNLA",
      "method": "exact-match",
      "actual_response": "MNLA",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.42000275850296e-05,
        "tree_retrieval": 0.08539344696328044,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.23721662512980402,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8234075307846069,
      "score": "5"
    },
    {
      "query": "How high is Granite Peak?",
      "predicted_response": "12,799 feet",
      "method": "exact-match",
      "actual_response": "12,799 feet",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.519995905458927e-05,
        "tree_retrieval": 0.08584454806987196,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.23690904187969863,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0630075931549072,
      "score": "5"
    },
    {
      "query": "What was the name given to the land that North Carolina transferred to the federal government in 1790?",
      "predicted_response": "Southwest Territory",
      "method": "exact-match",
      "actual_response": "Southwest Territory",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9399991035461426e-05,
        "tree_retrieval": 0.08445164398290217,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.30713924998417497,
      "bert_score": 0.9999998211860657,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.021000623703003,
      "score": "5"
    },
    {
      "query": "How many planes did the United States have in the Battle of Midway?",
      "predicted_response": "348",
      "method": "exact-match",
      "actual_response": "348",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.2600055672228336e-05,
        "tree_retrieval": 0.08502844593022019,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.28894762485288084,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8334561586380005,
      "score": "5"
    },
    {
      "query": "What is another name for the Eastern Roman Empire?",
      "predicted_response": "Byzantine Empire",
      "method": "exact-match",
      "actual_response": "Byzantine Empire",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9499991796910763e-05,
        "tree_retrieval": 0.08488094492349774,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.29622908285818994,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9485074281692505,
      "score": "5"
    },
    {
      "query": "What pontifax maximus favored the Catholic church? ",
      "predicted_response": "Constantine I",
      "method": "exact-match",
      "actual_response": "Constantine I",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.95999925583601e-05,
        "tree_retrieval": 0.0842563440091908,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.2647839579731226,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8883588910102844,
      "score": "5"
    },
    {
      "query": "What is a brief description of Calvinism?",
      "predicted_response": "the sovereignty or rule of God in all things",
      "method": "exact-match",
      "actual_response": "the sovereignty or rule of God in all things",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.5499961338937283e-05,
        "tree_retrieval": 0.08515474700834602,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.2377502911258489,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.083367943763733,
      "score": "5"
    },
    {
      "query": "A non-deterministic Turing machine has the ability to capture what facet of useful analysis?",
      "predicted_response": "mathematical models",
      "method": "exact-match",
      "actual_response": "mathematical models",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0099996365606785e-05,
        "tree_retrieval": 0.0986788849113509,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.2710238751024008,
      "bert_score": 0.9999996423721313,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9497608542442322,
      "score": "5"
    },
    {
      "query": "The values of the bourgeois public sphere included holding reason to the supreme, considering everything open to criticism, and the opposition of what?",
      "predicted_response": "secrecy of all sorts",
      "method": "exact-match",
      "actual_response": "secrecy of all sorts",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8199981898069382e-05,
        "tree_retrieval": 0.08392334298696369,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.30090337502770126,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0873037576675415,
      "score": "5"
    },
    {
      "query": "What provided an incentive to western empires to colonize Africa?",
      "predicted_response": "blank spaces on contemporary maps",
      "method": "exact-match",
      "actual_response": "blank spaces on contemporary maps",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0199997127056122e-05,
        "tree_retrieval": 0.0847450450528413,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.26793254213407636,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0563441514968872,
      "score": "5"
    },
    {
      "query": "What year did the Blackstone Group express interest in acquiring Dell?",
      "predicted_response": "2013",
      "method": "exact-match",
      "actual_response": "2013",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7799978852272034e-05,
        "tree_retrieval": 0.08671665098518133,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.23928158287890255,
      "bert_score": 1.0000009536743164,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8252943754196167,
      "score": "5"
    },
    {
      "query": "What was Kerry's role in the Yale Political Union as a sophomore?",
      "predicted_response": "Chairman of the Liberal Party",
      "method": "exact-match",
      "actual_response": "Chairman of the Liberal Party",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.1699928008019924e-05,
        "tree_retrieval": 0.08410014398396015,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.24200991704128683,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0633039474487305,
      "score": "5"
    },
    {
      "query": "What do bergschrunds resemble?",
      "predicted_response": "crevasses",
      "method": "exact-match",
      "actual_response": "crevasses",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.090000245720148e-05,
        "tree_retrieval": 0.08309094107244164,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.323149458039552,
      "bert_score": 1.0000004768371582,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9252892136573792,
      "score": "5"
    },
    {
      "query": "To whom did Luther send a letter containing his 95 Theses?",
      "predicted_response": "Archbishop Albrecht",
      "method": "exact-match",
      "actual_response": "Archbishop Albrecht",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9999995604157448e-05,
        "tree_retrieval": 0.08736135298386216,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.2832959999796003,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9447121024131775,
      "score": "5"
    },
    {
      "query": "In what year did Lobund at Notre Dame become an Institute?",
      "predicted_response": "1950",
      "method": "exact-match",
      "actual_response": "1950",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.880010288208723e-05,
        "tree_retrieval": 0.08516074600629508,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.23861637501977384,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.7846388816833496,
      "score": "5"
    },
    {
      "query": "What percentage of the European population consisted of monks?",
      "predicted_response": "less than one per cent",
      "method": "exact-match",
      "actual_response": "less than one per cent",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7999980375170708e-05,
        "tree_retrieval": 0.08592084900010377,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.23914208402857184,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0894526243209839,
      "score": "5"
    },
    {
      "query": "What advantage would taking St. John's have provided for the French?",
      "predicted_response": "the expedition would have strengthened France's hand at the negotiating table",
      "method": "exact-match",
      "actual_response": "the expedition would have strengthened France's hand at the negotiating table",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.190001007169485e-05,
        "tree_retrieval": 0.08737465203739703,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.24140266608446836,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0396236181259155,
      "score": "5"
    },
    {
      "query": "Where are most of these companies located in an airport?",
      "predicted_response": "within the departure areas",
      "method": "exact-match",
      "actual_response": "within the departure areas",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.789997961372137e-05,
        "tree_retrieval": 0.08412974304519594,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 0.2538849590346217,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0583711862564087,
      "score": "5"
    },
    {
      "query": "What is the earliest year the term \"redhead\" was used?",
      "predicted_response": "1510",
      "method": "exact-match",
      "actual_response": "1510",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9499991796910763e-05,
        "tree_retrieval": 4.868564186035655,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 5.0207436659839,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.7835979461669922,
      "score": "5"
    },
    {
      "query": "Which nation did Kerry travel to during her student years?",
      "predicted_response": "Switzerland",
      "method": "exact-match",
      "actual_response": "Switzerland",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.0900038331747055e-05,
        "tree_retrieval": 4.674959334894083,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.881477291928604,
      "bert_score": 0.999999463558197,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9241113662719727,
      "score": "5"
    },
    {
      "query": "Where in Greenland do glaciers advance 20-30 meters daily?",
      "predicted_response": "Jakobshavn Isbr\u00e6",
      "method": "exact-match",
      "actual_response": "Jakobshavn Isbr\u00e6",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.000104334205389e-05,
        "tree_retrieval": 4.486239898018539,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.712308082962409,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9309401512145996,
      "score": "5"
    },
    {
      "query": "What is the purpose of these statutes?",
      "predicted_response": "freedom of speech",
      "method": "exact-match",
      "actual_response": "freedom of speech",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.1100003980100155e-05,
        "tree_retrieval": 4.4873410071013495,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.673339666798711,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0812143087387085,
      "score": "5"
    },
    {
      "query": "Which places on Egypt's northern coast are popular for recreational tourism?",
      "predicted_response": "Dahab, Ras Sidr, Marsa Alam, Safaga",
      "method": "exact-match",
      "actual_response": "Dahab, Ras Sidr, Marsa Alam, Safaga",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.5999965146183968e-05,
        "tree_retrieval": 4.483416834962554,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.686360541032627,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9877368211746216,
      "score": "5"
    },
    {
      "query": "How did Whitehead suggest reality should be perceived?",
      "predicted_response": "experiential",
      "method": "exact-match",
      "actual_response": "experiential",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0199997127056122e-05,
        "tree_retrieval": 4.50390448293183,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.7098615830764174,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0307166576385498,
      "score": "5"
    },
    {
      "query": "Which organization represents all cable companies in Washington?",
      "predicted_response": "The National Cable & Telecommunications Association",
      "method": "exact-match",
      "actual_response": "The National Cable & Telecommunications Association",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.029999788850546e-05,
        "tree_retrieval": 4.437503973953426,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.593632583040744,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0473757982254028,
      "score": "5"
    },
    {
      "query": "Who stepped down from their role in 1839?",
      "predicted_response": "Melbourne",
      "method": "exact-match",
      "actual_response": "Melbourne",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.270001616328955e-05,
        "tree_retrieval": 4.450899839983322,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.690092083066702,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8353593349456787,
      "score": "5"
    },
    {
      "query": "What can be accomplished without constructing tall structures or eliminating shared spaces?",
      "predicted_response": "their approach to building design",
      "method": "exact-match",
      "actual_response": "high density",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.539995618164539e-05,
        "tree_retrieval": 4.940436878008768,
        "debate": 95.00336846895516,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 5.190211875131354,
      "bert_score": 0.8316892981529236,
      "f1_score": 0,
      "rouge_score": 0.0,
      "bleurt_score": -1.0811744928359985,
      "score": "1"
    },
    {
      "query": "What description was partially given to the minority leader?",
      "predicted_response": "The passage does not describe the minority leader.",
      "method": "debate",
      "actual_response": "He is spokesman for his party and enunciates its policies.",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.5799963623285294e-05,
        "tree_retrieval": 4.709132223972119,
        "debate": 46.00209692597855,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 50.87939175008796,
      "bert_score": 0.8602462410926819,
      "f1_score": 0,
      "rouge_score": 0.0,
      "bleurt_score": -1.3023539781570435,
      "score": "1"
    },
    {
      "query": "What position did France take as Prussia aimed to integrate various German kingdoms?",
      "predicted_response": "strongly opposed to any further alliance of German states",
      "method": "exact-match",
      "actual_response": "strongly opposed to any further alliance of German states",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.95999925583601e-05,
        "tree_retrieval": 4.779139586957172,
        "debate": 46.00209692597855,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 5.005053791915998,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0470067262649536,
      "score": "5"
    },
    {
      "query": "In which location was the Indo-Greek Kingdom situated?",
      "predicted_response": "South Asia",
      "method": "exact-match",
      "actual_response": "South Asia",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.4300068616867065e-05,
        "tree_retrieval": 4.489789442974143,
        "debate": 46.00209692597855,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.643789791036397,
      "bert_score": 0.9999995231628418,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9841454029083252,
      "score": "5"
    },
    {
      "query": "What contributed to improved fuse setting?",
      "predicted_response": "range measurement",
      "method": "exact-match",
      "actual_response": "range measurement",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.729997504502535e-05,
        "tree_retrieval": 4.504222186980769,
        "debate": 46.00209692597855,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.658095583086833,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0036875009536743,
      "score": "5"
    },
    {
      "query": "Which technique is employed to evaluate software performance under a particular load?",
      "predicted_response": "Load testing",
      "method": "exact-match",
      "actual_response": "Load testing",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8499984182417393e-05,
        "tree_retrieval": 4.458755358005874,
        "debate": 46.00209692597855,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.681266207946464,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9525935649871826,
      "score": "5"
    },
    {
      "query": "What is the global volume of growing stock forest in cubic meters?",
      "predicted_response": "434 billion",
      "method": "exact-match",
      "actual_response": "434 billion",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.740001168102026e-05,
        "tree_retrieval": 4.456792550976388,
        "debate": 46.00209692597855,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.691520916996524,
      "bert_score": 0.9999998211860657,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9508146047592163,
      "score": "5"
    },
    {
      "query": "What was the original Quantum theory?",
      "predicted_response": "Quantum Theory, developed in late 19th to early 20th centuries, was based on discrete particles called quanta. It predated modern quantum mechanics and was eventually replaced by quantum mechanics in 1925. This new framework laid the foundation for modern physics, having emerged as a complete and self-consistent theory.",
      "method": "debate",
      "actual_response": "a collection of results which predate modern quantum mechanics, but were never complete or self-consistent.",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.070000093430281e-05,
        "tree_retrieval": 4.881512484047562,
        "debate": 104.75742579903454,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 109.85782158304937,
      "bert_score": 0.8822678327560425,
      "f1_score": 0.16666666666666666,
      "rouge_score": 0.1846153846153846,
      "bleurt_score": -0.1372816115617752,
      "score": "4"
    },
    {
      "query": "Who currently holds the position of administrator on Norfolk Island?",
      "predicted_response": "Gary Hardgrave",
      "method": "exact-match",
      "actual_response": "Gary Hardgrave",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.1100003980100155e-05,
        "tree_retrieval": 4.884836283978075,
        "debate": 104.75742579903454,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 5.106574124889448,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9021804928779602,
      "score": "5"
    },
    {
      "query": "Which yearly sports event includes the Wanamaker Mile?",
      "predicted_response": "Millrose Games",
      "method": "exact-match",
      "actual_response": "Millrose Games",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.1800009310245514e-05,
        "tree_retrieval": 4.502091771923006,
        "debate": 104.75742579903454,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.68514029099606,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8905800580978394,
      "score": "5"
    },
    {
      "query": "Which significant battle took place in 731?",
      "predicted_response": "Battle of the Defile",
      "method": "exact-match",
      "actual_response": "Battle of the Defile",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9399991035461426e-05,
        "tree_retrieval": 4.563029046985321,
        "debate": 104.75742579903454,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.796270666876808,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0492271184921265,
      "score": "5"
    },
    {
      "query": "Which device appeared ideal for Link's arrow shooting skill?",
      "predicted_response": "Wii Remote",
      "method": "exact-match",
      "actual_response": "Wii Remote",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.200001083314419e-05,
        "tree_retrieval": 4.587063016020693,
        "debate": 104.75742579903454,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.755815000040457,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9552720189094543,
      "score": "5"
    },
    {
      "query": "In addition to Reims, which city in France is home to a famous Gothic cathedral?",
      "predicted_response": "Chartres",
      "method": "exact-match",
      "actual_response": "Chartres",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6700086891651154e-05,
        "tree_retrieval": 4.530953552923165,
        "debate": 104.75742579903454,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.72341729211621,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.915695071220398,
      "score": "5"
    },
    {
      "query": "Which state contributed land to Washington, D.C., but later had it returned?",
      "predicted_response": "Virginia",
      "method": "exact-match",
      "actual_response": "Virginia",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0800001695752144e-05,
        "tree_retrieval": 4.446051708073355,
        "debate": 104.75742579903454,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.598639457952231,
      "bert_score": 0.9999990463256836,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8640045523643494,
      "score": "5"
    },
    {
      "query": "Why did Sting and other artists call off their planned shows in Kazakhstan?",
      "predicted_response": "human rights concerns",
      "method": "exact-match",
      "actual_response": "human rights concerns",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.5999965146183968e-05,
        "tree_retrieval": 4.408709194976836,
        "debate": 104.75742579903454,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.585736165987328,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0685172080993652,
      "score": "5"
    },
    {
      "query": "When did Britain complete repayment of the US loan?",
      "predicted_response": "2006",
      "method": "exact-match",
      "actual_response": "2006",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7700094506144524e-05,
        "tree_retrieval": 4.747004071017727,
        "debate": 104.75742579903454,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.98938799998723,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8033151626586914,
      "score": "5"
    },
    {
      "query": "What information should users know when encoding music to prevent testing each track individually?",
      "predicted_response": "quality setting",
      "method": "exact-match",
      "actual_response": "quality setting",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9999995604157448e-05,
        "tree_retrieval": 4.530669539002702,
        "debate": 104.75742579903454,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.68423887505196,
      "bert_score": 0.9999995231628418,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0486217737197876,
      "score": "5"
    },
    {
      "query": "What is the full form of CRA?",
      "predicted_response": "Community Reinvestment Act",
      "method": "exact-match",
      "actual_response": "Community Reinvestment Act",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.150109063833952e-05,
        "tree_retrieval": 4.4982688430463895,
        "debate": 104.75742579903454,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.707253749947995,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0376451015472412,
      "score": "5"
    },
    {
      "query": "Which annelid species differ significantly from the rest?",
      "predicted_response": "Leeches have a complex life cycle with metamorphosis, multiple skin shedding, unique segmentation for muscular movement, and distinct body parts: the anterior proboscis is dedicated to feeding, while the posterior cloaca handles reproduction and waste removal. These features set them apart from other annelids.",
      "method": "debate",
      "actual_response": "leeches and their closest relatives",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.120000474154949e-05,
        "tree_retrieval": 4.50606006802991,
        "debate": 86.78983864397742,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 91.5304845420178,
      "bert_score": 0.8334688544273376,
      "f1_score": 0.08695652173913045,
      "rouge_score": 0.0816326530612245,
      "bleurt_score": -0.3362586796283722,
      "score": "4"
    },
    {
      "query": "Who was the owner of the building at 29 East 32nd Street from 1890 to 1917?",
      "predicted_response": "Grolier Club",
      "method": "exact-match",
      "actual_response": "Grolier Club",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.320006024092436e-05,
        "tree_retrieval": 4.811062708031386,
        "debate": 86.78983864397742,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.962997708935291,
      "bert_score": 1.0000005960464478,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9219409227371216,
      "score": "5"
    },
    {
      "query": "What additions can Parliament make to the citizen-proposed amendment?",
      "predicted_response": "a counter-proposal",
      "method": "exact-match",
      "actual_response": "a counter-proposal",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.270005203783512e-05,
        "tree_retrieval": 4.467248675064184,
        "debate": 86.78983864397742,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.6350957080721855,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0997300148010254,
      "score": "5"
    },
    {
      "query": "What purposes have various technologies been developed to serve, as suggested by the phrase \"There's an app for that\"?",
      "predicted_response": "assist hunters",
      "method": "exact-match",
      "actual_response": "assist hunters",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.989999484270811e-05,
        "tree_retrieval": 4.544823796022683,
        "debate": 86.78983864397742,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.696089333854616,
      "bert_score": 0.9999996423721313,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.1005691289901733,
      "score": "5"
    },
    {
      "query": "How many cities in India have been chosen for smart city development?",
      "predicted_response": "hundred",
      "method": "exact-match",
      "actual_response": "hundred",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.929999027401209e-05,
        "tree_retrieval": 4.661390584078617,
        "debate": 86.78983864397742,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.881317374994978,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9251022338867188,
      "score": "5"
    },
    {
      "query": "How is Liaoning categorized?",
      "predicted_response": "a training ship",
      "method": "exact-match",
      "actual_response": "a training ship",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.100000321865082e-05,
        "tree_retrieval": 4.53936326794792,
        "debate": 86.78983864397742,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.694312374806032,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0531569719314575,
      "score": "5"
    },
    {
      "query": "How many proteins typically make up a flagellum?",
      "predicted_response": "20 proteins",
      "method": "exact-match",
      "actual_response": "20 proteins",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.1600939109921455e-05,
        "tree_retrieval": 4.537700413959101,
        "debate": 86.78983864397742,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.694006999954581,
      "bert_score": 1.0000005960464478,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9217514991760254,
      "score": "5"
    },
    {
      "query": "What are Universities of Technology called in French-speaking regions?",
      "predicted_response": "Instituts de technologie",
      "method": "exact-match",
      "actual_response": "Instituts de technologie",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.209993105381727e-05,
        "tree_retrieval": 4.591943945037201,
        "debate": 86.78983864397742,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.77498404099606,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.006797432899475,
      "score": "5"
    },
    {
      "query": "Who among Alexander's commanders was the first to reach Bahrain?",
      "predicted_response": "Nearchus",
      "method": "exact-match",
      "actual_response": "Nearchus",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0399998649954796e-05,
        "tree_retrieval": 4.664277281030081,
        "debate": 86.78983864397742,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.918535125209019,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8017319440841675,
      "score": "5"
    },
    {
      "query": "Which under-construction building will exceed the height of Comcast Center?",
      "predicted_response": "Comcast Innovation and Technology Center",
      "method": "exact-match",
      "actual_response": "Comcast Innovation and Technology Center",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.100000321865082e-05,
        "tree_retrieval": 4.61163037607912,
        "debate": 86.78983864397742,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.765235791914165,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9811744689941406,
      "score": "5"
    },
    {
      "query": "How do the actions of the God in Dvaita differ from those in Advaita?",
      "predicted_response": "takes on a personal role",
      "method": "exact-match",
      "actual_response": "takes on a personal role",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.1100003980100155e-05,
        "tree_retrieval": 4.537263273028657,
        "debate": 86.78983864397742,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.689275834010914,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0939216613769531,
      "score": "5"
    },
    {
      "query": "Who became the Prime Minister in 1999?",
      "predicted_response": "Ehud Barak",
      "method": "exact-match",
      "actual_response": "Ehud Barak",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9799994081258774e-05,
        "tree_retrieval": 4.785524231032468,
        "debate": 86.78983864397742,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 5.027024667011574,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9082248210906982,
      "score": "5"
    },
    {
      "query": "Which UK company did Microsoft collaborate with to introduce IPTV to the 360?",
      "predicted_response": "BT",
      "method": "exact-match",
      "actual_response": "BT",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.029999788850546e-05,
        "tree_retrieval": 4.468960830941796,
        "debate": 86.78983864397742,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.647402832983062,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8395583629608154,
      "score": "5"
    },
    {
      "query": "When did someone file a Title IX complaint against Yale?",
      "predicted_response": "March 2011",
      "method": "exact-match",
      "actual_response": "March 2011",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.450002986937761e-05,
        "tree_retrieval": 4.4268799220444635,
        "debate": 86.78983864397742,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.64169712504372,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9811782836914062,
      "score": "5"
    },
    {
      "query": "What type of entity is MundoFox?",
      "predicted_response": "Spanish-language television networks",
      "method": "exact-match",
      "actual_response": "Spanish-language television networks",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0099996365606785e-05,
        "tree_retrieval": 4.503284111036919,
        "debate": 86.78983864397742,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.683035250054672,
      "bert_score": 0.9999998211860657,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0543640851974487,
      "score": "5"
    },
    {
      "query": "What is next to the abbey?",
      "predicted_response": "The Palace of Westminster is beside the abbey.",
      "method": "debate",
      "actual_response": "Palace of Westminster",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0800001695752144e-05,
        "tree_retrieval": 4.593105848995037,
        "debate": 61.79239050007891,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 66.64595729205757,
      "bert_score": 0.8923330903053284,
      "f1_score": 0.6666666666666666,
      "rouge_score": 0.5454545454545454,
      "bleurt_score": -0.05448929965496063,
      "score": "5"
    },
    {
      "query": "Who manufactured the Twin Famicom and the 19SC111 TV?",
      "predicted_response": "Sharp Corporation",
      "method": "exact-match",
      "actual_response": "Sharp Corporation",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.7299970649182796e-05,
        "tree_retrieval": 4.826240884955041,
        "debate": 61.79239050007891,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 5.00530670909211,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9227376580238342,
      "score": "5"
    },
    {
      "query": "What is an agreement made between multiple nations called?",
      "predicted_response": "A multilateral treaty is an agreement between several countries, binding and regulatory in nature, concerning specified issues or cooperation areas.",
      "method": "debate",
      "actual_response": "A multilateral treaty",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6899971999228e-05,
        "tree_retrieval": 4.463229871005751,
        "debate": 60.68313694105018,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 65.29987716698088,
      "bert_score": 0.8721232414245605,
      "f1_score": 0.19999999999999998,
      "rouge_score": 0.2608695652173913,
      "bleurt_score": 0.38891181349754333,
      "score": "5"
    },
    {
      "query": "In what year did the Gallipoli Campaign occur?",
      "predicted_response": "1915",
      "method": "exact-match",
      "actual_response": "1915",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.4999957531690598e-05,
        "tree_retrieval": 5.04962437809445,
        "debate": 60.68313694105018,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 5.3028610839974135,
      "bert_score": 1.0000003576278687,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8537719249725342,
      "score": "5"
    },
    {
      "query": "What is Alsace's historical flag called?",
      "predicted_response": "Rot-un-Wiss",
      "method": "exact-match",
      "actual_response": "Rot-un-Wiss",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.270001616328955e-05,
        "tree_retrieval": 4.465988326002844,
        "debate": 60.68313694105018,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.691888582892716,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0118333101272583,
      "score": "5"
    },
    {
      "query": "What is the name of Israel's legislative body?",
      "predicted_response": "the Knesset",
      "method": "exact-match",
      "actual_response": "the Knesset",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.1599967516958714e-05,
        "tree_retrieval": 4.646498712012544,
        "debate": 60.68313694105018,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.900296124862507,
      "bert_score": 0.9999998211860657,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9735682010650635,
      "score": "5"
    },
    {
      "query": "What were the nomadic tribes called that hunted bison?",
      "predicted_response": "Jornado",
      "method": "exact-match",
      "actual_response": "Jornado",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.230106085538864e-05,
        "tree_retrieval": 4.559566744952463,
        "debate": 60.68313694105018,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.796988666988909,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8818092942237854,
      "score": "5"
    },
    {
      "query": "In which scientific discipline did Wohler make a discovery?",
      "predicted_response": "organic chemistry",
      "method": "exact-match",
      "actual_response": "organic chemistry",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.120000474154949e-05,
        "tree_retrieval": 4.516137114027515,
        "debate": 60.68313694105018,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.687461332883686,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9222978949546814,
      "score": "5"
    },
    {
      "query": "What is Oklahoma's ranking in crude oil reserves compared to other states?",
      "predicted_response": "fifth",
      "method": "exact-match",
      "actual_response": "fifth",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.95999925583601e-05,
        "tree_retrieval": 4.536231044912711,
        "debate": 60.68313694105018,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.706496499944478,
      "bert_score": 0.9999997019767761,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9879858493804932,
      "score": "5"
    },
    {
      "query": "During which era did Queen Victoria rule?",
      "predicted_response": "Queen Victoria ruled from 1837 to 1901 during the Victorian era.",
      "method": "debate",
      "actual_response": "1837 to January 1901",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.450096119195223e-05,
        "tree_retrieval": 4.522071153041907,
        "debate": 60.80260740406811,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 65.49528666702099,
      "bert_score": 0.8643962144851685,
      "f1_score": 0.4285714285714285,
      "rouge_score": 0.39999999999999997,
      "bleurt_score": -0.48123979568481445,
      "score": "5"
    },
    {
      "query": "From where did many British colonists migrate to Florida?",
      "predicted_response": "coming from South Carolina, Georgia and England though there was also a group of settlers who came from the colony of Bermuda",
      "method": "exact-match",
      "actual_response": "coming from South Carolina, Georgia and England though there was also a group of settlers who came from the colony of Bermuda",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0199997127056122e-05,
        "tree_retrieval": 5.0985777960158885,
        "debate": 60.80260740406811,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 5.291801250074059,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.019597053527832,
      "score": "5"
    },
    {
      "query": "When was an investigation conducted regarding school casualties?",
      "predicted_response": "December 2008",
      "method": "exact-match",
      "actual_response": "December 2008",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8101028874516487e-05,
        "tree_retrieval": 4.628247679909691,
        "debate": 60.80260740406811,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.882691166829318,
      "bert_score": 1.0000007152557373,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9750790596008301,
      "score": "5"
    },
    {
      "query": "What term describes the competition among individuals of the more numerous sex for mating opportunities?",
      "predicted_response": "intrasexual competition",
      "method": "exact-match",
      "actual_response": "intrasexual competition",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.18999295309186e-05,
        "tree_retrieval": 4.613271125010215,
        "debate": 60.80260740406811,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.768366416916251,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9318580031394958,
      "score": "5"
    },
    {
      "query": "What are the smaller streams used for?",
      "predicted_response": "draining the surrounding land",
      "method": "exact-match",
      "actual_response": "draining the surrounding land",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.220097955316305e-05,
        "tree_retrieval": 4.501131437020376,
        "debate": 60.80260740406811,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.708008375018835,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0916777849197388,
      "score": "5"
    },
    {
      "query": "What is the length of Catalina Highway in miles?",
      "predicted_response": "25",
      "method": "exact-match",
      "actual_response": "25",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0099996365606785e-05,
        "tree_retrieval": 4.483781300019473,
        "debate": 60.80260740406811,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.635686625028029,
      "bert_score": 0.999999463558197,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8044775724411011,
      "score": "5"
    },
    {
      "query": "Why are consulting pharmacists more frequently engaging directly with patients?",
      "predicted_response": "because many elderly people are now taking numerous medications but continue to live outside of institutional settings",
      "method": "exact-match",
      "actual_response": "because many elderly people are now taking numerous medications but continue to live outside of institutional settings",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.160000778734684e-05,
        "tree_retrieval": 4.4292362639680505,
        "debate": 60.80260740406811,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.58818474994041,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.023844838142395,
      "score": "5"
    },
    {
      "query": "What did Romans prioritize over grave offerings in later times?",
      "predicted_response": "monumental endowments",
      "method": "exact-match",
      "actual_response": "monumental endowments",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9999995604157448e-05,
        "tree_retrieval": 4.480262598022819,
        "debate": 60.80260740406811,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.6517272079363465,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0025385618209839,
      "score": "5"
    },
    {
      "query": "Which song did Madonna face criticism for performing at the VMA?",
      "predicted_response": "\"Like a Virgin\"",
      "method": "exact-match",
      "actual_response": "\"Like a Virgin\"",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8199981898069382e-05,
        "tree_retrieval": 4.4787667320342734,
        "debate": 60.80260740406811,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.693763915915042,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0594892501831055,
      "score": "5"
    },
    {
      "query": "Which ores are most commonly used commercially?",
      "predicted_response": "Commercial ores predominantly consist of sulfides, namely copper, zinc, lead, and silver. They serve various industries, including mining, construction, electronics, and photography.",
      "method": "debate",
      "actual_response": "sulfides",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 9.630108252167702e-05,
        "tree_retrieval": 4.752662956016138,
        "debate": 63.210218574036844,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 68.17177770892158,
      "bert_score": 0.7865917682647705,
      "f1_score": 0.08695652173913045,
      "rouge_score": 0.08695652173913045,
      "bleurt_score": -0.48247000575065613,
      "score": "4"
    },
    {
      "query": "How would you define solar energy?",
      "predicted_response": "radiant light and heat from the Sun",
      "method": "exact-match",
      "actual_response": "radiant light and heat from the Sun",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.840091474354267e-05,
        "tree_retrieval": 4.944850751082413,
        "debate": 63.210218574036844,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 5.108058916172013,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0563148260116577,
      "score": "5"
    },
    {
      "query": "Who was the first king of the Sumerian dynasty?",
      "predicted_response": "Etana",
      "method": "exact-match",
      "actual_response": "Etana",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.1400006264448166e-05,
        "tree_retrieval": 4.536838018917479,
        "debate": 63.210218574036844,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.688488709041849,
      "bert_score": 0.9999994039535522,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.7926846742630005,
      "score": "5"
    },
    {
      "query": "What kind of heritage is associated with Tyneside?",
      "predicted_response": "shipbuilding",
      "method": "exact-match",
      "actual_response": "shipbuilding",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7099973522126675e-05,
        "tree_retrieval": 4.627743941033259,
        "debate": 63.210218574036844,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.790427458006889,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9190163612365723,
      "score": "5"
    },
    {
      "query": "How did Gautama dedicate his life following his enlightenment?",
      "predicted_response": "he spent the rest of his life teaching the path of awakening he had discovered",
      "method": "exact-match",
      "actual_response": "he spent the rest of his life teaching the path of awakening he had discovered",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.21000474691391e-05,
        "tree_retrieval": 4.6367987770354375,
        "debate": 63.210218574036844,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.884760957909748,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0260281562805176,
      "score": "5"
    },
    {
      "query": "During which era, alongside the Renaissance, were the Middle Ages criticized?",
      "predicted_response": "Enlightenment",
      "method": "exact-match",
      "actual_response": "Enlightenment",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.130000550299883e-05,
        "tree_retrieval": 4.506546683958732,
        "debate": 63.210218574036844,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.687707624863833,
      "bert_score": 0.9999997019767761,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.892325758934021,
      "score": "5"
    },
    {
      "query": "Who made up the bushi class?",
      "predicted_response": "ancient Japanese soldiers from traditional warrior families",
      "method": "exact-match",
      "actual_response": "ancient Japanese soldiers from traditional warrior families",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.060000017285347e-05,
        "tree_retrieval": 4.52905438200105,
        "debate": 63.210218574036844,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.6919246672187,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0583592653274536,
      "score": "5"
    },
    {
      "query": "What primarily influences the state's climate?",
      "predicted_response": "elevation of the terrain",
      "method": "exact-match",
      "actual_response": "elevation of the terrain",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0399998649954796e-05,
        "tree_retrieval": 4.726216317969374,
        "debate": 63.210218574036844,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.894033709075302,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0602728128433228,
      "score": "5"
    },
    {
      "query": "In which location can Al-Rashid Islamic Institute be found?",
      "predicted_response": "Cornwall, Ontario",
      "method": "exact-match",
      "actual_response": "Cornwall, Ontario",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.270106390118599e-05,
        "tree_retrieval": 4.490504031069577,
        "debate": 63.210218574036844,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.693361917044967,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9789361953735352,
      "score": "5"
    },
    {
      "query": "Who stripped Japan's Buddhist monks of their weapons?",
      "predicted_response": "Oda Nobunaga",
      "method": "exact-match",
      "actual_response": "Oda Nobunaga",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.240004975348711e-05,
        "tree_retrieval": 4.451317413942888,
        "debate": 63.210218574036844,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.623529750155285,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.903240442276001,
      "score": "5"
    },
    {
      "query": "What sports facility was established in 1952?",
      "predicted_response": "Wutaishan Sports Center",
      "method": "exact-match",
      "actual_response": "Wutaishan Sports Center",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8499984182417393e-05,
        "tree_retrieval": 4.563382978900336,
        "debate": 63.210218574036844,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.729924667160958,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.960528552532196,
      "score": "5"
    },
    {
      "query": "Who were the Normans' allies in Anatolia?",
      "predicted_response": "Turkish forces",
      "method": "exact-match",
      "actual_response": "Turkish forces",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.15000070258975e-05,
        "tree_retrieval": 4.584109704941511,
        "debate": 63.210218574036844,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 4.808187916176394,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9800757169723511,
      "score": "5"
    },
    {
      "query": "In which part of Florida is Miami situated?",
      "predicted_response": "Miami is located in southeastern Florida, within Miami-Dade County.",
      "method": "debate",
      "actual_response": "southeastern",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.519995905458927e-05,
        "tree_retrieval": 4.598304901039228,
        "debate": 66.32068328897003,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 71.14817137503996,
      "bert_score": 0.8139405846595764,
      "f1_score": 0.19999999999999998,
      "rouge_score": 0.18181818181818182,
      "bleurt_score": -0.5216273665428162,
      "score": "5"
    },
    {
      "query": "What was initially thought to be the cause of the Black Death?",
      "predicted_response": "the heavens",
      "method": "exact-match",
      "actual_response": "the heavens",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.210105933248997e-05,
        "tree_retrieval": 5.241876850952394,
        "debate": 66.32068328897003,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 5.436861958121881,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0002719163894653,
      "score": "5"
    },
    {
      "query": "Which aspect of treaties frequently requires interpretation when unclear?",
      "predicted_response": "The language",
      "method": "exact-match",
      "actual_response": "The language",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0399998649954796e-05,
        "tree_retrieval": 5.0522000750061125,
        "debate": 66.32068328897003,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 5.275231208186597,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9898022413253784,
      "score": "5"
    },
    {
      "query": "What are churches filled with monuments to?",
      "predicted_response": "monuments to the prosperous",
      "method": "exact-match",
      "actual_response": "monuments to the prosperous",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6400084607303143e-05,
        "tree_retrieval": 4.849862516974099,
        "debate": 66.32068328897003,
        "global_llm": 1.6067171780159697
      },
      "overall_latency": 5.021230584010482,
      "bert_score": 0.9999996423721313,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0703485012054443,
      "score": "5"
    },
    {
      "query": "What are yours?",
      "predicted_response": "It seems like the question is incomplete or lacks context. Could you provide more information or clarify what you are referring to?",
      "method": "global-llm",
      "actual_response": "fairs held at the shrines of Sufi saints",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0600931495428085e-05,
        "tree_retrieval": 4.839720980962738,
        "debate": 66.32068328897003,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 7.947929874993861,
      "bert_score": 0.8152301907539368,
      "f1_score": 0,
      "rouge_score": 0.06666666666666667,
      "bleurt_score": -1.2169396877288818,
      "score": "1"
    },
    {
      "query": "Who was unaware of the initial potential revenue the program could generate?",
      "predicted_response": "the public",
      "method": "exact-match",
      "actual_response": "the public",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8199981898069382e-05,
        "tree_retrieval": 4.974706535926089,
        "debate": 66.32068328897003,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 5.1961478751618415,
      "bert_score": 0.9999998211860657,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0246869325637817,
      "score": "5"
    },
    {
      "query": "What tree, other than a palm, is featured on the crest?",
      "predicted_response": "laurel",
      "method": "exact-match",
      "actual_response": "laurel",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.1100003980100155e-05,
        "tree_retrieval": 4.60866511601489,
        "debate": 66.32068328897003,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.790490792132914,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.7769014239311218,
      "score": "5"
    },
    {
      "query": "What is Mary Wollstonecraft's most famous work?",
      "predicted_response": "A Vindication of the Rights of Woman (1791)",
      "method": "exact-match",
      "actual_response": "A Vindication of the Rights of Woman (1791)",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.270001616328955e-05,
        "tree_retrieval": 4.574929396039806,
        "debate": 66.32068328897003,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.819248165935278,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.057505488395691,
      "score": "5"
    },
    {
      "query": "How does the capabilities approach view poverty?",
      "predicted_response": "capability deprivation",
      "method": "exact-match",
      "actual_response": "capability deprivation",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0099996365606785e-05,
        "tree_retrieval": 4.445811273995787,
        "debate": 66.32068328897003,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.664062374969944,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0638427734375,
      "score": "5"
    },
    {
      "query": "Which early architectural text originated in India?",
      "predicted_response": "Shilpa Shastras",
      "method": "exact-match",
      "actual_response": "Shilpa Shastras",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.410006709396839e-05,
        "tree_retrieval": 4.574159973068163,
        "debate": 66.32068328897003,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.7252239170484245,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9498510956764221,
      "score": "5"
    },
    {
      "query": "During which year did Fresno receive the highest amount of rainfall?",
      "predicted_response": "1885",
      "method": "exact-match",
      "actual_response": "1885",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9699993319809437e-05,
        "tree_retrieval": 4.91869866696652,
        "debate": 66.32068328897003,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 5.070991417160258,
      "bert_score": 1.0000003576278687,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8516998887062073,
      "score": "5"
    },
    {
      "query": "What are some of the most frequent triggers?",
      "predicted_response": "allergens, smoke (tobacco and other), air pollution, non selective beta-blockers, and sulfite-containing foods",
      "method": "exact-match",
      "actual_response": "allergens, smoke (tobacco and other), air pollution, non selective beta-blockers, and sulfite-containing foods",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0800001695752144e-05,
        "tree_retrieval": 4.724885641015135,
        "debate": 66.32068328897003,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.891467749839649,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.047251582145691,
      "score": "5"
    },
    {
      "query": "What is the duration of the rebirth cycle mentioned in the Vedas?",
      "predicted_response": "4,320,000 years",
      "method": "exact-match",
      "actual_response": "4,320,000 years",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.23000131174922e-05,
        "tree_retrieval": 4.443334311945364,
        "debate": 66.32068328897003,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.683069082908332,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0581315755844116,
      "score": "5"
    },
    {
      "query": "Which transformation maps a point in the square to its corresponding point under symmetry?",
      "predicted_response": "functions",
      "method": "exact-match",
      "actual_response": "functions",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.3900065571069717e-05,
        "tree_retrieval": 4.472907440038398,
        "debate": 66.32068328897003,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.626720583066344,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9022408723831177,
      "score": "5"
    },
    {
      "query": "What contributed to spreading Greek culture beyond its borders?",
      "predicted_response": "Greek culture spread through military conquests led by Alexander the Great and trading/colonizing activities of Greek tribes & city-states. Alexander's empire introduced Greek ideas, traditions, & language to diverse populations, while Greek networks & colonies shared their culture with new settlements, exerting significant influence.",
      "method": "debate",
      "actual_response": "the trading and colonizing activities of the Greek tribes and city states spread the Greek culture, religion and language",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9699993319809437e-05,
        "tree_retrieval": 4.397369347978383,
        "debate": 84.47757902694866,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 89.12547645904124,
      "bert_score": 0.8985863327980042,
      "f1_score": 0.3214285714285714,
      "rouge_score": 0.3492063492063492,
      "bleurt_score": 0.1482611447572708,
      "score": "5"
    },
    {
      "query": "How is man consistently perceived?",
      "predicted_response": "Man is perceived through distinctive qualities like gender, race, and socio-economic status, influenced by historical and cultural contexts. Perceptions are affected by cognitive processes and mental models, shaped by individual factors such as personality, values, beliefs, and life experiences.",
      "method": "debate",
      "actual_response": "an image of God",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.300001844763756e-05,
        "tree_retrieval": 4.947723172954284,
        "debate": 98.98426546598785,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 104.12306537502445,
      "bert_score": 0.8126574158668518,
      "f1_score": 0,
      "rouge_score": 0.0,
      "bleurt_score": -0.9344009757041931,
      "score": "1"
    },
    {
      "query": "In what ways do genetic alterations impact plants?",
      "predicted_response": "express themselves\") differently",
      "method": "exact-match",
      "actual_response": "express themselves\") differently",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.3099943064153194e-05,
        "tree_retrieval": 4.865321927005425,
        "debate": 98.98426546598785,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 5.095785625046119,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0786851644515991,
      "score": "5"
    },
    {
      "query": "What themes are commonly explored in a tragedy?",
      "predicted_response": "historical or mythological themes",
      "method": "exact-match",
      "actual_response": "historical or mythological themes",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.2400013878941536e-05,
        "tree_retrieval": 4.689374328008853,
        "debate": 98.98426546598785,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.890040333149955,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0338842868804932,
      "score": "5"
    },
    {
      "query": "How many Oscars did 'Jaws' receive?",
      "predicted_response": "Jaws won 3 Academy Awards: Best Picture, Best Director, and Best Sound.",
      "method": "debate",
      "actual_response": "three",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7499976567924023e-05,
        "tree_retrieval": 4.464516825042665,
        "debate": 59.678572301985696,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 64.296731374925,
      "bert_score": 0.8022106289863586,
      "f1_score": 0,
      "rouge_score": 0.0,
      "bleurt_score": -0.8656048774719238,
      "score": "3"
    },
    {
      "query": "In which location in Oklahoma can you find a notable Egyptian art collection?",
      "predicted_response": "Mabee-Gerrer Museum of Art in Shawnee",
      "method": "exact-match",
      "actual_response": "Mabee-Gerrer Museum of Art in Shawnee",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.989999484270811e-05,
        "tree_retrieval": 4.901096701039933,
        "debate": 59.678572301985696,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 5.084401082945988,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0167642831802368,
      "score": "5"
    },
    {
      "query": "Who provides financial support for these homeless shelters?",
      "predicted_response": "the city",
      "method": "exact-match",
      "actual_response": "the city",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.090000245720148e-05,
        "tree_retrieval": 4.544113620999269,
        "debate": 59.678572301985696,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.799423499964178,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9600675106048584,
      "score": "5"
    },
    {
      "query": "What kind of group were the \"king's friends\"?",
      "predicted_response": "neo-Tory",
      "method": "exact-match",
      "actual_response": "neo-Tory",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.1400006264448166e-05,
        "tree_retrieval": 4.698883878067136,
        "debate": 59.678572301985696,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.9734012498520315,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0007683038711548,
      "score": "5"
    },
    {
      "query": "What purpose did Luther believe God had for sending the Turks?",
      "predicted_response": "destroy the antichrist",
      "method": "exact-match",
      "actual_response": "destroy the antichrist",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.1800009310245514e-05,
        "tree_retrieval": 4.574351753923111,
        "debate": 59.678572301985696,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.784573750104755,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0476640462875366,
      "score": "5"
    },
    {
      "query": "For what purpose has pain occasionally been cited as a reason to grant terminally ill patients a right?",
      "predicted_response": "end their lives",
      "method": "exact-match",
      "actual_response": "end their lives",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.1700048819184303e-05,
        "tree_retrieval": 4.478659636923112,
        "debate": 59.678572301985696,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 5.09280841704458,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.080140471458435,
      "score": "5"
    },
    {
      "query": "How is all waste from university landscaping utilized?",
      "predicted_response": "composted",
      "method": "exact-match",
      "actual_response": "composted",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8799986466765404e-05,
        "tree_retrieval": 4.844768531038426,
        "debate": 59.678572301985696,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.997083541937172,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9960067272186279,
      "score": "5"
    },
    {
      "query": "Who was the Cubs' manager in 1969?",
      "predicted_response": "The Cubs' manager in 1969 is unknown from the given context.",
      "method": "debate",
      "actual_response": "Leo Durocher",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0399998649954796e-05,
        "tree_retrieval": 4.829028939013369,
        "debate": 68.63299233198632,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 73.6147676659748,
      "bert_score": 0.776154100894928,
      "f1_score": 0,
      "rouge_score": 0.0,
      "bleurt_score": -1.5058326721191406,
      "score": "1"
    },
    {
      "query": "In which product did zinc compounds exhibit minimal differences in absorption?",
      "predicted_response": "maize tortillas",
      "method": "exact-match",
      "actual_response": "maize tortillas",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.4400069378316402e-05,
        "tree_retrieval": 4.893835471943021,
        "debate": 68.63299233198632,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 5.064052209025249,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0125677585601807,
      "score": "5"
    },
    {
      "query": "When might the dielectric's polarization fail to keep up with the voltage?",
      "predicted_response": "driven with a time-varying voltage that changes rapidly",
      "method": "exact-match",
      "actual_response": "driven with a time-varying voltage that changes rapidly",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9999995604157448e-05,
        "tree_retrieval": 4.570278008002788,
        "debate": 68.63299233198632,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.8039532077964395,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0709419250488281,
      "score": "5"
    },
    {
      "query": "What is the current name of the country formerly known as South-West Africa?",
      "predicted_response": "Namibia",
      "method": "exact-match",
      "actual_response": "Namibia",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9501039534807205e-05,
        "tree_retrieval": 4.488382727024145,
        "debate": 68.63299233198632,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.6425105419475585,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8646789789199829,
      "score": "5"
    },
    {
      "query": "What is the frequency of XPT departures to Sydney?",
      "predicted_response": "twice a day",
      "method": "exact-match",
      "actual_response": "twice a day",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.140004213899374e-05,
        "tree_retrieval": 4.518827566993423,
        "debate": 68.63299233198632,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.697993334149942,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0704240798950195,
      "score": "5"
    },
    {
      "query": "Can antibiotics affect the effectiveness of birth control pills?",
      "predicted_response": "The majority of studies indicate antibiotics do interfere with contraceptive pills",
      "method": "exact-match",
      "actual_response": "The majority of studies indicate antibiotics do interfere with contraceptive pills",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0399998649954796e-05,
        "tree_retrieval": 4.509414555039257,
        "debate": 68.63299233198632,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.741848540958017,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0645235776901245,
      "score": "5"
    },
    {
      "query": "What is the number of propositions that constitute the separationist stance on the Establishment Clause?",
      "predicted_response": "two",
      "method": "exact-match",
      "actual_response": "two",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.070000093430281e-05,
        "tree_retrieval": 4.782323064981028,
        "debate": 68.63299233198632,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.978425499983132,
      "bert_score": 0.9999997019767761,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.7901197671890259,
      "score": "5"
    },
    {
      "query": "Which area is referred to as Jhamel?",
      "predicted_response": "Jhamsikhel",
      "method": "exact-match",
      "actual_response": "Jhamsikhel",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9999995604157448e-05,
        "tree_retrieval": 4.5505063839955255,
        "debate": 68.63299233198632,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.741198250092566,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8741967678070068,
      "score": "5"
    },
    {
      "query": "What is the number of Methodist churches in Plymouth?",
      "predicted_response": "over twenty",
      "method": "exact-match",
      "actual_response": "over twenty",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.239993333816528e-05,
        "tree_retrieval": 4.544211582979187,
        "debate": 68.63299233198632,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.731495999963954,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0919932126998901,
      "score": "5"
    },
    {
      "query": "From where did the next group of settlers on Norfolk Island originate?",
      "predicted_response": "the Pitcairn Islands",
      "method": "exact-match",
      "actual_response": "the Pitcairn Islands",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9799994081258774e-05,
        "tree_retrieval": 4.536091057001613,
        "debate": 68.63299233198632,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.7125296669546515,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9747878313064575,
      "score": "5"
    },
    {
      "query": "Which record company signed the season fourteen winner of American Idol?",
      "predicted_response": "Big Machine Records",
      "method": "exact-match",
      "actual_response": "Big Machine Records",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.2600055672228336e-05,
        "tree_retrieval": 4.504946867004037,
        "debate": 68.63299233198632,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.669816832989454,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9588057398796082,
      "score": "5"
    },
    {
      "query": "Who governed Egypt during the Suez Crisis?",
      "predicted_response": "Gamal Abdel Nasser",
      "method": "exact-match",
      "actual_response": "Gamal Abdel Nasser",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.5900080800056458e-05,
        "tree_retrieval": 4.512493176036514,
        "debate": 68.63299233198632,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.684407917084172,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9351990222930908,
      "score": "5"
    },
    {
      "query": "Which Doctor was initially called \"his secret\"?",
      "predicted_response": "the War Doctor",
      "method": "exact-match",
      "actual_response": "the War Doctor",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0099996365606785e-05,
        "tree_retrieval": 4.472157096024603,
        "debate": 68.63299233198632,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.704960542032495,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0153037309646606,
      "score": "5"
    },
    {
      "query": "When did the last Greek resistance occur?",
      "predicted_response": "88 BC",
      "method": "debate",
      "actual_response": "88 BC",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.100000321865082e-05,
        "tree_retrieval": 4.782504918053746,
        "debate": 56.37904087698553,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 61.40208279201761,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8785771131515503,
      "score": "5"
    },
    {
      "query": "What type of district does the San Bernardino - Riverside area have?",
      "predicted_response": "business",
      "method": "exact-match",
      "actual_response": "business",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.449994932860136e-05,
        "tree_retrieval": 4.850454654078931,
        "debate": 56.37904087698553,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 5.0052550421096385,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8863682746887207,
      "score": "5"
    },
    {
      "query": "Who are the heads of each house tasked with updating their group on the governor-general's actions?",
      "predicted_response": "President of the Senate and the Speaker of the House of Representatives",
      "method": "exact-match",
      "actual_response": "President of the Senate and the Speaker of the House of Representatives",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8399983420968056e-05,
        "tree_retrieval": 4.570404393016361,
        "debate": 56.37904087698553,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.725754375103861,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0583747625350952,
      "score": "5"
    },
    {
      "query": "After the death of which religious figure did General Dupuy disclose Napoleon's reasons for religious tolerance?",
      "predicted_response": "Pope Pius VI",
      "method": "exact-match",
      "actual_response": "Pope Pius VI",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0800001695752144e-05,
        "tree_retrieval": 4.439370378036983,
        "debate": 56.37904087698553,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.646904791938141,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9734905362129211,
      "score": "5"
    },
    {
      "query": "What appears on the Start Screen?",
      "predicted_response": "a customizable array of tiles linking to various apps and desktop programs",
      "method": "exact-match",
      "actual_response": "a customizable array of tiles linking to various apps and desktop programs",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0399998649954796e-05,
        "tree_retrieval": 4.408340872032568,
        "debate": 56.37904087698553,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.679260000120848,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0660301446914673,
      "score": "5"
    },
    {
      "query": "What did Philo's opinions on the lack of instruments conflict with?",
      "predicted_response": "Jewish scriptures",
      "method": "exact-match",
      "actual_response": "Jewish scriptures",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.1100003980100155e-05,
        "tree_retrieval": 4.464145538047887,
        "debate": 56.37904087698553,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.618895957944915,
      "bert_score": 1.0000004768371582,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9773221015930176,
      "score": "5"
    },
    {
      "query": "When was A Study of High Definition Television Systems published?",
      "predicted_response": "1979",
      "method": "exact-match",
      "actual_response": "1979",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0800001695752144e-05,
        "tree_retrieval": 4.393678304972127,
        "debate": 56.37904087698553,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.566284334054217,
      "bert_score": 1.0000004768371582,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8253446817398071,
      "score": "5"
    },
    {
      "query": "What was the intended equivalent value of one dollar?",
      "predicted_response": "a Spanish milled dollar",
      "method": "exact-match",
      "actual_response": "a Spanish milled dollar",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9699993319809437e-05,
        "tree_retrieval": 4.865957239991985,
        "debate": 56.37904087698553,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 5.0700185829773545,
      "bert_score": 0.9999996423721313,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0727546215057373,
      "score": "5"
    },
    {
      "query": "How much would disease incidence decrease with improved water and sanitation?",
      "predicted_response": "22\u201327% reduction in disease incidence",
      "method": "exact-match",
      "actual_response": "22\u201327% reduction in disease incidence",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8799986466765404e-05,
        "tree_retrieval": 4.4898716129828244,
        "debate": 56.37904087698553,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.686550709186122,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0818818807601929,
      "score": "5"
    },
    {
      "query": "What qualifications might the Government mandate for a teacher to begin teaching?",
      "predicted_response": "certification by a recognized body",
      "method": "exact-match",
      "actual_response": "certification by a recognized body",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.95999925583601e-05,
        "tree_retrieval": 4.579140966990963,
        "debate": 56.37904087698553,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.7398838750086725,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0649981498718262,
      "score": "5"
    },
    {
      "query": "By whom was Foot replaced?",
      "predicted_response": "Neil Kinnock",
      "method": "exact-match",
      "actual_response": "Neil Kinnock",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.130000550299883e-05,
        "tree_retrieval": 4.497714367927983,
        "debate": 56.37904087698553,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.64913645805791,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9083579778671265,
      "score": "5"
    },
    {
      "query": "What is the distance to Canelones' neighboring capital?",
      "predicted_response": "46 kilometres",
      "method": "exact-match",
      "actual_response": "46 kilometres",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.150097422301769e-05,
        "tree_retrieval": 4.577709817909636,
        "debate": 56.37904087698553,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.771429833956063,
      "bert_score": 0.9999997019767761,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0099753141403198,
      "score": "5"
    },
    {
      "query": "Against what was Jefferson's Statute of Religious Freedom drafted?",
      "predicted_response": "a bill",
      "method": "exact-match",
      "actual_response": "a bill",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0199997127056122e-05,
        "tree_retrieval": 4.541094052023254,
        "debate": 56.37904087698553,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.712003583088517,
      "bert_score": 1.0000005960464478,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0194571018218994,
      "score": "5"
    },
    {
      "query": "Who are the artists that influenced Madonna?",
      "predicted_response": "Frida Kahlo",
      "method": "exact-match",
      "actual_response": "Frida Kahlo",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.279990050941706e-05,
        "tree_retrieval": 4.580792730092071,
        "debate": 56.37904087698553,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.768504333915189,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9176698923110962,
      "score": "5"
    },
    {
      "query": "Which sector has the smallest workforce?",
      "predicted_response": "Industry and manufacturing",
      "method": "exact-match",
      "actual_response": "Industry and manufacturing",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.120000474154949e-05,
        "tree_retrieval": 4.763555030920543,
        "debate": 56.37904087698553,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.914313750108704,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0037364959716797,
      "score": "5"
    },
    {
      "query": "Which Gran Turismo game was unveiled in 2007 but released later?",
      "predicted_response": "Gran Turismo 5 Prologue",
      "method": "exact-match",
      "actual_response": "Gran Turismo 5 Prologue",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8600101359188557e-05,
        "tree_retrieval": 4.620314455009066,
        "debate": 56.37904087698553,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.877074416028336,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.009635090827942,
      "score": "5"
    },
    {
      "query": "During which years was the civil war fought?",
      "predicted_response": "1992 to 1997",
      "method": "exact-match",
      "actual_response": "1992 to 1997",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.2100011594593525e-05,
        "tree_retrieval": 4.540017093997449,
        "debate": 56.37904087698553,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.6978291249834,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0169957876205444,
      "score": "5"
    },
    {
      "query": "What color, besides lemon yellow, is found in uranium glass?",
      "predicted_response": "orange-red",
      "method": "exact-match",
      "actual_response": "orange-red",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.130000550299883e-05,
        "tree_retrieval": 4.536562166991644,
        "debate": 56.37904087698553,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 5.106921500060707,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0518840551376343,
      "score": "5"
    },
    {
      "query": "Which game, popular in both Britain and America, is gaining popularity in pubs?",
      "predicted_response": "pool",
      "method": "exact-match",
      "actual_response": "pool",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.099988680332899e-05,
        "tree_retrieval": 4.781508070998825,
        "debate": 56.37904087698553,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.969257875112817,
      "bert_score": 1.0000003576278687,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.7722023725509644,
      "score": "5"
    },
    {
      "query": "Which location in Miami was considered for hosting Super Bowl 50?",
      "predicted_response": "Sun Life Stadium",
      "method": "exact-match",
      "actual_response": "Sun Life Stadium",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6399968191981316e-05,
        "tree_retrieval": 4.596070157014765,
        "debate": 56.37904087698553,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.7484179579187185,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9539350271224976,
      "score": "5"
    },
    {
      "query": "What year did John pass away?",
      "predicted_response": "John's death year is unspecified in the given context.",
      "method": "debate",
      "actual_response": "18 October",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.390002530068159e-05,
        "tree_retrieval": 4.540032783057541,
        "debate": 43.77555483800825,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 48.47341595799662,
      "bert_score": 0.8729612827301025,
      "f1_score": 0,
      "rouge_score": 0.0,
      "bleurt_score": -0.9788016080856323,
      "score": "1"
    },
    {
      "query": "What did Popper claim to be objective and independent of its subject?",
      "predicted_response": "Knowledge, per Popper, is objective and independent. It's unaffected by a person's perspective or beliefs, and doesn't depend on the object being studied.",
      "method": "debate",
      "actual_response": "knowledge",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.630007945001125e-05,
        "tree_retrieval": 5.212171478080563,
        "debate": 62.12112511298619,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 67.55912616685964,
      "bert_score": 0.8033860921859741,
      "f1_score": 0.0909090909090909,
      "rouge_score": 0.07407407407407407,
      "bleurt_score": -0.25255927443504333,
      "score": "4"
    },
    {
      "query": "Who is recognized for the belief that humanity holds the utmost importance in the world?",
      "predicted_response": "Wu of Zhou",
      "method": "exact-match",
      "actual_response": "Wu of Zhou",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9799994081258774e-05,
        "tree_retrieval": 4.966126618906856,
        "debate": 62.12112511298619,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 5.178109999978915,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9416054487228394,
      "score": "5"
    },
    {
      "query": "What does ECCE mean in the context of Swazi education?",
      "predicted_response": "early childhood care and education",
      "method": "exact-match",
      "actual_response": "early childhood care and education",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.2100011594593525e-05,
        "tree_retrieval": 4.673239917960018,
        "debate": 62.12112511298619,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.88929779198952,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.029485821723938,
      "score": "5"
    },
    {
      "query": "What became unpopular during this period?",
      "predicted_response": "Political instability among Greek states resulted in regular conflicts and wars, affecting tourism by causing high hotel vacancy rates.",
      "method": "debate",
      "actual_response": "Wood-panelling",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8600101359188557e-05,
        "tree_retrieval": 4.7255682540126145,
        "debate": 77.51969580596779,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 82.53227191697806,
      "bert_score": 0.809044361114502,
      "f1_score": 0,
      "rouge_score": 0.0,
      "bleurt_score": -1.12616765499115,
      "score": "1"
    },
    {
      "query": "What type of art held significance in religious symbolism and decoration during the Ottoman Empire?",
      "predicted_response": "carpet weaving",
      "method": "exact-match",
      "actual_response": "carpet weaving",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.1400006264448166e-05,
        "tree_retrieval": 4.859995399019681,
        "debate": 77.51969580596779,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 5.0684397500008345,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9661898612976074,
      "score": "5"
    },
    {
      "query": "In what year was the International Chopin Piano Competition founded?",
      "predicted_response": "1927",
      "method": "exact-match",
      "actual_response": "1927",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.759997732937336e-05,
        "tree_retrieval": 4.568969252984971,
        "debate": 77.51969580596779,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.722884666873142,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8661757707595825,
      "score": "5"
    },
    {
      "query": "In what year did Seoul Semiconductor launch the first high DC voltage LED?",
      "predicted_response": "2009",
      "method": "exact-match",
      "actual_response": "2009",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.929999027401209e-05,
        "tree_retrieval": 4.5367923938902095,
        "debate": 77.51969580596779,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.718105542007834,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.827873945236206,
      "score": "5"
    },
    {
      "query": "What led to the formation of different Protestant branches?",
      "predicted_response": "how they have been influenced by important movements since the Reformation",
      "method": "exact-match",
      "actual_response": "how they have been influenced by important movements since the Reformation",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.070000093430281e-05,
        "tree_retrieval": 4.486900037969463,
        "debate": 77.51969580596779,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.75387275009416,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0421985387802124,
      "score": "5"
    },
    {
      "query": "What is a typical background for a Japanese wrestler?",
      "predicted_response": "legitimate martial arts",
      "method": "exact-match",
      "actual_response": "legitimate martial arts",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0399998649954796e-05,
        "tree_retrieval": 4.68001051293686,
        "debate": 77.51969580596779,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.887035542167723,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0100258588790894,
      "score": "5"
    },
    {
      "query": "What enhancement did Germany make to the 20 mm to increase its effectiveness?",
      "predicted_response": "a 3.7 cm",
      "method": "exact-match",
      "actual_response": "a 3.7 cm",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.95999925583601e-05,
        "tree_retrieval": 4.400011506979354,
        "debate": 77.51969580596779,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.580231084022671,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.1065832376480103,
      "score": "5"
    },
    {
      "query": "How many RIAA certifications has Beyonc\u00e9 received?",
      "predicted_response": "Beyonc\u00e9 holds the record for the most RIAA certifications worldwide with 64.",
      "method": "debate",
      "actual_response": "64",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9699993319809437e-05,
        "tree_retrieval": 4.515954295056872,
        "debate": 48.69324554596096,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 53.39212462492287,
      "bert_score": 0.8116400837898254,
      "f1_score": 0.18181818181818182,
      "rouge_score": 0.15384615384615385,
      "bleurt_score": -1.1351932287216187,
      "score": "4"
    },
    {
      "query": "How did Lothar Wolfgang Nordheim characterize von Neumann?",
      "predicted_response": "\"fastest mind I ever met\",",
      "method": "exact-match",
      "actual_response": "\"fastest mind I ever met\",",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9401038773357868e-05,
        "tree_retrieval": 4.767396523966454,
        "debate": 48.69324554596096,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 5.000987166073173,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0544222593307495,
      "score": "5"
    },
    {
      "query": "When was the importance of \"separation\" to the Constitution's Religion Clauses clearly stated?",
      "predicted_response": "1947",
      "method": "exact-match",
      "actual_response": "1947",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0200928449630737e-05,
        "tree_retrieval": 4.683076916029677,
        "debate": 48.69324554596096,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.834997209021822,
      "bert_score": 1.0000003576278687,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.826357364654541,
      "score": "5"
    },
    {
      "query": "How does bronchial thermoplasty impact patients?",
      "predicted_response": "it appears to decrease the subsequent rate.",
      "method": "exact-match",
      "actual_response": "it appears to decrease the subsequent rate.",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.2600051276385784e-05,
        "tree_retrieval": 4.817866608966142,
        "debate": 48.69324554596096,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 5.002917665988207,
      "bert_score": 0.9999997019767761,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0987862348556519,
      "score": "5"
    },
    {
      "query": "Randomly selected individuals from various groups might share more similarities with each other than with members of their own what?",
      "predicted_response": "cluster",
      "method": "exact-match",
      "actual_response": "cluster",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.5099958293139935e-05,
        "tree_retrieval": 4.478408745955676,
        "debate": 48.69324554596096,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.737553291022778,
      "bert_score": 0.9999998211860657,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.7862832546234131,
      "score": "5"
    },
    {
      "query": "Who led the Puritans as their religious figure during their initial settlement?",
      "predicted_response": "Reverend John Davenport was the religious leader of the Puritans during their early settlement.",
      "method": "debate",
      "actual_response": "Reverend John Davenport",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.95999925583601e-05,
        "tree_retrieval": 4.837557334918529,
        "debate": 63.97614289005287,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 68.99683591700159,
      "bert_score": 0.8862955570220947,
      "f1_score": 0.4,
      "rouge_score": 0.35294117647058826,
      "bleurt_score": 0.19762904942035675,
      "score": "5"
    },
    {
      "query": "What occurred with Sky's contract in 2006?",
      "predicted_response": "Sky's monopoly was broken from August 2006 when Setanta Sports was awarded rights to show two out of the six packages of matches available.",
      "method": "exact-match",
      "actual_response": "Sky's monopoly was broken from August 2006 when Setanta Sports was awarded rights to show two out of the six packages of matches available.",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0300929211080074e-05,
        "tree_retrieval": 4.9882135359803215,
        "debate": 63.97614289005287,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 5.201210333965719,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0342332124710083,
      "score": "5"
    },
    {
      "query": "What relates mass to space-time?",
      "predicted_response": "General relativity",
      "method": "exact-match",
      "actual_response": "General relativity",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.130000550299883e-05,
        "tree_retrieval": 4.543411373044364,
        "debate": 63.97614289005287,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.777408582856879,
      "bert_score": 0.9999995827674866,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9292874336242676,
      "score": "5"
    },
    {
      "query": "Which German coast did the RAF bomb?",
      "predicted_response": "Wilhelmshaven",
      "method": "exact-match",
      "actual_response": "Wilhelmshaven",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0099996365606785e-05,
        "tree_retrieval": 4.512019062996842,
        "debate": 63.97614289005287,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.689613166963682,
      "bert_score": 0.9999998211860657,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9161422252655029,
      "score": "5"
    },
    {
      "query": "What are the three tiers of higher education in Estonia?",
      "predicted_response": "bachelor's, master's, and doctoral studies",
      "method": "exact-match",
      "actual_response": "bachelor's, master's, and doctoral studies",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.859998494386673e-05,
        "tree_retrieval": 4.495540835079737,
        "debate": 63.97614289005287,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.778879499994218,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.071853518486023,
      "score": "5"
    },
    {
      "query": "What class does the Sao Paulo aircraft carrier belong to?",
      "predicted_response": "Clemenceau",
      "method": "exact-match",
      "actual_response": "Clemenceau",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.200001083314419e-05,
        "tree_retrieval": 4.572654473013245,
        "debate": 63.97614289005287,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.784441333031282,
      "bert_score": 0.9999998211860657,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8548162579536438,
      "score": "5"
    },
    {
      "query": "What is another name for abstract expressionism?",
      "predicted_response": "the New York School",
      "method": "exact-match",
      "actual_response": "the New York School",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.1800009310245514e-05,
        "tree_retrieval": 4.6028890640009195,
        "debate": 63.97614289005287,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.88409079099074,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9987560510635376,
      "score": "5"
    },
    {
      "query": "What was the impact of the civil war on Afghanistan's economy?",
      "predicted_response": "one of the poorest countries on earth",
      "method": "exact-match",
      "actual_response": "one of the poorest countries on earth",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.130000550299883e-05,
        "tree_retrieval": 4.942498101037927,
        "debate": 63.97614289005287,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 5.203775875037536,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0825549364089966,
      "score": "5"
    },
    {
      "query": "Who were the two other renowned artists in the Super Bowl 50 halftime show?",
      "predicted_response": "Coldplay and Beyonc\u00e9 performed at the Super Bowl 50 halftime show with unspecified additional artists.",
      "method": "debate",
      "actual_response": "Beyonc\u00e9 and Bruno Mars",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.120000474154949e-05,
        "tree_retrieval": 4.581047898042016,
        "debate": 72.49153173109517,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 77.25197829096578,
      "bert_score": 0.8721941709518433,
      "f1_score": 0.22222222222222224,
      "rouge_score": 0.10526315789473685,
      "bleurt_score": -0.4399113357067108,
      "score": "4"
    },
    {
      "query": "Who was the top pick in the 1998 NFL draft?",
      "predicted_response": "Manning",
      "method": "exact-match",
      "actual_response": "Manning",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.1400006264448166e-05,
        "tree_retrieval": 4.784569654031657,
        "debate": 72.49153173109517,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.936048582894728,
      "bert_score": 1.0000003576278687,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8551865816116333,
      "score": "5"
    },
    {
      "query": "In which year was Tem\u00fcjin, later known as Genghis Khan, probably born?",
      "predicted_response": "Temujin was born in 1162 in Del\u00fc\u00fcn Boldog, present-day Mongolia.",
      "method": "debate",
      "actual_response": "1162",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8699985705316067e-05,
        "tree_retrieval": 4.449005921953358,
        "debate": 51.55344066407997,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 56.15682008303702,
      "bert_score": 0.8010614514350891,
      "f1_score": 0.18181818181818182,
      "rouge_score": 0.15384615384615385,
      "bleurt_score": -0.9554849863052368,
      "score": "5"
    },
    {
      "query": "What is the third type of content available at Home besides free and won?",
      "predicted_response": "premium",
      "method": "exact-match",
      "actual_response": "premium",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.029999788850546e-05,
        "tree_retrieval": 4.974116647965275,
        "debate": 51.55344066407997,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 5.191257250029594,
      "bert_score": 0.9999997019767761,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9303838014602661,
      "score": "5"
    },
    {
      "query": "Who held the Arena Football League championship title previously?",
      "predicted_response": "San Jose SaberCats",
      "method": "exact-match",
      "actual_response": "San Jose SaberCats",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.610008232295513e-05,
        "tree_retrieval": 4.477641078992747,
        "debate": 51.55344066407997,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.671372249955311,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9809756278991699,
      "score": "5"
    },
    {
      "query": "To which civilization did the pottery belong?",
      "predicted_response": "Timucua",
      "method": "exact-match",
      "actual_response": "Timucua",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8999987989664078e-05,
        "tree_retrieval": 4.410912423976697,
        "debate": 51.55344066407997,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.593068375019357,
      "bert_score": 1.0000003576278687,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8536363244056702,
      "score": "5"
    },
    {
      "query": "In which neighborhood is the Marriott Starr Pass Resort & Spa located?",
      "predicted_response": "The Marriott Starr Pass Resort & Spa lies within the Starr Pass neighborhood, a part of the Greater Houston area.",
      "method": "debate",
      "actual_response": "Starr Pass",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8699985705316067e-05,
        "tree_retrieval": 4.502505616983399,
        "debate": 68.10132059594616,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 72.77462358307093,
      "bert_score": 0.824015736579895,
      "f1_score": 0.23529411764705882,
      "rouge_score": 0.1904761904761905,
      "bleurt_score": -0.687298059463501,
      "score": "4"
    },
    {
      "query": "Besides obtaining money, what is another purpose of using bribes against others?",
      "predicted_response": "harm",
      "method": "exact-match",
      "actual_response": "harm",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.1400006264448166e-05,
        "tree_retrieval": 5.078832410974428,
        "debate": 68.10132059594616,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 5.292410667054355,
      "bert_score": 0.9999998211860657,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9087929129600525,
      "score": "5"
    },
    {
      "query": "For how long was a king elected in the Roman Kingdom?",
      "predicted_response": "life term",
      "method": "exact-match",
      "actual_response": "life term",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0099996365606785e-05,
        "tree_retrieval": 4.5808008189778775,
        "debate": 68.10132059594616,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.736113625112921,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0329148769378662,
      "score": "5"
    },
    {
      "query": "Who was the prominent political leader who criticized The Times in 1944 for not backing the British Army?",
      "predicted_response": "Winston Churchill",
      "method": "exact-match",
      "actual_response": "Winston Churchill",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.170000854879618e-05,
        "tree_retrieval": 4.499290560022928,
        "debate": 68.10132059594616,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.665303207933903,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9458638429641724,
      "score": "5"
    },
    {
      "query": "In what year did The Russian Revolution occur?",
      "predicted_response": "1917",
      "method": "exact-match",
      "actual_response": "1917",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8499984182417393e-05,
        "tree_retrieval": 4.577431923011318,
        "debate": 68.10132059594616,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.754632125142962,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8707802295684814,
      "score": "5"
    },
    {
      "query": "What was the number of Chinese troops participating in the rescue efforts?",
      "predicted_response": "135,000",
      "method": "exact-match",
      "actual_response": "135,000",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9199989512562752e-05,
        "tree_retrieval": 4.544103653053753,
        "debate": 68.10132059594616,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.696215874981135,
      "bert_score": 1.0000004768371582,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.01627516746521,
      "score": "5"
    },
    {
      "query": "What is the US's total area in miles, as stated by the Encyclopedia Britannica?",
      "predicted_response": "3,676,486 sq mi",
      "method": "exact-match",
      "actual_response": "3,676,486 sq mi",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0000926926732063e-05,
        "tree_retrieval": 4.525672218063846,
        "debate": 68.10132059594616,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.774193708784878,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.1177921295166016,
      "score": "5"
    },
    {
      "query": "Where in France was Cluny Abbey established?",
      "predicted_response": "M\u00e2con",
      "method": "exact-match",
      "actual_response": "M\u00e2con",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.210105933248997e-05,
        "tree_retrieval": 4.5381263620220125,
        "debate": 68.10132059594616,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.728709582937881,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.7814981341362,
      "score": "5"
    },
    {
      "query": "What did Josel request the city of Strasbourg to ban from being sold?",
      "predicted_response": "Luther's anti-Jewish works",
      "method": "exact-match",
      "actual_response": "Luther's anti-Jewish works",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.459995448589325e-05,
        "tree_retrieval": 4.783816808951087,
        "debate": 68.10132059594616,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.9494196670129895,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0436159372329712,
      "score": "5"
    },
    {
      "query": "Who moderated the initial debate between Bush and Kerry?",
      "predicted_response": "Jim Lehrer",
      "method": "exact-match",
      "actual_response": "Jim Lehrer",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.1100003980100155e-05,
        "tree_retrieval": 4.472182577010244,
        "debate": 68.10132059594616,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.629684540908784,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9198075532913208,
      "score": "5"
    },
    {
      "query": "What is the status of the Catalan Countries?",
      "predicted_response": "no official status",
      "method": "exact-match",
      "actual_response": "no official status",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.729997504502535e-05,
        "tree_retrieval": 4.559607836068608,
        "debate": 68.10132059594616,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.742778708925471,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.027785062789917,
      "score": "5"
    },
    {
      "query": "When did the Persian Gulf War hostilities start?",
      "predicted_response": "The Persian Gulf War began in January 1991.",
      "method": "debate",
      "actual_response": "January 1991",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.970104105770588e-05,
        "tree_retrieval": 4.4424598859623075,
        "debate": 45.178247562027536,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 49.858226333977655,
      "bert_score": 0.8695801496505737,
      "f1_score": 0.4444444444444445,
      "rouge_score": 0.4,
      "bleurt_score": -0.34668514132499695,
      "score": "5"
    },
    {
      "query": "What became the student body's objective?",
      "predicted_response": "winning the big game",
      "method": "exact-match",
      "actual_response": "winning the big game",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0800001695752144e-05,
        "tree_retrieval": 4.817902815993875,
        "debate": 45.178247562027536,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.972114082891494,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0807400941848755,
      "score": "5"
    },
    {
      "query": "In what year was the Mann Act enacted?",
      "predicted_response": "June 25, 1910",
      "method": "exact-match",
      "actual_response": "June 25, 1910",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.1100003980100155e-05,
        "tree_retrieval": 4.6441584709100425,
        "debate": 45.178247562027536,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.887725542066619,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.11072838306427,
      "score": "5"
    },
    {
      "query": "What is the process for appointing judges?",
      "predicted_response": "elected by the voters",
      "method": "exact-match",
      "actual_response": "elected by the voters",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.200001083314419e-05,
        "tree_retrieval": 4.576183774974197,
        "debate": 45.178247562027536,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.786048083100468,
      "bert_score": 1.0000003576278687,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0817350149154663,
      "score": "5"
    },
    {
      "query": "Who informed Mary that she would give birth to the Messiah?",
      "predicted_response": "Gabriel",
      "method": "exact-match",
      "actual_response": "Gabriel",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.2800016924738884e-05,
        "tree_retrieval": 4.806836263043806,
        "debate": 45.178247562027536,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 5.00619195913896,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.7661246061325073,
      "score": "5"
    },
    {
      "query": "Who established groups such as the FA?",
      "predicted_response": "Former public schoolboys",
      "method": "exact-match",
      "actual_response": "Former public schoolboys",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.929999027401209e-05,
        "tree_retrieval": 4.577392714913003,
        "debate": 45.178247562027536,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.792088249931112,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0528662204742432,
      "score": "5"
    },
    {
      "query": "Which British king or queen is depicted above the frame surrounding the arches and entrance?",
      "predicted_response": "Queen Victoria",
      "method": "exact-match",
      "actual_response": "Queen Victoria",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.4099950678646564e-05,
        "tree_retrieval": 4.476622023968957,
        "debate": 45.178247562027536,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.663452083943412,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.957355260848999,
      "score": "5"
    },
    {
      "query": "What kind of philosopher was Hobbes?",
      "predicted_response": "empiricist",
      "method": "exact-match",
      "actual_response": "empiricist",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.1800009310245514e-05,
        "tree_retrieval": 4.433643845957704,
        "debate": 45.178247562027536,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.69240616611205,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9779011011123657,
      "score": "5"
    },
    {
      "query": "What is the current number of artworks housed in the National Museum?",
      "predicted_response": "200,000 artworks at the National Museum",
      "method": "debate",
      "actual_response": "200,000",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9999995604157448e-05,
        "tree_retrieval": 4.362442263984121,
        "debate": 49.47118611901533,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 54.00152958394028,
      "bert_score": 0.8812357187271118,
      "f1_score": 0.33333333333333337,
      "rouge_score": 0.4444444444444445,
      "bleurt_score": -0.25718480348587036,
      "score": "5"
    },
    {
      "query": "Where is the Holiday Festival of Lights held?",
      "predicted_response": "James Island County Park",
      "method": "exact-match",
      "actual_response": "James Island County Park",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.829998265951872e-05,
        "tree_retrieval": 5.044639036990702,
        "debate": 49.47118611901533,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 5.197185042081401,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9515003561973572,
      "score": "5"
    },
    {
      "query": "During which war was Buckingham Palace left undamaged?",
      "predicted_response": "World War I",
      "method": "exact-match",
      "actual_response": "World War I",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.95999925583601e-05,
        "tree_retrieval": 4.4650942999869585,
        "debate": 49.47118611901533,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.640297791920602,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.1058157682418823,
      "score": "5"
    },
    {
      "query": "What was IBM's 2004 operating margin?",
      "predicted_response": "16.8%",
      "method": "exact-match",
      "actual_response": "16.8%",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.95999925583601e-05,
        "tree_retrieval": 4.576980640063994,
        "debate": 49.47118611901533,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 5.298923084046692,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0093518495559692,
      "score": "5"
    },
    {
      "query": "Can you name a well-known restaurant in Little Havana?",
      "predicted_response": "Versailles",
      "method": "exact-match",
      "actual_response": "Versailles",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.810009755194187e-05,
        "tree_retrieval": 4.882030542008579,
        "debate": 49.47118611901533,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 5.035997667117044,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.7817168235778809,
      "score": "5"
    },
    {
      "query": "In which year did the Tenth Doctor feature in the Sarah Jane series?",
      "predicted_response": "2009",
      "method": "exact-match",
      "actual_response": "2009",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.2100011594593525e-05,
        "tree_retrieval": 4.609049046994187,
        "debate": 49.47118611901533,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.763223000103608,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.827873945236206,
      "score": "5"
    },
    {
      "query": "When did Blair declare his resignation?",
      "predicted_response": "September 2006",
      "method": "exact-match",
      "actual_response": "September 2006",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.1900941394269466e-05,
        "tree_retrieval": 4.510461515048519,
        "debate": 49.47118611901533,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.666402833070606,
      "bert_score": 0.9999996423721313,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9681576490402222,
      "score": "5"
    },
    {
      "query": "Who became famous for his poor rendition of \"She Bangs\" during an audition?",
      "predicted_response": "William Hung gained fame with a poor performance of \"She Bangs\" during American Idol auditions in 2004, not 2007.",
      "method": "debate",
      "actual_response": "William Hung",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0199997127056122e-05,
        "tree_retrieval": 4.543068509083241,
        "debate": 72.50441964203492,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 77.22490070806816,
      "bert_score": 0.8438844084739685,
      "f1_score": 0.19999999999999998,
      "rouge_score": 0.1904761904761905,
      "bleurt_score": -1.2985047101974487,
      "score": "5"
    },
    {
      "query": "What sustains the tree?",
      "predicted_response": "sapling-sized sprouts",
      "method": "exact-match",
      "actual_response": "sapling-sized sprouts",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9499991796910763e-05,
        "tree_retrieval": 4.899631195003167,
        "debate": 72.50441964203492,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 5.1235764590092,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0696970224380493,
      "score": "5"
    },
    {
      "query": "What is the English translation of Zugunruhe?",
      "predicted_response": "migratory restlessness",
      "method": "exact-match",
      "actual_response": "migratory restlessness",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.989999484270811e-05,
        "tree_retrieval": 4.577461684006266,
        "debate": 72.50441964203492,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.728958583902568,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0424739122390747,
      "score": "5"
    },
    {
      "query": "What occurred following Kanye's controversial remark?",
      "predicted_response": "Rick Kaplan cut off the microphone and then cut away to Chris Tucker",
      "method": "exact-match",
      "actual_response": "Rick Kaplan cut off the microphone and then cut away to Chris Tucker",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.989999484270811e-05,
        "tree_retrieval": 4.532646003994159,
        "debate": 72.50441964203492,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.701808541081846,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0608361959457397,
      "score": "5"
    },
    {
      "query": "When did the Kievan state divide into twelve distinct principalities?",
      "predicted_response": "12th century,",
      "method": "exact-match",
      "actual_response": "12th century,",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9700924642384052e-05,
        "tree_retrieval": 4.888601467013359,
        "debate": 72.50441964203492,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 5.043518124846742,
      "bert_score": 0.9999997019767761,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.091225266456604,
      "score": "5"
    },
    {
      "query": "What are Nigeria's religions apart from the two most prominent ones?",
      "predicted_response": "Nigeria's religions, apart from Christianity and Islam, comprise animism and other indigenous religions. Approximately 10% of Nigerians practice these lesser-known faiths.",
      "method": "debate",
      "actual_response": "a tiny minority of adherents of Animism and other religions",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.090000245720148e-05,
        "tree_retrieval": 4.530160923954099,
        "debate": 61.019691328983754,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 65.79051099997014,
      "bert_score": 0.874104917049408,
      "f1_score": 0.33333333333333326,
      "rouge_score": 0.24242424242424243,
      "bleurt_score": 0.06288982927799225,
      "score": "5"
    },
    {
      "query": "What does the government of Saint Helena produce independently?",
      "predicted_response": "coinage and banknotes",
      "method": "exact-match",
      "actual_response": "coinage and banknotes",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.700102049857378e-05,
        "tree_retrieval": 4.964270928991027,
        "debate": 61.019691328983754,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 5.1385317500680685,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0320954322814941,
      "score": "5"
    },
    {
      "query": "In which location can the lake in Atenco be found?",
      "predicted_response": "Federal District",
      "method": "exact-match",
      "actual_response": "Federal District",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.2500054910779e-05,
        "tree_retrieval": 4.536103392019868,
        "debate": 61.019691328983754,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.712400957942009,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9840481281280518,
      "score": "5"
    },
    {
      "query": "To which other large agency's formation was the DHS likened?",
      "predicted_response": "the Department of Defense",
      "method": "exact-match",
      "actual_response": "the Department of Defense",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9999995604157448e-05,
        "tree_retrieval": 4.534829598036595,
        "debate": 61.019691328983754,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.778481958201155,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0317355394363403,
      "score": "5"
    },
    {
      "query": "What is the function of the Sieve of Eratosthenes?",
      "predicted_response": "compute primes",
      "method": "exact-match",
      "actual_response": "compute primes",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.2800984121859074e-05,
        "tree_retrieval": 4.551524639944546,
        "debate": 61.019691328983754,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.779722249833867,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0702271461486816,
      "score": "5"
    },
    {
      "query": "Who is the creator of Minecraft?",
      "predicted_response": "Markus \"Notch\" Persson",
      "method": "exact-match",
      "actual_response": "Markus \"Notch\" Persson",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.100000321865082e-05,
        "tree_retrieval": 4.521433808957227,
        "debate": 61.019691328983754,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.675294333137572,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9775742292404175,
      "score": "5"
    },
    {
      "query": "On whose likeness was the ship on wheels modeled?",
      "predicted_response": "Nerthus or Freyr",
      "method": "exact-match",
      "actual_response": "Nerthus or Freyr",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.029999788850546e-05,
        "tree_retrieval": 4.699145251070149,
        "debate": 61.019691328983754,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.850129291880876,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9856759309768677,
      "score": "5"
    },
    {
      "query": "In which location do Eleonora's falcons nest?",
      "predicted_response": "Mediterranean islands",
      "method": "exact-match",
      "actual_response": "Mediterranean islands",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.2799940779805183e-05,
        "tree_retrieval": 4.564198405016214,
        "debate": 61.019691328983754,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.731431791791692,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0079814195632935,
      "score": "5"
    },
    {
      "query": "What features do certain dictionaries include?",
      "predicted_response": "sub-index",
      "method": "exact-match",
      "actual_response": "sub-index",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.100000321865082e-05,
        "tree_retrieval": 4.562927983002737,
        "debate": 61.019691328983754,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.754394625080749,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9534413814544678,
      "score": "5"
    },
    {
      "query": "How is designing defined in the context of built environments?",
      "predicted_response": "architectural lighting design",
      "method": "exact-match",
      "actual_response": "architectural lighting design",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.120000474154949e-05,
        "tree_retrieval": 4.650937631959096,
        "debate": 61.019691328983754,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.914637584006414,
      "bert_score": 1.0000003576278687,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.017285704612732,
      "score": "3"
    },
    {
      "query": "What is the name of the LGBT newspaper?",
      "predicted_response": "Seattle Gay News",
      "method": "exact-match",
      "actual_response": "Seattle Gay News",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0800001695752144e-05,
        "tree_retrieval": 4.667034707963467,
        "debate": 61.019691328983754,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.916731290984899,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9719201326370239,
      "score": "5"
    },
    {
      "query": "What changes have occurred in the local dialects of French-speaking areas?",
      "predicted_response": "have almost disappeared",
      "method": "exact-match",
      "actual_response": "have almost disappeared",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.2100011594593525e-05,
        "tree_retrieval": 4.4730800609104335,
        "debate": 61.019691328983754,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.648773832945153,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0763576030731201,
      "score": "5"
    },
    {
      "query": "What makes this specific rule contentious?",
      "predicted_response": "the Fish and Wildlife Service (FWS) loses much ability to further protect a species if the mitigation measures by the landowner prove insufficient.",
      "method": "exact-match",
      "actual_response": "the Fish and Wildlife Service (FWS) loses much ability to further protect a species if the mitigation measures by the landowner prove insufficient.",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.300001844763756e-05,
        "tree_retrieval": 4.5432527519296855,
        "debate": 61.019691328983754,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.721498833969235,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0176934003829956,
      "score": "5"
    },
    {
      "query": "In which four years did England exit the UEFA European Championship at the group stage?",
      "predicted_response": "1980, 1988, 1992 and 2000",
      "method": "exact-match",
      "actual_response": "1980, 1988, 1992 and 2000",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.929999027401209e-05,
        "tree_retrieval": 4.882322629913688,
        "debate": 61.019691328983754,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 5.138271708972752,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0484496355056763,
      "score": "5"
    },
    {
      "query": "Who was interred in the abbey on April 4, 1727?",
      "predicted_response": "Isaac Newton",
      "method": "exact-match",
      "actual_response": "Isaac Newton",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.3999949917197227e-05,
        "tree_retrieval": 4.586165269021876,
        "debate": 61.019691328983754,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.738363499986008,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8779375553131104,
      "score": "5"
    },
    {
      "query": "What vanished and was later reconstructed to play period music?",
      "predicted_response": "authentic instruments",
      "method": "exact-match",
      "actual_response": "authentic instruments",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.95999925583601e-05,
        "tree_retrieval": 4.512572820065543,
        "debate": 61.019691328983754,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.721523833926767,
      "bert_score": 1.0000007152557373,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9698896408081055,
      "score": "5"
    },
    {
      "query": "Which is one of the most challenging derbies held in Egypt?",
      "predicted_response": "The Cairo Derby",
      "method": "exact-match",
      "actual_response": "The Cairo Derby",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.300001844763756e-05,
        "tree_retrieval": 4.514868906931952,
        "debate": 61.019691328983754,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.69839912513271,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9872046709060669,
      "score": "5"
    },
    {
      "query": "In which article by Donnelly and Embrechts did warnings and research on CDOs first appear?",
      "predicted_response": "2006",
      "method": "exact-match",
      "actual_response": "2006",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0499999411404133e-05,
        "tree_retrieval": 4.54829135898035,
        "debate": 61.019691328983754,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.740604917053133,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8033155798912048,
      "score": "1"
    },
    {
      "query": "Who is the focal point of all mystical experiences?",
      "predicted_response": "Christ",
      "method": "exact-match",
      "actual_response": "Christ",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.090000245720148e-05,
        "tree_retrieval": 4.4450753319542855,
        "debate": 61.019691328983754,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.621539708226919,
      "bert_score": 0.9999995827674866,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9097690582275391,
      "score": "5"
    },
    {
      "query": "What are the components of gastric juice?",
      "predicted_response": "hydrochloric acid and pepsin",
      "method": "exact-match",
      "actual_response": "hydrochloric acid and pepsin",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.889998722821474e-05,
        "tree_retrieval": 4.506250667967834,
        "debate": 61.019691328983754,
        "global_llm": 2.9034230960533023
      },
      "overall_latency": 4.661404666956514,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.002150535583496,
      "score": "5"
    }
  ]
}