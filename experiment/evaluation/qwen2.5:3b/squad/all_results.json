{
  "results": [
    {
      "query": "Who was Cixi's son?",
      "predicted_response": "Tongzhi",
      "method": "exact-match",
      "actual_response": "Tongzhi",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.850196324288845e-05,
        "tree_retrieval": 0.27603846101555973,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.4831195840379223,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.069361686706543
    },
    {
      "query": "In kilometers, how long is the Staten Island Ferry route?",
      "predicted_response": "8.4",
      "method": "exact-match",
      "actual_response": "8.4",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 1.8601014744490385e-05,
        "tree_retrieval": 0.07085374998860061,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.13736762502230704,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.01296067237854
    },
    {
      "query": "What was the name of the miner's leader that was blamed for the strike?",
      "predicted_response": "Arthur Scargill",
      "method": "exact-match",
      "actual_response": "Arthur Scargill",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.5402015782892704e-05,
        "tree_retrieval": 0.0758640129934065,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.14222870906814933,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.028001070022583
    },
    {
      "query": "How much was allocated?",
      "predicted_response": "\u20b9 52.7 million",
      "method": "exact-match",
      "actual_response": "\u20b9 52.7 million",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.170098014175892e-05,
        "tree_retrieval": 0.07223054999485612,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.14076424995437264,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0732905864715576
    },
    {
      "query": "When did Virginia adopt The Principle of Partus Sequitur Ventrem?",
      "predicted_response": "1662",
      "method": "exact-match",
      "actual_response": "1662",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.530199708417058e-05,
        "tree_retrieval": 0.06986017798772082,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.14550899993628263,
      "bert_score": 0.9999996423721313,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0334352254867554
    },
    {
      "query": "When did the Borders Books open up in Ann Arbor?",
      "predicted_response": "1971",
      "method": "exact-match",
      "actual_response": "1971",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6402005460113287e-05,
        "tree_retrieval": 0.07270458404673263,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.1451524159638211,
      "bert_score": 1.0000005960464478,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0275993347167969
    },
    {
      "query": "What has the research that has been conducted for the influence of genetics made many people think?",
      "predicted_response": "that biology and environment factors play a complex role in forming it",
      "method": "exact-match",
      "actual_response": "that biology and environment factors play a complex role in forming it",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8202019166201353e-05,
        "tree_retrieval": 0.07136488700052723,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.16300208307802677,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9165321588516235
    },
    {
      "query": "How big was the crew of a bomber?",
      "predicted_response": "four to five crewmen",
      "method": "exact-match",
      "actual_response": "four to five crewmen",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.1403033062815666e-05,
        "tree_retrieval": 0.07277298904955387,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.14453033404424787,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0771594047546387
    },
    {
      "query": "What, typically, is the speed of the second movement in a classical piece?",
      "predicted_response": "slow",
      "method": "exact-match",
      "actual_response": "slow",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.1502026356756687e-05,
        "tree_retrieval": 0.07191932696150616,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.13795120792929083,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9910584092140198
    },
    {
      "query": "Along with the Marianas, where do typhoons that begin in the Marshalls sometimes terminate?",
      "predicted_response": "the Philippines",
      "method": "exact-match",
      "actual_response": "the Philippines",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.340204082429409e-05,
        "tree_retrieval": 0.07340713503072038,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.15851779200602323,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0828087329864502
    },
    {
      "query": "How much of Nigeria's male population can read?",
      "predicted_response": "75.7%",
      "method": "exact-match",
      "actual_response": "75.7%",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.5802000891417265e-05,
        "tree_retrieval": 0.07302930799778551,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.14793383399955928,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0410242080688477
    },
    {
      "query": "The true diversity within the insect species remains what?",
      "predicted_response": "uncertain",
      "method": "exact-match",
      "actual_response": "uncertain",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.0301027689129114e-05,
        "tree_retrieval": 0.07307391095673665,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.14060762501321733,
      "bert_score": 1.0000008344650269,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0784876346588135
    },
    {
      "query": "What school does not include the Mahayava scriptures in its canon?",
      "predicted_response": "the Theravada",
      "method": "exact-match",
      "actual_response": "the Theravada",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.100097481161356e-05,
        "tree_retrieval": 0.07119127397891134,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.14489766699261963,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0546424388885498
    },
    {
      "query": "How much rain does the Sahel savannah area get per year?",
      "predicted_response": "less than 500 millimetres (20 in)",
      "method": "exact-match",
      "actual_response": "less than 500 millimetres (20 in)",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.1201965864747763e-05,
        "tree_retrieval": 0.06957125599728897,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.13680404203478247,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0294049978256226
    },
    {
      "query": "What are the financial districts of London known as?",
      "predicted_response": "the City of London and Canary Wharf",
      "method": "exact-match",
      "actual_response": "the City of London and Canary Wharf",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.08019807562232e-05,
        "tree_retrieval": 0.07092775497585535,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.13692716602236032,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0025506019592285
    },
    {
      "query": "Approximately how many old masters works are included in the V&A collection?",
      "predicted_response": "2,000",
      "method": "exact-match",
      "actual_response": "2,000",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.3801985662430525e-05,
        "tree_retrieval": 0.07228415296413004,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.14329229097347707,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0273948907852173
    },
    {
      "query": "By coping the Arsenal team kit exactly, what nickname was given to Sporting Clube de Braga?",
      "predicted_response": "Os Arsenalistas",
      "method": "exact-match",
      "actual_response": "Os Arsenalistas",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.0602019503712654e-05,
        "tree_retrieval": 0.06975556904217228,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.13873750006314367,
      "bert_score": 0.9999996423721313,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9826005101203918
    },
    {
      "query": "Which year was royal assent last withheld in the UK?",
      "predicted_response": "1708",
      "method": "exact-match",
      "actual_response": "1708",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.2903004214167595e-05,
        "tree_retrieval": 0.0728201920283027,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.14524700003676116,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.015062689781189
    },
    {
      "query": "Children who have experienced sever diarrhea are more likely to have what effect?",
      "predicted_response": "significantly lower scores on a series of tests of intelligence",
      "method": "exact-match",
      "actual_response": "significantly lower scores on a series of tests of intelligence",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7702015358954668e-05,
        "tree_retrieval": 0.07074114104034379,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.14396245800890028,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9464283585548401
    },
    {
      "query": "How is the recognized official language of Burma displayed ? ",
      "predicted_response": "It is written in a script consisting of circular and semi-circular letters",
      "method": "exact-match",
      "actual_response": "It is written in a script consisting of circular and semi-circular letters",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.4602049961686134e-05,
        "tree_retrieval": 0.07093445502687246,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.14367929100990295,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9930315613746643
    },
    {
      "query": "What dis the new term, post-punk, cover?",
      "predicted_response": "groups moving beyond punk's sonic template into disparate areas",
      "method": "exact-match",
      "actual_response": "groups moving beyond punk's sonic template into disparate areas",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.3800996132194996e-05,
        "tree_retrieval": 0.0704216180020012,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.15275050001218915,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9855169653892517
    },
    {
      "query": "What did a study done by Elizabeth Loftus and John palmer show?",
      "predicted_response": "that when people were provided with misleading information they tended to misremember, a phenomenon known as the misinformation effect.",
      "method": "exact-match",
      "actual_response": "that when people were provided with misleading information they tended to misremember, a phenomenon known as the misinformation effect.",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8902024496346712e-05,
        "tree_retrieval": 0.07204223598819226,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.16243762499652803,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9034003615379333
    },
    {
      "query": "What well-known archeologist believed the Amazon didn't have many inhabitants?",
      "predicted_response": "Betty Meggers",
      "method": "exact-match",
      "actual_response": "Betty Meggers",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.0502018742263317e-05,
        "tree_retrieval": 0.07012899697292596,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.13578879192937165,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9760449528694153
    },
    {
      "query": "What are there no longer limitations on since 1990?",
      "predicted_response": "residency registration",
      "method": "exact-match",
      "actual_response": "residency registration",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6202003937214613e-05,
        "tree_retrieval": 0.07226005202392116,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.1405974170193076,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0056910514831543
    },
    {
      "query": "What happens if the mode is activated on the fire-recall floor?",
      "predicted_response": "the elevator will have an alternate floor to recall to",
      "method": "exact-match",
      "actual_response": "the elevator will have an alternate floor to recall to",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7001951821148396e-05,
        "tree_retrieval": 0.07107366598211229,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.13847620808519423,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8725520968437195
    },
    {
      "query": "What does the Fermat primality test depend upon?",
      "predicted_response": "np\u2261n (mod p)",
      "method": "exact-match",
      "actual_response": "np\u2261n (mod p)",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0701979994773865e-05,
        "tree_retrieval": 0.0717310129548423,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.14082433399744332,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9970036149024963
    },
    {
      "query": "What year was the PlayStation 3 released?",
      "predicted_response": "2006",
      "method": "exact-match",
      "actual_response": "2006",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6102003175765276e-05,
        "tree_retrieval": 0.06959455698961392,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.14313674997538328,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0723955631256104
    },
    {
      "query": "Who is the current captian of the cricket team?",
      "predicted_response": "Rakep Patel",
      "method": "exact-match",
      "actual_response": "Rakep Patel",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.5601009838283062e-05,
        "tree_retrieval": 0.0734210359514691,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.1666807500878349,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0598909854888916
    },
    {
      "query": "Who helped with the Free Elections?",
      "predicted_response": "UN Office of Electoral Affairs",
      "method": "exact-match",
      "actual_response": "UN Office of Electoral Affairs",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7102010790258646e-05,
        "tree_retrieval": 0.07231305597815663,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.13736599998082966,
      "bert_score": 0.9999995827674866,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0042321681976318
    },
    {
      "query": "Where is the Fylde coast?",
      "predicted_response": "Blackpool Urban Area",
      "method": "exact-match",
      "actual_response": "Blackpool Urban Area",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.300197957083583e-05,
        "tree_retrieval": 0.07163330598268658,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.13838670903351158,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0974078178405762
    },
    {
      "query": "Where is the Nea Moni Monastery located?",
      "predicted_response": "Chios",
      "method": "exact-match",
      "actual_response": "Chios",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0202034395188093e-05,
        "tree_retrieval": 0.07108756602974609,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.14004462503362447,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0363538265228271
    },
    {
      "query": "What was the first carrier-launched airstrike?",
      "predicted_response": "the Tondern Raid in July 1918",
      "method": "exact-match",
      "actual_response": "the Tondern Raid in July 1918",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.2900989279150963e-05,
        "tree_retrieval": 0.06916702695889398,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.13465866690967232,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0269477367401123
    },
    {
      "query": "What is the county's payroll?",
      "predicted_response": "greater than $733 million.",
      "method": "exact-match",
      "actual_response": "greater than $733 million.",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.4601002223789692e-05,
        "tree_retrieval": 0.07253237097756937,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.1448261250043288,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0162701606750488
    },
    {
      "query": "What do forces have with regard to additive quantities?",
      "predicted_response": "magnitude and direction",
      "method": "exact-match",
      "actual_response": "magnitude and direction",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.520296351984143e-05,
        "tree_retrieval": 0.07291929901111871,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.14528404199518263,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0915021896362305
    },
    {
      "query": "What percent decrease did audio CD sales experience?",
      "predicted_response": "50%",
      "method": "exact-match",
      "actual_response": "50%",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7102010790258646e-05,
        "tree_retrieval": 0.07106216496322304,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.14418533397838473,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.00872004032135
    },
    {
      "query": "Who was influenced by Western European and Islamic tendencies?",
      "predicted_response": "Greek masters working in Sicily",
      "method": "exact-match",
      "actual_response": "Greek masters working in Sicily",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.4101987946778536e-05,
        "tree_retrieval": 0.07529047195566818,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.1440413340460509,
      "bert_score": 0.9999997019767761,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.031867265701294
    },
    {
      "query": "Who said in 2012 that the fight would change from military to law enforcement?",
      "predicted_response": "Jeh Johnson",
      "method": "exact-match",
      "actual_response": "Jeh Johnson",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 1.7501006368547678e-05,
        "tree_retrieval": 0.07224995095748454,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.14255658397451043,
      "bert_score": 1.0000003576278687,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9620630145072937
    },
    {
      "query": "Which company made Spectre?",
      "predicted_response": "Eon Productions",
      "method": "exact-match",
      "actual_response": "Eon Productions",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 6.150500848889351e-05,
        "tree_retrieval": 0.07502181100426242,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.14141312497667968,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0393056869506836
    },
    {
      "query": "What sports activity is featured in The Times on Mondays?",
      "predicted_response": "football",
      "method": "exact-match",
      "actual_response": "football",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.4101987946778536e-05,
        "tree_retrieval": 0.06915877998108044,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.14097145898267627,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9818461537361145
    },
    {
      "query": "What area differs from other areas in the United Kingdom regarding education?",
      "predicted_response": "Wales",
      "method": "exact-match",
      "actual_response": "Wales",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.310104900971055e-05,
        "tree_retrieval": 0.07128233602270484,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.13677495904266834,
      "bert_score": 0.9999992251396179,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0956119298934937
    },
    {
      "query": "What type of paper is The Siasat Daily?",
      "predicted_response": "major Urdu papers",
      "method": "exact-match",
      "actual_response": "major Urdu papers",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.310298779979348e-05,
        "tree_retrieval": 0.07084290398051962,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.14296120801009238,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.1238956451416016
    },
    {
      "query": "Which Soviet countries did the RSFSR border on the west?",
      "predicted_response": "the Ukrainian, Belarusian, Estonian, Latvian and Lithuanian SSRs",
      "method": "exact-match",
      "actual_response": "the Ukrainian, Belarusian, Estonian, Latvian and Lithuanian SSRs",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.94029775634408e-05,
        "tree_retrieval": 0.07136094197630882,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.13846133404877037,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0071278810501099
    },
    {
      "query": "This type of problem happens in any organization where the interests of the people who financially support it conflicts with what?",
      "predicted_response": "the primary purpose of the institution",
      "method": "exact-match",
      "actual_response": "the primary purpose of the institution",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.460199175402522e-05,
        "tree_retrieval": 0.07212109700776637,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.1391188750276342,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9458703398704529
    },
    {
      "query": "What is the Windows 8 Pro price?",
      "predicted_response": "$199.99",
      "method": "exact-match",
      "actual_response": "$199.99",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.360198413953185e-05,
        "tree_retrieval": 0.07211239699972793,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.14135004195850343,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0597538948059082
    },
    {
      "query": "What online service did the National Archives decide to use to showcase its photographic holdings?",
      "predicted_response": "Flickr",
      "method": "exact-match",
      "actual_response": "Flickr",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8102018404752016e-05,
        "tree_retrieval": 0.07167326495982707,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.14073191699571908,
      "bert_score": 0.9999997019767761,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0999910831451416
    },
    {
      "query": "What is a broad, modern working definition of a gene?",
      "predicted_response": "any discrete locus of heritable, genomic sequence which affect an organism's traits by being expressed as a functional product",
      "method": "exact-match",
      "actual_response": "any discrete locus of heritable, genomic sequence which affect an organism's traits by being expressed as a functional product",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.560199936851859e-05,
        "tree_retrieval": 0.07274154300102964,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.13839129195548594,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8827047944068909
    },
    {
      "query": "Why is tort law hard to standardize?",
      "predicted_response": "immense size and diversity",
      "method": "exact-match",
      "actual_response": "immense size and diversity",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 6.390403723344207e-05,
        "tree_retrieval": 0.07311777101131156,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.1411560830892995,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0237494707107544
    },
    {
      "query": "What had a hold over social life in the Ottoman Empire as well as other cultures?",
      "predicted_response": "religion",
      "method": "exact-match",
      "actual_response": "religion",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.2202998884022236e-05,
        "tree_retrieval": 0.06988103297771886,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.1686932910233736,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0507709980010986
    },
    {
      "query": "Who does the evaluations for drugs in the United Kingdom?",
      "predicted_response": "European Medicines Agency",
      "method": "exact-match",
      "actual_response": "European Medicines Agency",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.5902001652866602e-05,
        "tree_retrieval": 0.07272154098609462,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.13874933391343802,
      "bert_score": 0.9999997019767761,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0151920318603516
    },
    {
      "query": "In what year was the God's House Tower built?",
      "predicted_response": "1417",
      "method": "exact-match",
      "actual_response": "1417",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.5702000129967928e-05,
        "tree_retrieval": 0.06901126896264032,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.13670391705818474,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0390154123306274
    },
    {
      "query": "What is a PoP?",
      "predicted_response": "point of presence",
      "method": "exact-match",
      "actual_response": "point of presence",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.0100967958569527e-05,
        "tree_retrieval": 0.07012505002785474,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.14561445801518857,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0651674270629883
    },
    {
      "query": "What tree leaves did Androsthenes describe?",
      "predicted_response": "tamarind",
      "method": "exact-match",
      "actual_response": "tamarind",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7401954866945744e-05,
        "tree_retrieval": 0.0725678310263902,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.13847962499130517,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0493814945220947
    },
    {
      "query": "Where is the North Hills Shopping center?",
      "predicted_response": "Midtown Raleigh",
      "method": "exact-match",
      "actual_response": "Midtown Raleigh",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.730201231315732e-05,
        "tree_retrieval": 0.06908447400201112,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.13721720897592604,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0314044952392578
    },
    {
      "query": "What does law enforcement in some countries use to profile suspects?",
      "predicted_response": "race",
      "method": "exact-match",
      "actual_response": "race",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7202011551707983e-05,
        "tree_retrieval": 0.0701096499687992,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.15291291591711342,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0467904806137085
    },
    {
      "query": "What is it easier to do regarding God?",
      "predicted_response": "state what God is not",
      "method": "exact-match",
      "actual_response": "state what God is not",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7902016881853342e-05,
        "tree_retrieval": 0.06992433598497882,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.134170625009574,
      "bert_score": 0.9999997019767761,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0435986518859863
    },
    {
      "query": " What is the second most populous of Switzerland's cantons?",
      "predicted_response": "Canton of Bern",
      "method": "exact-match",
      "actual_response": "Canton of Bern",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.0501029212027788e-05,
        "tree_retrieval": 0.07058458495885134,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.13677174993790686,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9741231799125671
    },
    {
      "query": "When did the Reconstruction Era end?",
      "predicted_response": "1877",
      "method": "exact-match",
      "actual_response": "1877",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.070202026516199e-05,
        "tree_retrieval": 0.06978312600404024,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.15197504207026213,
      "bert_score": 0.9999998211860657,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9785473942756653
    },
    {
      "query": "808s & Heartbreak was released by what company? ",
      "predicted_response": "Island Def Jam",
      "method": "exact-match",
      "actual_response": "Island Def Jam",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7402013074606657e-05,
        "tree_retrieval": 0.06917698099277914,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.1326642920030281,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.063862681388855
    },
    {
      "query": "What type of stimuli causes pain?",
      "predicted_response": "intense or damaging",
      "method": "exact-match",
      "actual_response": "intense or damaging",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.5902001652866602e-05,
        "tree_retrieval": 0.06967131700366735,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.14360304200090468,
      "bert_score": 0.9999998211860657,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.1341965198516846
    },
    {
      "query": "When did season 14 premiere?",
      "predicted_response": "January 7, 2015",
      "method": "exact-match",
      "actual_response": "January 7, 2015",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.630200469866395e-05,
        "tree_retrieval": 0.07220720400800928,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.1538028750801459,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0398178100585938
    },
    {
      "query": "Maududi was trained as a lawyer, but chose what professional for himself instead?",
      "predicted_response": "journalism",
      "method": "exact-match",
      "actual_response": "journalism",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 8.320598863065243e-05,
        "tree_retrieval": 0.07155215501552448,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.14752770797349513,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0404086112976074
    },
    {
      "query": "Who is the professional head of the British Armed Forces?",
      "predicted_response": "Chief of the Defence Staff",
      "method": "exact-match",
      "actual_response": "Chief of the Defence Staff",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.5402965042740107e-05,
        "tree_retrieval": 0.07001574302557856,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.13745454198215157,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9524481892585754
    },
    {
      "query": "War was started with the Ottomans when?",
      "predicted_response": "17 March 1821",
      "method": "exact-match",
      "actual_response": "17 March 1821",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.1302024833858013e-05,
        "tree_retrieval": 0.07125013403128833,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.13805174990557134,
      "bert_score": 1.0000005960464478,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0970252752304077
    },
    {
      "query": "What was published the same years?",
      "predicted_response": "Annus Mirabilis Papers",
      "method": "exact-match",
      "actual_response": "Annus Mirabilis Papers",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.2701977286487818e-05,
        "tree_retrieval": 0.07271464099176228,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.14529608306474984,
      "bert_score": 1.0000004768371582,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0209676027297974
    },
    {
      "query": "What can tamers do in order to give their partners advantages?",
      "predicted_response": "slide game cards through their \"Digivices",
      "method": "exact-match",
      "actual_response": "slide game cards through their \"Digivices",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.2202031686902046e-05,
        "tree_retrieval": 0.07065638998756185,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.1375227919779718,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.037635087966919
    },
    {
      "query": "What does Stephen Jay Gould call philosophy that deals in the supernatural?",
      "predicted_response": "non-overlapping magisteria\" (NOMA)",
      "method": "exact-match",
      "actual_response": "non-overlapping magisteria\" (NOMA)",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.4702999982982874e-05,
        "tree_retrieval": 0.07061778701609,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.14308187505230308,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9924795031547546
    },
    {
      "query": "The path was important for the expansion of which road?",
      "predicted_response": "El Camino Real de Tierra Adentro",
      "method": "exact-match",
      "actual_response": "El Camino Real de Tierra Adentro",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.850198118016124e-05,
        "tree_retrieval": 0.07096141198417172,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.13907699997071177,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0117204189300537
    },
    {
      "query": "What did Burke think a social hierarchy should be based on?",
      "predicted_response": "property",
      "method": "exact-match",
      "actual_response": "property",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.540300531312823e-05,
        "tree_retrieval": 0.06932019197847694,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.1489264169940725,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9808395504951477
    },
    {
      "query": "What is the term given to 122nd Street by Wizard in Taxi Driver indicating the area is majority black?",
      "predicted_response": "\"Mau Mau Land\"",
      "method": "exact-match",
      "actual_response": "\"Mau Mau Land\"",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.4901994038373232e-05,
        "tree_retrieval": 0.06988383299903944,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.15167833294253796,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9589753746986389
    },
    {
      "query": "What were the only proteins necessary to the circadian timekeeper experiment?",
      "predicted_response": "KaiA, KaiB, KaiC",
      "method": "exact-match",
      "actual_response": "KaiA, KaiB, KaiC",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.270203549414873e-05,
        "tree_retrieval": 0.07020865700906143,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.1366197500610724,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9845021367073059
    },
    {
      "query": "How many delegates passed the motion?",
      "predicted_response": "129",
      "method": "exact-match",
      "actual_response": "129",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6802008505910635e-05,
        "tree_retrieval": 0.07261393300723284,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.1443914999254048,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9316842555999756
    },
    {
      "query": "How are incomes distributed in Sweden?",
      "predicted_response": "more equally",
      "method": "exact-match",
      "actual_response": "more equally",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.900202525779605e-05,
        "tree_retrieval": 0.0706077860086225,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.13507391698658466,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.18402507901191711
    },
    {
      "query": "What group declared independence as Asawad?",
      "predicted_response": "MNLA",
      "method": "exact-match",
      "actual_response": "MNLA",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.600101288408041e-05,
        "tree_retrieval": 0.07165036298101768,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.15482962504029274,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9227818846702576
    },
    {
      "query": "How high is Granite Peak?",
      "predicted_response": "12,799 feet",
      "method": "exact-match",
      "actual_response": "12,799 feet",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.4201988708227873e-05,
        "tree_retrieval": 0.07230351096950471,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.16514462500344962,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.135951280593872
    },
    {
      "query": "What was the name given to the land that North Carolina transferred to the federal government in 1790?",
      "predicted_response": "Southwest Territory",
      "method": "exact-match",
      "actual_response": "Southwest Territory",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.920499097555876e-05,
        "tree_retrieval": 0.07057828403776512,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.1374968329910189,
      "bert_score": 0.9999998211860657,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.034688949584961
    },
    {
      "query": "How many planes did the United States have in the Battle of Midway?",
      "predicted_response": "348",
      "method": "exact-match",
      "actual_response": "348",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.880297299474478e-05,
        "tree_retrieval": 0.07144714798778296,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.14671679202001542,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9768097996711731
    },
    {
      "query": "What is another name for the Eastern Roman Empire?",
      "predicted_response": "Byzantine Empire",
      "method": "exact-match",
      "actual_response": "Byzantine Empire",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 7.070496212691069e-05,
        "tree_retrieval": 0.0709903149981983,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.18306475004646927,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0339752435684204
    },
    {
      "query": "What pontifax maximus favored the Catholic church? ",
      "predicted_response": "Constantine I",
      "method": "exact-match",
      "actual_response": "Constantine I",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.630200469866395e-05,
        "tree_retrieval": 0.07016305398428813,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.15322224993724376,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0200412273406982
    },
    {
      "query": "What is a brief description of Calvinism?",
      "predicted_response": "the sovereignty or rule of God in all things",
      "method": "exact-match",
      "actual_response": "the sovereignty or rule of God in all things",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6702007744461298e-05,
        "tree_retrieval": 0.07127863599453121,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.14306716597639024,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9495947957038879
    },
    {
      "query": "A non-deterministic Turing machine has the ability to capture what facet of useful analysis?",
      "predicted_response": "mathematical models",
      "method": "exact-match",
      "actual_response": "mathematical models",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.730201231315732e-05,
        "tree_retrieval": 0.07283604994881898,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.1781658340478316,
      "bert_score": 0.9999996423721313,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9948444962501526
    },
    {
      "query": "The values of the bourgeois public sphere included holding reason to the supreme, considering everything open to criticism, and the opposition of what?",
      "predicted_response": "secrecy of all sorts",
      "method": "exact-match",
      "actual_response": "secrecy of all sorts",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.94029775634408e-05,
        "tree_retrieval": 0.07105491898255423,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.15334241709206253,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0189406871795654
    },
    {
      "query": "What provided an incentive to western empires to colonize Africa?",
      "predicted_response": "blank spaces on contemporary maps",
      "method": "exact-match",
      "actual_response": "blank spaces on contemporary maps",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.5902001652866602e-05,
        "tree_retrieval": 0.07102131698047742,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.14557012496516109,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0010732412338257
    },
    {
      "query": "What year did the Blackstone Group express interest in acquiring Dell?",
      "predicted_response": "2013",
      "method": "exact-match",
      "actual_response": "2013",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.0201026927679777e-05,
        "tree_retrieval": 0.07327228196663782,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.15197324997279793,
      "bert_score": 1.0000009536743164,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0094513893127441
    },
    {
      "query": "What was Kerry's role in the Yale Political Union as a sophomore?",
      "predicted_response": "Chairman of the Liberal Party",
      "method": "exact-match",
      "actual_response": "Chairman of the Liberal Party",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0802038963884115e-05,
        "tree_retrieval": 0.07071939395973459,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.1538762500276789,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9861997961997986
    },
    {
      "query": "What do bergschrunds resemble?",
      "predicted_response": "crevasses",
      "method": "exact-match",
      "actual_response": "crevasses",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.470205072313547e-05,
        "tree_retrieval": 0.06915518001187593,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.13613287499174476,
      "bert_score": 1.0000004768371582,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0751757621765137
    },
    {
      "query": "To whom did Luther send a letter containing his 95 Theses?",
      "predicted_response": "Archbishop Albrecht",
      "method": "exact-match",
      "actual_response": "Archbishop Albrecht",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.600200241431594e-05,
        "tree_retrieval": 0.07329138403292745,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.1543850840535015,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9965358376502991
    },
    {
      "query": "In what year did Lobund at Notre Dame become an Institute?",
      "predicted_response": "1950",
      "method": "exact-match",
      "actual_response": "1950",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.010298289358616e-05,
        "tree_retrieval": 0.0775321859982796,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.14633150002919137,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.080525279045105
    },
    {
      "query": "What percentage of the European population consisted of monks?",
      "predicted_response": "less than one per cent",
      "method": "exact-match",
      "actual_response": "less than one per cent",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9101967811584473e-05,
        "tree_retrieval": 0.07209097396116704,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.13944683398585767,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9860675930976868
    },
    {
      "query": "What advantage would taking St. John's have provided for the French?",
      "predicted_response": "the expedition would have strengthened France's hand at the negotiating table",
      "method": "exact-match",
      "actual_response": "the expedition would have strengthened France's hand at the negotiating table",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.460199175402522e-05,
        "tree_retrieval": 0.07266291603446007,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.15445091691799462,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9272240996360779
    },
    {
      "query": "Where are most of these companies located in an airport?",
      "predicted_response": "within the departure areas",
      "method": "exact-match",
      "actual_response": "within the departure areas",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.600200241431594e-05,
        "tree_retrieval": 0.07038954901508987,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 0.1409761670511216,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9628992676734924
    },
    {
      "query": "What is the earliest year the term \"redhead\" was used?",
      "predicted_response": "1510",
      "method": "exact-match",
      "actual_response": "1510",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 1.9400962628424168e-05,
        "tree_retrieval": 5.788604212983046,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.863718374981545,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0505789518356323
    },
    {
      "query": "Which nation did Kerry travel to during her student years?",
      "predicted_response": "Switzerland",
      "method": "exact-match",
      "actual_response": "Switzerland",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.57010105997324e-05,
        "tree_retrieval": 5.238187307026237,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.349345125025138,
      "bert_score": 0.999999463558197,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0796077251434326
    },
    {
      "query": "Where in Greenland do glaciers advance 20-30 meters daily?",
      "predicted_response": "Jakobshavn Isbr\u00e6",
      "method": "exact-match",
      "actual_response": "Jakobshavn Isbr\u00e6",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.3100990802049637e-05,
        "tree_retrieval": 5.366804482007865,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.445845957961865,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0366476774215698
    },
    {
      "query": "What is the purpose of these statutes?",
      "predicted_response": "freedom of speech",
      "method": "exact-match",
      "actual_response": "freedom of speech",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.780302358791232e-05,
        "tree_retrieval": 5.408499301993288,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.481178041896783,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9961149096488953
    },
    {
      "query": "Which places on Egypt's northern coast are popular for recreational tourism?",
      "predicted_response": "Dahab, Ras Sidr, Marsa Alam, Safaga",
      "method": "exact-match",
      "actual_response": "Dahab, Ras Sidr, Marsa Alam, Safaga",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.5401997845619917e-05,
        "tree_retrieval": 5.389918572967872,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.4714516249950975,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9996127486228943
    },
    {
      "query": "How did Whitehead suggest reality should be perceived?",
      "predicted_response": "experiential",
      "method": "exact-match",
      "actual_response": "experiential",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8402020689100027e-05,
        "tree_retrieval": 5.644306216039695,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.730969791999087,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0644036531448364
    },
    {
      "query": "Which organization represents all cable companies in Washington?",
      "predicted_response": "The National Cable & Telecommunications Association",
      "method": "exact-match",
      "actual_response": "The National Cable & Telecommunications Association",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.5002053007483482e-05,
        "tree_retrieval": 5.232135611993726,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.300221165991388,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9524157643318176
    },
    {
      "query": "Who stepped down from their role in 1839?",
      "predicted_response": "Melbourne",
      "method": "exact-match",
      "actual_response": "Melbourne",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6102003175765276e-05,
        "tree_retrieval": 5.3495353239704855,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.431926040910184,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0140864849090576
    },
    {
      "query": "What can be accomplished without constructing tall structures or eliminating shared spaces?",
      "predicted_response": "their approach to building design",
      "method": "exact-match",
      "actual_response": "high density",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.530199708417058e-05,
        "tree_retrieval": 5.441095682035666,
        "debate": 23.49626210302813,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.508176791016012,
      "bert_score": 0.8316892981529236,
      "f1_score": 0,
      "rouge_score": 0.0,
      "bleurt_score": -1.0911014080047607
    },
    {
      "query": "What description was partially given to the minority leader?",
      "predicted_response": "The minority leader is a party spokesperson, articulating policies and positions. This role emerged during the 20th century.",
      "method": "debate",
      "actual_response": "He is spokesman for his party and enunciates its policies.",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.830201992765069e-05,
        "tree_retrieval": 5.28528007096611,
        "debate": 57.790381100960076,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 63.164310374995694,
      "bert_score": 0.8944793343544006,
      "f1_score": 0.32,
      "rouge_score": 0.21428571428571427,
      "bleurt_score": -0.633389949798584
    },
    {
      "query": "What position did France take as Prussia aimed to integrate various German kingdoms?",
      "predicted_response": "strongly opposed to any further alliance of German states",
      "method": "exact-match",
      "actual_response": "strongly opposed to any further alliance of German states",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.45029984600842e-05,
        "tree_retrieval": 5.513839931983966,
        "debate": 57.790381100960076,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.580064374953508,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9446428418159485
    },
    {
      "query": "In which location was the Indo-Greek Kingdom situated?",
      "predicted_response": "South Asia",
      "method": "exact-match",
      "actual_response": "South Asia",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.5400950107723475e-05,
        "tree_retrieval": 5.583103634999134,
        "debate": 57.790381100960076,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.656367791001685,
      "bert_score": 0.9999996423721313,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0734001398086548
    },
    {
      "query": "What contributed to improved fuse setting?",
      "predicted_response": "range measurement",
      "method": "exact-match",
      "actual_response": "range measurement",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6202003937214613e-05,
        "tree_retrieval": 5.608240836998448,
        "debate": 57.790381100960076,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.713063084054738,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.047179102897644
    },
    {
      "query": "Which technique is employed to evaluate software performance under a particular load?",
      "predicted_response": "Load testing",
      "method": "exact-match",
      "actual_response": "Load testing",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6802008505910635e-05,
        "tree_retrieval": 5.384773219004273,
        "debate": 57.790381100960076,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.4680701670004055,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9958773255348206
    },
    {
      "query": "What is the global volume of growing stock forest in cubic meters?",
      "predicted_response": "434 billion",
      "method": "exact-match",
      "actual_response": "434 billion",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.2701995223760605e-05,
        "tree_retrieval": 5.263631299021654,
        "debate": 57.790381100960076,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.330815042019822,
      "bert_score": 0.9999998211860657,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0721756219863892
    },
    {
      "query": "What was the original Quantum theory?",
      "predicted_response": "Quantum theory originated with Max Planck in 1900. He introduced energy quanta and presented the formula E = h\u03bd (where h is Planck's constant and \u03bd is frequency of radiation) as the energy of a vibrating system. This laid foundations for future developments like Heisenberg\u2019s uncertainty principle and Schr\u00f6dinger's equation, but it did not constitute a complete or self-consistent theory of quantum phenomena; instead, it provided essential insights for the evolution into modern quantum mechanics.",
      "method": "debate",
      "actual_response": "a collection of results which predate modern quantum mechanics, but were never complete or self-consistent.",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9003014788031578e-05,
        "tree_retrieval": 5.382508573005907,
        "debate": 181.99946160201216,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 187.4609481249936,
      "bert_score": 0.8619294762611389,
      "f1_score": 0.19277108433734938,
      "rouge_score": 0.12765957446808512,
      "bleurt_score": -0.8705556392669678
    },
    {
      "query": "Who currently holds the position of administrator on Norfolk Island?",
      "predicted_response": "Gary Hardgrave",
      "method": "exact-match",
      "actual_response": "Gary Hardgrave",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7202011551707983e-05,
        "tree_retrieval": 5.491832352010533,
        "debate": 181.99946160201216,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.555495292064734,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.057018518447876
    },
    {
      "query": "Which yearly sports event includes the Wanamaker Mile?",
      "predicted_response": "Millrose Games",
      "method": "exact-match",
      "actual_response": "Millrose Games",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.930202754214406e-05,
        "tree_retrieval": 5.247012749954592,
        "debate": 181.99946160201216,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.3134984580101445,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0285835266113281
    },
    {
      "query": "Which significant battle took place in 731?",
      "predicted_response": "Battle of the Defile",
      "method": "exact-match",
      "actual_response": "Battle of the Defile",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.630297189578414e-05,
        "tree_retrieval": 5.230028793972451,
        "debate": 181.99946160201216,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.379575958009809,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0061123371124268
    },
    {
      "query": "Which device appeared ideal for Link's arrow shooting skill?",
      "predicted_response": "Wii Remote",
      "method": "exact-match",
      "actual_response": "Wii Remote",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.090198151767254e-05,
        "tree_retrieval": 5.485849757969845,
        "debate": 181.99946160201216,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.551685124984942,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0716050863265991
    },
    {
      "query": "In addition to Reims, which city in France is home to a famous Gothic cathedral?",
      "predicted_response": "Chartres",
      "method": "exact-match",
      "actual_response": "Chartres",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6802008505910635e-05,
        "tree_retrieval": 5.378962863993365,
        "debate": 181.99946160201216,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.444685208960436,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0836443901062012
    },
    {
      "query": "Which state contributed land to Washington, D.C., but later had it returned?",
      "predicted_response": "Virginia",
      "method": "exact-match",
      "actual_response": "Virginia",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.2800988517701626e-05,
        "tree_retrieval": 5.2405295870266855,
        "debate": 181.99946160201216,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.336765625048429,
      "bert_score": 0.9999990463256836,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0345892906188965
    },
    {
      "query": "Why did Sting and other artists call off their planned shows in Kazakhstan?",
      "predicted_response": "human rights concerns",
      "method": "exact-match",
      "actual_response": "human rights concerns",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.920196857303381e-05,
        "tree_retrieval": 5.460943184967618,
        "debate": 181.99946160201216,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.527347124996595,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0524269342422485
    },
    {
      "query": "When did Britain complete repayment of the US loan?",
      "predicted_response": "2006",
      "method": "exact-match",
      "actual_response": "2006",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6102003175765276e-05,
        "tree_retrieval": 5.161504739022348,
        "debate": 181.99946160201216,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.230394209036604,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0723955631256104
    },
    {
      "query": "What information should users know when encoding music to prevent testing each track individually?",
      "predicted_response": "quality setting",
      "method": "exact-match",
      "actual_response": "quality setting",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.4602049961686134e-05,
        "tree_retrieval": 5.318271107040346,
        "debate": 181.99946160201216,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.4653654580470175,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.088935136795044
    },
    {
      "query": "What is the full form of CRA?",
      "predicted_response": "Community Reinvestment Act",
      "method": "exact-match",
      "actual_response": "Community Reinvestment Act",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8101960197091103e-05,
        "tree_retrieval": 5.307963314990047,
        "debate": 181.99946160201216,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.405948083032854,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0179429054260254
    },
    {
      "query": "Which annelid species differ significantly from the rest?",
      "predicted_response": "- Annelids like leeches possess external suckers on their heads and a more complex nervous system with distinct ganglia, which distinguishes them from others in their kind.\n   - Unlike many other annelid species, they lack the common segmentation pattern.",
      "method": "debate",
      "actual_response": "leeches and their closest relatives",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.3903011828660965e-05,
        "tree_retrieval": 5.633840419002809,
        "debate": 71.05963288195198,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 76.77649379102513,
      "bert_score": 0.8217885494232178,
      "f1_score": 0.15,
      "rouge_score": 0.14285714285714288,
      "bleurt_score": -1.3011980056762695
    },
    {
      "query": "Who was the owner of the building at 29 East 32nd Street from 1890 to 1917?",
      "predicted_response": "Grolier Club",
      "method": "exact-match",
      "actual_response": "Grolier Club",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.200299736112356e-05,
        "tree_retrieval": 5.691123891971074,
        "debate": 71.05963288195198,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.760063290945254,
      "bert_score": 1.0000005960464478,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0313740968704224
    },
    {
      "query": "What additions can Parliament make to the citizen-proposed amendment?",
      "predicted_response": "a counter-proposal",
      "method": "exact-match",
      "actual_response": "a counter-proposal",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.140299279242754e-05,
        "tree_retrieval": 5.238235445984174,
        "debate": 71.05963288195198,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.312679499969818,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0196605920791626
    },
    {
      "query": "What purposes have various technologies been developed to serve, as suggested by the phrase \"There's an app for that\"?",
      "predicted_response": "assist hunters",
      "method": "exact-match",
      "actual_response": "assist hunters",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.3901986423879862e-05,
        "tree_retrieval": 5.155106954043731,
        "debate": 71.05963288195198,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.26631791703403,
      "bert_score": 0.9999996423721313,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9986024498939514
    },
    {
      "query": "How many cities in India have been chosen for smart city development?",
      "predicted_response": "hundred",
      "method": "exact-match",
      "actual_response": "hundred",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6502006221562624e-05,
        "tree_retrieval": 5.2160167780239135,
        "debate": 71.05963288195198,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.3032134170643985,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0197592973709106
    },
    {
      "query": "How is Liaoning categorized?",
      "predicted_response": "a training ship",
      "method": "exact-match",
      "actual_response": "a training ship",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.1802028641104698e-05,
        "tree_retrieval": 5.633919948013499,
        "debate": 71.05963288195198,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.7219127079006284,
      "bert_score": 1.0000003576278687,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0626126527786255
    },
    {
      "query": "How many proteins typically make up a flagellum?",
      "predicted_response": "20 proteins",
      "method": "exact-match",
      "actual_response": "20 proteins",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.3701984900981188e-05,
        "tree_retrieval": 5.29905814997619,
        "debate": 71.05963288195198,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.361350917024538,
      "bert_score": 1.0000004768371582,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0631753206253052
    },
    {
      "query": "What are Universities of Technology called in French-speaking regions?",
      "predicted_response": "Instituts de technologie",
      "method": "exact-match",
      "actual_response": "Instituts de technologie",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.730201231315732e-05,
        "tree_retrieval": 5.655700603034347,
        "debate": 71.05963288195198,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.73428941599559,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9912434220314026
    },
    {
      "query": "Who among Alexander's commanders was the first to reach Bahrain?",
      "predicted_response": "Nearchus",
      "method": "exact-match",
      "actual_response": "Nearchus",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7502013836055994e-05,
        "tree_retrieval": 5.254176255024504,
        "debate": 71.05963288195198,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.340341459028423,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0528005361557007
    },
    {
      "query": "Which under-construction building will exceed the height of Comcast Center?",
      "predicted_response": "Comcast Innovation and Technology Center",
      "method": "exact-match",
      "actual_response": "Comcast Innovation and Technology Center",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.1502026356756687e-05,
        "tree_retrieval": 5.26453368604416,
        "debate": 71.05963288195198,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.3356499170186,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0013175010681152
    },
    {
      "query": "How do the actions of the God in Dvaita differ from those in Advaita?",
      "predicted_response": "takes on a personal role",
      "method": "exact-match",
      "actual_response": "takes on a personal role",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7301954105496407e-05,
        "tree_retrieval": 5.265066917985678,
        "debate": 71.05963288195198,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.329928750055842,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0868701934814453
    },
    {
      "query": "Who became the Prime Minister in 1999?",
      "predicted_response": "Ehud Barak",
      "method": "exact-match",
      "actual_response": "Ehud Barak",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.480300074443221e-05,
        "tree_retrieval": 5.346421119989827,
        "debate": 71.05963288195198,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.410291042062454,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0462796688079834
    },
    {
      "query": "Which UK company did Microsoft collaborate with to introduce IPTV to the 360?",
      "predicted_response": "BT",
      "method": "exact-match",
      "actual_response": "BT",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.070097252726555e-05,
        "tree_retrieval": 5.342233794974163,
        "debate": 71.05963288195198,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.419615833088756,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0612413883209229
    },
    {
      "query": "When did someone file a Title IX complaint against Yale?",
      "predicted_response": "March 2011",
      "method": "exact-match",
      "actual_response": "March 2011",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7102010790258646e-05,
        "tree_retrieval": 5.27517042105319,
        "debate": 71.05963288195198,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.340349000063725,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0976982116699219
    },
    {
      "query": "What type of entity is MundoFox?",
      "predicted_response": "Spanish-language television networks",
      "method": "exact-match",
      "actual_response": "Spanish-language television networks",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8202019166201353e-05,
        "tree_retrieval": 5.630146524985321,
        "debate": 71.05963288195198,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.6962043340317905,
      "bert_score": 0.9999998211860657,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0142618417739868
    },
    {
      "query": "What is next to the abbey?",
      "predicted_response": "The Palace of Westminster, chapels, libraries, guest houses, and administrative buildings can be considered elements next to an abbey. These structures may be within the abbey grounds or in close proximity. Without additional context about the specific meaning of \"next,\" these examples provide a general understanding of what might be near an abbey.",
      "method": "debate",
      "actual_response": "Palace of Westminster",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.40019871853292e-05,
        "tree_retrieval": 5.383877729007509,
        "debate": 177.81950347794918,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 183.27304904093035,
      "bert_score": 0.8447315096855164,
      "f1_score": 0.12,
      "rouge_score": 0.10714285714285715,
      "bleurt_score": -1.4964563846588135
    },
    {
      "query": "Who manufactured the Twin Famicom and the 19SC111 TV?",
      "predicted_response": "Sharp Corporation",
      "method": "exact-match",
      "actual_response": "Sharp Corporation",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.330099232494831e-05,
        "tree_retrieval": 5.76200827001594,
        "debate": 177.81950347794918,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.848490041098557,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.106461524963379
    },
    {
      "query": "What is an agreement made between multiple nations called?",
      "predicted_response": "Multilateral treaty or international agreement refers to an agreement between multiple nations. The term \"multilateral\" encompasses treaties concluded among several countries.",
      "method": "debate",
      "actual_response": "A multilateral treaty",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.230098471045494e-05,
        "tree_retrieval": 5.192781336023472,
        "debate": 66.31270517600933,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 71.57471349998377,
      "bert_score": 0.8512908816337585,
      "f1_score": 0.1904761904761905,
      "rouge_score": 0.16666666666666666,
      "bleurt_score": 0.14565403759479523
    },
    {
      "query": "In what year did the Gallipoli Campaign occur?",
      "predicted_response": "1915",
      "method": "exact-match",
      "actual_response": "1915",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8701964765787125e-05,
        "tree_retrieval": 5.6634628269821405,
        "debate": 66.31270517600933,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.764664167072624,
      "bert_score": 1.0000003576278687,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0477744340896606
    },
    {
      "query": "What is Alsace's historical flag called?",
      "predicted_response": "Rot-un-Wiss",
      "method": "exact-match",
      "actual_response": "Rot-un-Wiss",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7202011551707983e-05,
        "tree_retrieval": 5.216178919014055,
        "debate": 66.31270517600933,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.290192292071879,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0076088905334473
    },
    {
      "query": "What is the name of Israel's legislative body?",
      "predicted_response": "the Knesset",
      "method": "exact-match",
      "actual_response": "the Knesset",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8401962481439114e-05,
        "tree_retrieval": 5.240818941965699,
        "debate": 66.31270517600933,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.324292999925092,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.035542368888855
    },
    {
      "query": "What were the nomadic tribes called that hunted bison?",
      "predicted_response": "Jornado",
      "method": "exact-match",
      "actual_response": "Jornado",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.4901994038373232e-05,
        "tree_retrieval": 5.606908662011847,
        "debate": 66.31270517600933,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.7052654169965535,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0346674919128418
    },
    {
      "query": "In which scientific discipline did Wohler make a discovery?",
      "predicted_response": "organic chemistry",
      "method": "exact-match",
      "actual_response": "organic chemistry",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.4801003746688366e-05,
        "tree_retrieval": 5.387267511046957,
        "debate": 66.31270517600933,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.479644834063947,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0267077684402466
    },
    {
      "query": "What is Oklahoma's ranking in crude oil reserves compared to other states?",
      "predicted_response": "fifth",
      "method": "exact-match",
      "actual_response": "fifth",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.820196095854044e-05,
        "tree_retrieval": 5.275925447000191,
        "debate": 66.31270517600933,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.3420827919617295,
      "bert_score": 0.9999997019767761,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.1138430833816528
    },
    {
      "query": "During which era did Queen Victoria rule?",
      "predicted_response": "Queen Victoria ruled during the Victorian Era (1837-1901). At 18, she became queen following her uncle's death, King George III. The era marked a crucial point in British history.",
      "method": "debate",
      "actual_response": "1837 to January 1901",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 8.890702156350017e-05,
        "tree_retrieval": 5.2526396220200695,
        "debate": 97.76826295797946,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 103.10729720804375,
      "bert_score": 0.8305080533027649,
      "f1_score": 0,
      "rouge_score": 0.1142857142857143,
      "bleurt_score": -1.5671175718307495
    },
    {
      "query": "From where did many British colonists migrate to Florida?",
      "predicted_response": "coming from South Carolina, Georgia and England though there was also a group of settlers who came from the colony of Bermuda",
      "method": "exact-match",
      "actual_response": "coming from South Carolina, Georgia and England though there was also a group of settlers who came from the colony of Bermuda",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.500199479982257e-05,
        "tree_retrieval": 5.482144317997154,
        "debate": 97.76826295797946,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.605176374898292,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8572158217430115
    },
    {
      "query": "When was an investigation conducted regarding school casualties?",
      "predicted_response": "December 2008",
      "method": "exact-match",
      "actual_response": "December 2008",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.760201459750533e-05,
        "tree_retrieval": 5.296185053011868,
        "debate": 97.76826295797946,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.366363291977905,
      "bert_score": 1.0000007152557373,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0834574699401855
    },
    {
      "query": "What term describes the competition among individuals of the more numerous sex for mating opportunities?",
      "predicted_response": "intrasexual competition",
      "method": "exact-match",
      "actual_response": "intrasexual competition",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.320294829085469e-05,
        "tree_retrieval": 5.269659432990011,
        "debate": 97.76826295797946,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.336497584008612,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0230984687805176
    },
    {
      "query": "What are the smaller streams used for?",
      "predicted_response": "draining the surrounding land",
      "method": "exact-match",
      "actual_response": "draining the surrounding land",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.250300116837025e-05,
        "tree_retrieval": 5.628397630003747,
        "debate": 97.76826295797946,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.693680249969475,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9717736840248108
    },
    {
      "query": "What is the length of Catalina Highway in miles?",
      "predicted_response": "25",
      "method": "exact-match",
      "actual_response": "25",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.690195105969906e-05,
        "tree_retrieval": 5.341337313002441,
        "debate": 97.76826295797946,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.421382833039388,
      "bert_score": 0.999999463558197,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9702582955360413
    },
    {
      "query": "Why are consulting pharmacists more frequently engaging directly with patients?",
      "predicted_response": "because many elderly people are now taking numerous medications but continue to live outside of institutional settings",
      "method": "exact-match",
      "actual_response": "because many elderly people are now taking numerous medications but continue to live outside of institutional settings",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.230400711297989e-05,
        "tree_retrieval": 5.530772604979575,
        "debate": 97.76826295797946,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.617027958971448,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9211373925209045
    },
    {
      "query": "What did Romans prioritize over grave offerings in later times?",
      "predicted_response": "monumental endowments",
      "method": "exact-match",
      "actual_response": "monumental endowments",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.5702000129967928e-05,
        "tree_retrieval": 5.222258916008286,
        "debate": 97.76826295797946,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.289370083948597,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0035676956176758
    },
    {
      "query": "Which song did Madonna face criticism for performing at the VMA?",
      "predicted_response": "\"Like a Virgin\"",
      "method": "exact-match",
      "actual_response": "\"Like a Virgin\"",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.780195791274309e-05,
        "tree_retrieval": 5.318416100984905,
        "debate": 97.76826295797946,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.3894020000007,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.002030849456787
    },
    {
      "query": "Which ores are most commonly used commercially?",
      "predicted_response": "Iron Ores, Bauxite, and Copper Ores are commonly used in commercial applications. Iron Ores primarily produce iron through blast furnace smelting, while Bauxite provides aluminum metal. Copper Ores are essential for electrical and industrial uses. Their compositions and processing methods cater to diverse global industrial needs.",
      "method": "debate",
      "actual_response": "sulfides",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.5501998607069254e-05,
        "tree_retrieval": 5.339000973966904,
        "debate": 96.54138175398111,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 101.9665579169523,
      "bert_score": 0.7812966108322144,
      "f1_score": 0,
      "rouge_score": 0.0,
      "bleurt_score": -1.0312566757202148
    },
    {
      "query": "How would you define solar energy?",
      "predicted_response": "radiant light and heat from the Sun",
      "method": "exact-match",
      "actual_response": "radiant light and heat from the Sun",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.700201002880931e-05,
        "tree_retrieval": 5.950845120009035,
        "debate": 96.54138175398111,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 6.019213291001506,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0165436267852783
    },
    {
      "query": "Who was the first king of the Sumerian dynasty?",
      "predicted_response": "Etana",
      "method": "exact-match",
      "actual_response": "Etana",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.540300531312823e-05,
        "tree_retrieval": 5.940499596996233,
        "debate": 96.54138175398111,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 6.009378166985698,
      "bert_score": 0.9999994039535522,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9454672932624817
    },
    {
      "query": "What kind of heritage is associated with Tyneside?",
      "predicted_response": "shipbuilding",
      "method": "exact-match",
      "actual_response": "shipbuilding",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6201014406979084e-05,
        "tree_retrieval": 5.57944783300627,
        "debate": 96.54138175398111,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.646870540920645,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.1304341554641724
    },
    {
      "query": "How did Gautama dedicate his life following his enlightenment?",
      "predicted_response": "he spent the rest of his life teaching the path of awakening he had discovered",
      "method": "exact-match",
      "actual_response": "he spent the rest of his life teaching the path of awakening he had discovered",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7902016881853342e-05,
        "tree_retrieval": 5.372080112982076,
        "debate": 96.54138175398111,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.438165667001158,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9400157928466797
    },
    {
      "query": "During which era, alongside the Renaissance, were the Middle Ages criticized?",
      "predicted_response": "Enlightenment",
      "method": "exact-match",
      "actual_response": "Enlightenment",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.700201002880931e-05,
        "tree_retrieval": 5.261196135019418,
        "debate": 96.54138175398111,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.322736417059787,
      "bert_score": 0.9999997019767761,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0873390436172485
    },
    {
      "query": "Who made up the bushi class?",
      "predicted_response": "ancient Japanese soldiers from traditional warrior families",
      "method": "exact-match",
      "actual_response": "ancient Japanese soldiers from traditional warrior families",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.3401982616633177e-05,
        "tree_retrieval": 5.336742274987046,
        "debate": 96.54138175398111,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.40757516596932,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9771100878715515
    },
    {
      "query": "What primarily influences the state's climate?",
      "predicted_response": "elevation of the terrain",
      "method": "exact-match",
      "actual_response": "elevation of the terrain",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.570302553474903e-05,
        "tree_retrieval": 5.308324262034148,
        "debate": 96.54138175398111,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.37315729202237,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9714440703392029
    },
    {
      "query": "In which location can Al-Rashid Islamic Institute be found?",
      "predicted_response": "Cornwall, Ontario",
      "method": "exact-match",
      "actual_response": "Cornwall, Ontario",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.050298593938351e-05,
        "tree_retrieval": 5.201921633968595,
        "debate": 96.54138175398111,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.269815625040792,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0546554327011108
    },
    {
      "query": "Who stripped Japan's Buddhist monks of their weapons?",
      "predicted_response": "Oda Nobunaga",
      "method": "exact-match",
      "actual_response": "Oda Nobunaga",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6401947252452374e-05,
        "tree_retrieval": 5.272320017975289,
        "debate": 96.54138175398111,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.371708124992438,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0264947414398193
    },
    {
      "query": "What sports facility was established in 1952?",
      "predicted_response": "Wutaishan Sports Center",
      "method": "exact-match",
      "actual_response": "Wutaishan Sports Center",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.110299050807953e-05,
        "tree_retrieval": 5.5879899059655145,
        "debate": 96.54138175398111,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.660818791948259,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0054354667663574
    },
    {
      "query": "Who were the Normans' allies in Anatolia?",
      "predicted_response": "Turkish forces",
      "method": "exact-match",
      "actual_response": "Turkish forces",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.600101288408041e-05,
        "tree_retrieval": 5.157805151015054,
        "debate": 96.54138175398111,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.22341008298099,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0110188722610474
    },
    {
      "query": "In which part of Florida is Miami situated?",
      "predicted_response": "Miami is located in Miami-Dade County, southeastern Florida.",
      "method": "debate",
      "actual_response": "southeastern",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.740296233445406e-05,
        "tree_retrieval": 5.377943389001302,
        "debate": 79.88242623599945,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 85.36648633296136,
      "bert_score": 0.8115010857582092,
      "f1_score": 0.2222222222222222,
      "rouge_score": 0.19999999999999998,
      "bleurt_score": -0.9131359457969666
    },
    {
      "query": "What was initially thought to be the cause of the Black Death?",
      "predicted_response": "the heavens",
      "method": "exact-match",
      "actual_response": "the heavens",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.030298441648483e-05,
        "tree_retrieval": 5.534974617999978,
        "debate": 79.88242623599945,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.627419834025204,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.95914226770401
    },
    {
      "query": "Which aspect of treaties frequently requires interpretation when unclear?",
      "predicted_response": "The language",
      "method": "exact-match",
      "actual_response": "The language",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.030195901170373e-05,
        "tree_retrieval": 5.332644191978034,
        "debate": 79.88242623599945,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.428119041025639,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9651509523391724
    },
    {
      "query": "What are churches filled with monuments to?",
      "predicted_response": "monuments to the prosperous",
      "method": "exact-match",
      "actual_response": "monuments to the prosperous",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.2401975002139807e-05,
        "tree_retrieval": 5.49280272400938,
        "debate": 79.88242623599945,
        "global_llm": 0.6817589619895443
      },
      "overall_latency": 5.559270957950503,
      "bert_score": 0.9999996423721313,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9808912873268127
    },
    {
      "query": "What are yours?",
      "predicted_response": "Could you please provide more context or clarify what you are referring to with \"yours\"? This will help me give a more accurate response.",
      "method": "global-llm",
      "actual_response": "fairs held at the shrines of Sufi saints",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.200098242610693e-05,
        "tree_retrieval": 5.458309116016608,
        "debate": 79.88242623599945,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 6.861488207941875,
      "bert_score": 0.8146948218345642,
      "f1_score": 0,
      "rouge_score": 0.0,
      "bleurt_score": -1.5851216316223145
    },
    {
      "query": "Who was unaware of the initial potential revenue the program could generate?",
      "predicted_response": "the public",
      "method": "exact-match",
      "actual_response": "the public",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.730201231315732e-05,
        "tree_retrieval": 5.8396047729766,
        "debate": 79.88242623599945,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.911923666950315,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9409414529800415
    },
    {
      "query": "What tree, other than a palm, is featured on the crest?",
      "predicted_response": "laurel",
      "method": "exact-match",
      "actual_response": "laurel",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.000203287228942e-05,
        "tree_retrieval": 5.246313059993554,
        "debate": 79.88242623599945,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.316814541001804,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9899116158485413
    },
    {
      "query": "What is Mary Wollstonecraft's most famous work?",
      "predicted_response": "A Vindication of the Rights of Woman (1791)",
      "method": "exact-match",
      "actual_response": "A Vindication of the Rights of Woman (1791)",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.5201996322721243e-05,
        "tree_retrieval": 5.569044437026605,
        "debate": 79.88242623599945,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.634418583009392,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.949314534664154
    },
    {
      "query": "How does the capabilities approach view poverty?",
      "predicted_response": "capability deprivation",
      "method": "exact-match",
      "actual_response": "capability deprivation",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.9202026780694723e-05,
        "tree_retrieval": 5.135379411978647,
        "debate": 79.88242623599945,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.202902541961521,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0485591888427734
    },
    {
      "query": "Which early architectural text originated in India?",
      "predicted_response": "Shilpa Shastras",
      "method": "exact-match",
      "actual_response": "Shilpa Shastras",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.4601002223789692e-05,
        "tree_retrieval": 5.337097069947049,
        "debate": 79.88242623599945,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.444243084057234,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0193036794662476
    },
    {
      "query": "During which year did Fresno receive the highest amount of rainfall?",
      "predicted_response": "1885",
      "method": "exact-match",
      "actual_response": "1885",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.800201764330268e-05,
        "tree_retrieval": 5.444928504992276,
        "debate": 79.88242623599945,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.507463000016287,
      "bert_score": 1.0000003576278687,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.010869026184082
    },
    {
      "query": "What are some of the most frequent triggers?",
      "predicted_response": "allergens, smoke (tobacco and other), air pollution, non selective beta-blockers, and sulfite-containing foods",
      "method": "exact-match",
      "actual_response": "allergens, smoke (tobacco and other), air pollution, non selective beta-blockers, and sulfite-containing foods",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6402005460113287e-05,
        "tree_retrieval": 5.349141423997935,
        "debate": 79.88242623599945,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.498006750014611,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9301503300666809
    },
    {
      "query": "What is the duration of the rebirth cycle mentioned in the Vedas?",
      "predicted_response": "4,320,000 years",
      "method": "exact-match",
      "actual_response": "4,320,000 years",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.4401990231126547e-05,
        "tree_retrieval": 5.414458248007577,
        "debate": 79.88242623599945,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.487351499963552,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0263041257858276
    },
    {
      "query": "Which transformation maps a point in the square to its corresponding point under symmetry?",
      "predicted_response": "functions",
      "method": "exact-match",
      "actual_response": "functions",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.630297189578414e-05,
        "tree_retrieval": 5.794163712009322,
        "debate": 79.88242623599945,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.858135625021532,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9584846496582031
    },
    {
      "query": "What contributed to spreading Greek culture beyond its borders?",
      "predicted_response": "Alexander the Great's conquests in the 4th century BCE facilitated the spread of Greek culture into various regions, including Egypt, Persia (modern-day Iran), and India. Greeks established colonies, maintaining cultural connections to their homeland while also imposing governance and introducing Greek art, literature, philosophy, science, and architectural styles.\n\n   The Hellenistic kingdoms that emerged from these foreign territories further spread Greek culture by supporting educational initiatives and cultural exchange programs.\n\n   Trade routes such as the Silk Road played a crucial role in spreading Greek culture by allowing for cultural exchanges between merchants and local populations along these trade networks.",
      "method": "debate",
      "actual_response": "the trading and colonizing activities of the Greek tribes and city states spread the Greek culture, religion and language",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.330300725996494e-05,
        "tree_retrieval": 5.260017168009654,
        "debate": 175.1514251929475,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 180.47882700001355,
      "bert_score": 0.8536390662193298,
      "f1_score": 0.14814814814814817,
      "rouge_score": 0.15126050420168066,
      "bleurt_score": -0.4075622260570526
    },
    {
      "query": "How is man consistently perceived?",
      "predicted_response": "Humans are fundamentally perceptive beings, extending beyond basic sensory input to complex cognitive processes. This allows for diverse forms of intellectual and creative expression. Philosophically, humans have reasoning, problem-solving, abstract understanding, ethical reasoning, scientific inquiry, and artistic expression capabilities. Culturally, they are central to understanding practices, societal roles, and human societies. Biologically, they are recognized for bipedalism, language capabilities, social organization, advanced cognitive functions, and higher-order thinking. This aligns with philosophical schools emphasizing perception as a means to access external realities, significantly impacting self-perception and interaction with the world.",
      "method": "debate",
      "actual_response": "an image of God",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.470100298523903e-05,
        "tree_retrieval": 5.796224793011788,
        "debate": 220.21019289601827,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 226.0739022080088,
      "bert_score": 0.8064736723899841,
      "f1_score": 0.022222222222222223,
      "rouge_score": 0.020833333333333332,
      "bleurt_score": -1.2887474298477173
    },
    {
      "query": "In what ways do genetic alterations impact plants?",
      "predicted_response": "express themselves\") differently",
      "method": "exact-match",
      "actual_response": "express themselves\") differently",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.3201981093734503e-05,
        "tree_retrieval": 5.539856356976088,
        "debate": 220.21019289601827,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.639735374948941,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0776280164718628
    },
    {
      "query": "What themes are commonly explored in a tragedy?",
      "predicted_response": "historical or mythological themes",
      "method": "exact-match",
      "actual_response": "historical or mythological themes",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6702007744461298e-05,
        "tree_retrieval": 5.317196142976172,
        "debate": 220.21019289601827,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.399193000048399,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0167100429534912
    },
    {
      "query": "How many Oscars did 'Jaws' receive?",
      "predicted_response": "Jaws won three Academy Awards in the categories of Best Art Direction and Best Visual Effects (two awards).",
      "method": "debate",
      "actual_response": "three",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.5501998607069254e-05,
        "tree_retrieval": 5.354648774024099,
        "debate": 85.33996199199464,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 90.76301129197236,
      "bert_score": 0.7912953495979309,
      "f1_score": 0.1111111111111111,
      "rouge_score": 0.10526315789473684,
      "bleurt_score": -1.7968534231185913
    },
    {
      "query": "In which location in Oklahoma can you find a notable Egyptian art collection?",
      "predicted_response": "Mabee-Gerrer Museum of Art in Shawnee",
      "method": "exact-match",
      "actual_response": "Mabee-Gerrer Museum of Art in Shawnee",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7102010790258646e-05,
        "tree_retrieval": 5.689649796986487,
        "debate": 85.33996199199464,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.832384542096406,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9476702809333801
    },
    {
      "query": "Who provides financial support for these homeless shelters?",
      "predicted_response": "the city",
      "method": "exact-match",
      "actual_response": "the city",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.760300412774086e-05,
        "tree_retrieval": 5.772949400008656,
        "debate": 85.33996199199464,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.917451750021428,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9277294874191284
    },
    {
      "query": "What kind of group were the \"king's friends\"?",
      "predicted_response": "neo-Tory",
      "method": "exact-match",
      "actual_response": "neo-Tory",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6102003175765276e-05,
        "tree_retrieval": 5.50428269605618,
        "debate": 85.33996199199464,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.622098791995086,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0912129878997803
    },
    {
      "query": "What purpose did Luther believe God had for sending the Turks?",
      "predicted_response": "destroy the antichrist",
      "method": "exact-match",
      "actual_response": "destroy the antichrist",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 6.110500544309616e-05,
        "tree_retrieval": 5.273325815971475,
        "debate": 85.33996199199464,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.343836458050646,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9773337244987488
    },
    {
      "query": "For what purpose has pain occasionally been cited as a reason to grant terminally ill patients a right?",
      "predicted_response": "end their lives",
      "method": "exact-match",
      "actual_response": "end their lives",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.2902037017047405e-05,
        "tree_retrieval": 5.389713831013069,
        "debate": 85.33996199199464,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.451630000025034,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0342135429382324
    },
    {
      "query": "How is all waste from university landscaping utilized?",
      "predicted_response": "composted",
      "method": "exact-match",
      "actual_response": "composted",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8402952011674643e-05,
        "tree_retrieval": 5.297401204006746,
        "debate": 85.33996199199464,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.377539874985814,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.1009106636047363
    },
    {
      "query": "Who was the Cubs' manager in 1969?",
      "predicted_response": "1969: Leo Durocher managed the Chicago Cubs, though there was a brief stint by Joe Maddaroni. For accurate details on their management sequence and duration, more verification is required.",
      "method": "debate",
      "actual_response": "Leo Durocher",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7102010790258646e-05,
        "tree_retrieval": 5.24809309799457,
        "debate": 112.19112550996942,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 117.53216441697441,
      "bert_score": 0.8522722125053406,
      "f1_score": 0.13793103448275862,
      "rouge_score": 0.12903225806451613,
      "bleurt_score": -1.691799283027649
    },
    {
      "query": "In which product did zinc compounds exhibit minimal differences in absorption?",
      "predicted_response": "maize tortillas",
      "method": "exact-match",
      "actual_response": "maize tortillas",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.090203972533345e-05,
        "tree_retrieval": 5.48987876198953,
        "debate": 112.19112550996942,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.602546209003776,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0714354515075684
    },
    {
      "query": "When might the dielectric's polarization fail to keep up with the voltage?",
      "predicted_response": "driven with a time-varying voltage that changes rapidly",
      "method": "exact-match",
      "actual_response": "driven with a time-varying voltage that changes rapidly",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.4501990992575884e-05,
        "tree_retrieval": 5.162061404029373,
        "debate": 112.19112550996942,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.257470333017409,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9672173857688904
    },
    {
      "query": "What is the current name of the country formerly known as South-West Africa?",
      "predicted_response": "Namibia",
      "method": "exact-match",
      "actual_response": "Namibia",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.3901986423879862e-05,
        "tree_retrieval": 5.500344862986822,
        "debate": 112.19112550996942,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.567816584021784,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0540235042572021
    },
    {
      "query": "What is the frequency of XPT departures to Sydney?",
      "predicted_response": "twice a day",
      "method": "exact-match",
      "actual_response": "twice a day",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.3901986423879862e-05,
        "tree_retrieval": 5.249121437955182,
        "debate": 112.19112550996942,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.37454345903825,
      "bert_score": 1.0000003576278687,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.026235580444336
    },
    {
      "query": "Can antibiotics affect the effectiveness of birth control pills?",
      "predicted_response": "The majority of studies indicate antibiotics do interfere with contraceptive pills",
      "method": "exact-match",
      "actual_response": "The majority of studies indicate antibiotics do interfere with contraceptive pills",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.7102977987378836e-05,
        "tree_retrieval": 5.4327498359489255,
        "debate": 112.19112550996942,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.498655458912253,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9578032493591309
    },
    {
      "query": "What is the number of propositions that constitute the separationist stance on the Establishment Clause?",
      "predicted_response": "two",
      "method": "exact-match",
      "actual_response": "two",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.5802000891417265e-05,
        "tree_retrieval": 5.1537062040297315,
        "debate": 112.19112550996942,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.311405207961798,
      "bert_score": 0.9999997019767761,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9411049485206604
    },
    {
      "query": "Which area is referred to as Jhamel?",
      "predicted_response": "Jhamsikhel",
      "method": "exact-match",
      "actual_response": "Jhamsikhel",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.790195867419243e-05,
        "tree_retrieval": 5.402893402031623,
        "debate": 112.19112550996942,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.517572208074853,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0113675594329834
    },
    {
      "query": "What is the number of Methodist churches in Plymouth?",
      "predicted_response": "over twenty",
      "method": "exact-match",
      "actual_response": "over twenty",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7502013836055994e-05,
        "tree_retrieval": 5.485395930008963,
        "debate": 112.19112550996942,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.56311008404009,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0368001461029053
    },
    {
      "query": "From where did the next group of settlers on Norfolk Island originate?",
      "predicted_response": "the Pitcairn Islands",
      "method": "exact-match",
      "actual_response": "the Pitcairn Islands",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7401954866945744e-05,
        "tree_retrieval": 5.293867435015272,
        "debate": 112.19112550996942,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.382943416014314,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0135860443115234
    },
    {
      "query": "Which record company signed the season fourteen winner of American Idol?",
      "predicted_response": "Big Machine Records",
      "method": "exact-match",
      "actual_response": "Big Machine Records",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.5101006031036377e-05,
        "tree_retrieval": 5.5371810420183465,
        "debate": 112.19112550996942,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.641065875068307,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0197899341583252
    },
    {
      "query": "Who governed Egypt during the Suez Crisis?",
      "predicted_response": "Gamal Abdel Nasser",
      "method": "exact-match",
      "actual_response": "Gamal Abdel Nasser",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.5201996322721243e-05,
        "tree_retrieval": 5.313646559952758,
        "debate": 112.19112550996942,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.3887178329750896,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0484157800674438
    },
    {
      "query": "Which Doctor was initially called \"his secret\"?",
      "predicted_response": "the War Doctor",
      "method": "exact-match",
      "actual_response": "the War Doctor",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7402013074606657e-05,
        "tree_retrieval": 5.284888437017798,
        "debate": 112.19112550996942,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.345918125007302,
      "bert_score": 1.0000003576278687,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0263330936431885
    },
    {
      "query": "When did the last Greek resistance occur?",
      "predicted_response": "The last major Greek resistance against Roman expansion occurred in 88 BC, led by Mithridates VI Eupator of Pontus. Throughout Greek history, there have been numerous smaller conflicts and rebellions across different regions and time periods, signifying a continuous thread of resistance efforts rather than isolated incidents.",
      "method": "debate",
      "actual_response": "88 BC",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7502013836055994e-05,
        "tree_retrieval": 5.274620851967484,
        "debate": 131.4609187260503,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 136.84143250004854,
      "bert_score": 0.7977044582366943,
      "f1_score": 0.0851063829787234,
      "rouge_score": 0.08163265306122448,
      "bleurt_score": -1.5302801132202148
    },
    {
      "query": "What type of district does the San Bernardino - Riverside area have?",
      "predicted_response": "business",
      "method": "exact-match",
      "actual_response": "business",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.560199936851859e-05,
        "tree_retrieval": 5.833259369013831,
        "debate": 131.4609187260503,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.936947584035806,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9404962658882141
    },
    {
      "query": "Who are the heads of each house tasked with updating their group on the governor-general's actions?",
      "predicted_response": "President of the Senate and the Speaker of the House of Representatives",
      "method": "exact-match",
      "actual_response": "President of the Senate and the Speaker of the House of Representatives",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.750195562839508e-05,
        "tree_retrieval": 5.34783962299116,
        "debate": 131.4609187260503,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.430309999966994,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9039496183395386
    },
    {
      "query": "After the death of which religious figure did General Dupuy disclose Napoleon's reasons for religious tolerance?",
      "predicted_response": "Pope Pius VI",
      "method": "exact-match",
      "actual_response": "Pope Pius VI",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.5702000129967928e-05,
        "tree_retrieval": 5.427407619019505,
        "debate": 131.4609187260503,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.52346816600766,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0386707782745361
    },
    {
      "query": "What appears on the Start Screen?",
      "predicted_response": "a customizable array of tiles linking to various apps and desktop programs",
      "method": "exact-match",
      "actual_response": "a customizable array of tiles linking to various apps and desktop programs",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.730201231315732e-05,
        "tree_retrieval": 5.7185403849580325,
        "debate": 131.4609187260503,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.782239416032098,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9360402822494507
    },
    {
      "query": "What did Philo's opinions on the lack of instruments conflict with?",
      "predicted_response": "Jewish scriptures",
      "method": "exact-match",
      "actual_response": "Jewish scriptures",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.3400993086397648e-05,
        "tree_retrieval": 5.318917945027351,
        "debate": 131.4609187260503,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.3926073329057544,
      "bert_score": 1.0000004768371582,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0032769441604614
    },
    {
      "query": "When was A Study of High Definition Television Systems published?",
      "predicted_response": "1979",
      "method": "exact-match",
      "actual_response": "1979",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.460199175402522e-05,
        "tree_retrieval": 5.321255813993048,
        "debate": 131.4609187260503,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.385589166893624,
      "bert_score": 1.0000004768371582,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0146467685699463
    },
    {
      "query": "What was the intended equivalent value of one dollar?",
      "predicted_response": "a Spanish milled dollar",
      "method": "exact-match",
      "actual_response": "a Spanish milled dollar",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.980499554425478e-05,
        "tree_retrieval": 5.311895290971734,
        "debate": 131.4609187260503,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.399740542052314,
      "bert_score": 0.9999996423721313,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.049902081489563
    },
    {
      "query": "How much would disease incidence decrease with improved water and sanitation?",
      "predicted_response": "22\u201327% reduction in disease incidence",
      "method": "exact-match",
      "actual_response": "22\u201327% reduction in disease incidence",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.0602019503712654e-05,
        "tree_retrieval": 5.372979797946755,
        "debate": 131.4609187260503,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.435531875002198,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.016383171081543
    },
    {
      "query": "What qualifications might the Government mandate for a teacher to begin teaching?",
      "predicted_response": "certification by a recognized body",
      "method": "exact-match",
      "actual_response": "certification by a recognized body",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.660200698301196e-05,
        "tree_retrieval": 5.318173563980963,
        "debate": 131.4609187260503,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.386595583055168,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9950867295265198
    },
    {
      "query": "By whom was Foot replaced?",
      "predicted_response": "Neil Kinnock",
      "method": "exact-match",
      "actual_response": "Neil Kinnock",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.270098775625229e-05,
        "tree_retrieval": 5.338710371986963,
        "debate": 131.4609187260503,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.440608166973107,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9937120079994202
    },
    {
      "query": "What is the distance to Canelones' neighboring capital?",
      "predicted_response": "46 kilometres",
      "method": "exact-match",
      "actual_response": "46 kilometres",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.720195334404707e-05,
        "tree_retrieval": 5.912241524027195,
        "debate": 131.4609187260503,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.987884458038025,
      "bert_score": 0.9999997019767761,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0714490413665771
    },
    {
      "query": "Against what was Jefferson's Statute of Religious Freedom drafted?",
      "predicted_response": "a bill",
      "method": "exact-match",
      "actual_response": "a bill",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7102010790258646e-05,
        "tree_retrieval": 5.306770758994389,
        "debate": 131.4609187260503,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.384464541100897,
      "bert_score": 1.0000004768371582,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9971150755882263
    },
    {
      "query": "Who are the artists that influenced Madonna?",
      "predicted_response": "Frida Kahlo",
      "method": "exact-match",
      "actual_response": "Frida Kahlo",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7402013074606657e-05,
        "tree_retrieval": 5.394569490978029,
        "debate": 131.4609187260503,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.462765584001318,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0068020820617676
    },
    {
      "query": "Which sector has the smallest workforce?",
      "predicted_response": "Industry and manufacturing",
      "method": "exact-match",
      "actual_response": "Industry and manufacturing",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8103007934987545e-05,
        "tree_retrieval": 7.875990856962744,
        "debate": 131.4609187260503,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 7.946103332913481,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0345004796981812
    },
    {
      "query": "Which Gran Turismo game was unveiled in 2007 but released later?",
      "predicted_response": "Gran Turismo 5 Prologue",
      "method": "exact-match",
      "actual_response": "Gran Turismo 5 Prologue",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.350300878286362e-05,
        "tree_retrieval": 7.174888243025634,
        "debate": 131.4609187260503,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 7.24389083404094,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0450475215911865
    },
    {
      "query": "During which years was the civil war fought?",
      "predicted_response": "1992 to 1997",
      "method": "exact-match",
      "actual_response": "1992 to 1997",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.780403105542064e-05,
        "tree_retrieval": 7.773434592003468,
        "debate": 131.4609187260503,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 7.846172666992061,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.1336443424224854
    },
    {
      "query": "What color, besides lemon yellow, is found in uranium glass?",
      "predicted_response": "orange-red",
      "method": "exact-match",
      "actual_response": "orange-red",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.7304976508021355e-05,
        "tree_retrieval": 7.377582932007499,
        "debate": 131.4609187260503,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 7.442337165935896,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.077150583267212
    },
    {
      "query": "Which game, popular in both Britain and America, is gaining popularity in pubs?",
      "predicted_response": "pool",
      "method": "exact-match",
      "actual_response": "pool",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.560199936851859e-05,
        "tree_retrieval": 6.001930052996613,
        "debate": 131.4609187260503,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 6.162606375059113,
      "bert_score": 1.0000003576278687,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.1071033477783203
    },
    {
      "query": "Which location in Miami was considered for hosting Super Bowl 50?",
      "predicted_response": "Sun Life Stadium",
      "method": "exact-match",
      "actual_response": "Sun Life Stadium",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7702015358954668e-05,
        "tree_retrieval": 5.419925473979674,
        "debate": 131.4609187260503,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.503168834024109,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0283164978027344
    },
    {
      "query": "What year did John pass away?",
      "predicted_response": "18th October",
      "method": "debate",
      "actual_response": "18 October",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.600200241431594e-05,
        "tree_retrieval": 5.755050748004578,
        "debate": 49.94080258102622,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 55.80216133396607,
      "bert_score": 0.8634649515151978,
      "f1_score": 0.5,
      "rouge_score": 0.5,
      "bleurt_score": 0.9668329954147339
    },
    {
      "query": "What did Popper claim to be objective and independent of its subject?",
      "predicted_response": "Scientific theories are objective entities, independent of specific subjects according to Popper. He advocated falsificationism as a critical method for testing these theories, stating good theories must be capable of being tested and potentially proven false through empirical evidence. This approach prioritizes rigorous testing over theoretical content, allowing scientific theories to be evaluated consistently across domains. Popper's philosophy aims to establish a general criterion for evaluating the validity of scientific claims without relying on existing knowledge or specific domain theories, promoting innovation and progress in science.",
      "method": "debate",
      "actual_response": "knowledge",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.0001974664628506e-05,
        "tree_retrieval": 5.644854689016938,
        "debate": 134.24624681699788,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 139.96131766599137,
      "bert_score": 0.7884548902511597,
      "f1_score": 0.02380952380952381,
      "rouge_score": 0.022727272727272724,
      "bleurt_score": -1.436274528503418
    },
    {
      "query": "Who is recognized for the belief that humanity holds the utmost importance in the world?",
      "predicted_response": "Wu of Zhou",
      "method": "exact-match",
      "actual_response": "Wu of Zhou",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.700201002880931e-05,
        "tree_retrieval": 5.656848970975261,
        "debate": 134.24624681699788,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.75357525004074,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.032030463218689
    },
    {
      "query": "What does ECCE mean in the context of Swazi education?",
      "predicted_response": "early childhood care and education",
      "method": "exact-match",
      "actual_response": "early childhood care and education",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6702007744461298e-05,
        "tree_retrieval": 5.329742016969249,
        "debate": 134.24624681699788,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.435761375003494,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9979564547538757
    },
    {
      "query": "What became unpopular during this period?",
      "predicted_response": "1970s saw a decline in mainstream pop music styles like Motown and certain rock genres on Top 40 radio. This was influenced by new musical trends and changing tastes. Furthermore, technological advancements or shifting social norms could have contributed to this shift. For instance, older computer models might have been replaced by newer ones, while traditional fashion might have evolved towards brighter colors and more casual styles. Providing a specific time period would enable a more precise analysis.",
      "method": "debate",
      "actual_response": "Wood-panelling",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.2800988517701626e-05,
        "tree_retrieval": 5.568832814984489,
        "debate": 191.12157800997375,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 196.87097741605248,
      "bert_score": 0.7858381271362305,
      "f1_score": 0,
      "rouge_score": 0.0,
      "bleurt_score": -1.6083208322525024
    },
    {
      "query": "What type of art held significance in religious symbolism and decoration during the Ottoman Empire?",
      "predicted_response": "carpet weaving",
      "method": "exact-match",
      "actual_response": "carpet weaving",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7702015358954668e-05,
        "tree_retrieval": 6.0418436790350825,
        "debate": 191.12157800997375,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 6.110432249959558,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0284016132354736
    },
    {
      "query": "In what year was the International Chopin Piano Competition founded?",
      "predicted_response": "1927",
      "method": "exact-match",
      "actual_response": "1927",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.2002979423850775e-05,
        "tree_retrieval": 5.467670932004694,
        "debate": 191.12157800997375,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.543014124967158,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0347647666931152
    },
    {
      "query": "In what year did Seoul Semiconductor launch the first high DC voltage LED?",
      "predicted_response": "2009",
      "method": "exact-match",
      "actual_response": "2009",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 5.150400102138519e-05,
        "tree_retrieval": 5.477211580961011,
        "debate": 191.12157800997375,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.621332374983467,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0155655145645142
    },
    {
      "query": "What led to the formation of different Protestant branches?",
      "predicted_response": "how they have been influenced by important movements since the Reformation",
      "method": "exact-match",
      "actual_response": "how they have been influenced by important movements since the Reformation",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 6.350502371788025e-05,
        "tree_retrieval": 6.299954921007156,
        "debate": 191.12157800997375,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 6.4009482919937,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9463935494422913
    },
    {
      "query": "What is a typical background for a Japanese wrestler?",
      "predicted_response": "legitimate martial arts",
      "method": "exact-match",
      "actual_response": "legitimate martial arts",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.4801993276923895e-05,
        "tree_retrieval": 5.5564476500148885,
        "debate": 191.12157800997375,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.7271137909265235,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0676251649856567
    },
    {
      "query": "What enhancement did Germany make to the 20 mm to increase its effectiveness?",
      "predicted_response": "a 3.7 cm",
      "method": "exact-match",
      "actual_response": "a 3.7 cm",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.560199936851859e-05,
        "tree_retrieval": 5.66600008396199,
        "debate": 191.12157800997375,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.733967250096612,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.1379566192626953
    },
    {
      "query": "How many RIAA certifications has Beyonc\u00e9 received?",
      "predicted_response": "Beyonc\u00e9 has received 64 RIAA certifications.",
      "method": "debate",
      "actual_response": "64",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.700201002880931e-05,
        "tree_retrieval": 5.537674326042179,
        "debate": 52.26125342299929,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 57.93440158397425,
      "bert_score": 0.821014940738678,
      "f1_score": 0.2857142857142857,
      "rouge_score": 0.2857142857142857,
      "bleurt_score": -2.259448528289795
    },
    {
      "query": "How did Lothar Wolfgang Nordheim characterize von Neumann?",
      "predicted_response": "\"fastest mind I ever met\",",
      "method": "exact-match",
      "actual_response": "\"fastest mind I ever met\",",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6702007744461298e-05,
        "tree_retrieval": 6.196705122012645,
        "debate": 52.26125342299929,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 6.2920116250170395,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0187610387802124
    },
    {
      "query": "When was the importance of \"separation\" to the Constitution's Religion Clauses clearly stated?",
      "predicted_response": "1947",
      "method": "exact-match",
      "actual_response": "1947",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.6603032387793064e-05,
        "tree_retrieval": 5.483003242989071,
        "debate": 52.26125342299929,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.551178375026211,
      "bert_score": 1.0000003576278687,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9922916293144226
    },
    {
      "query": "How does bronchial thermoplasty impact patients?",
      "predicted_response": "it appears to decrease the subsequent rate.",
      "method": "exact-match",
      "actual_response": "it appears to decrease the subsequent rate.",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.1902996599674225e-05,
        "tree_retrieval": 5.448944105010014,
        "debate": 52.26125342299929,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.639709624927491,
      "bert_score": 0.9999997019767761,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9433581233024597
    },
    {
      "query": "Randomly selected individuals from various groups might share more similarities with each other than with members of their own what?",
      "predicted_response": "cluster",
      "method": "exact-match",
      "actual_response": "cluster",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.730201231315732e-05,
        "tree_retrieval": 5.368541579984594,
        "debate": 52.26125342299929,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.4337324580410495,
      "bert_score": 0.9999998211860657,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9808846116065979
    },
    {
      "query": "Who led the Puritans as their religious figure during their initial settlement?",
      "predicted_response": "William Brewster and Governor William Bradford were significant figures leading the Puritans in the Plymouth Colony. Although Bradford held a more prominent leadership role, serving as governor for over 36 years, his position evolved following their joint arrival in 1620. Brewster's involvement in organizing the colony also places him among primary figures during the initial settlement. Consequently, both leaders can be regarded as religious figures who guided the Puritans at Plymouth Colony.",
      "method": "debate",
      "actual_response": "Reverend John Davenport",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.100298974663019e-05,
        "tree_retrieval": 5.157512311008759,
        "debate": 158.67205578298308,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 163.97413424996193,
      "bert_score": 0.7781039476394653,
      "f1_score": 0,
      "rouge_score": 0.0,
      "bleurt_score": -1.0573740005493164
    },
    {
      "query": "What occurred with Sky's contract in 2006?",
      "predicted_response": "Sky's monopoly was broken from August 2006 when Setanta Sports was awarded rights to show two out of the six packages of matches available.",
      "method": "exact-match",
      "actual_response": "Sky's monopoly was broken from August 2006 when Setanta Sports was awarded rights to show two out of the six packages of matches available.",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.780195791274309e-05,
        "tree_retrieval": 5.647973062994424,
        "debate": 158.67205578298308,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.806356290937401,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9073612689971924
    },
    {
      "query": "What relates mass to space-time?",
      "predicted_response": "General relativity",
      "method": "exact-match",
      "actual_response": "General relativity",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.530296428129077e-05,
        "tree_retrieval": 5.448407432995737,
        "debate": 158.67205578298308,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.5129714999347925,
      "bert_score": 0.9999996423721313,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.018505334854126
    },
    {
      "query": "Which German coast did the RAF bomb?",
      "predicted_response": "Wilhelmshaven",
      "method": "exact-match",
      "actual_response": "Wilhelmshaven",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8702954296022654e-05,
        "tree_retrieval": 5.680870009004138,
        "debate": 158.67205578298308,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.759133459068835,
      "bert_score": 0.9999998211860657,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0349339246749878
    },
    {
      "query": "What are the three tiers of higher education in Estonia?",
      "predicted_response": "bachelor's, master's, and doctoral studies",
      "method": "exact-match",
      "actual_response": "bachelor's, master's, and doctoral studies",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.020304186269641e-05,
        "tree_retrieval": 5.276790203992277,
        "debate": 158.67205578298308,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.343390249996446,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9499539732933044
    },
    {
      "query": "What class does the Sao Paulo aircraft carrier belong to?",
      "predicted_response": "Clemenceau",
      "method": "exact-match",
      "actual_response": "Clemenceau",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.660396371036768e-05,
        "tree_retrieval": 5.420635064016096,
        "debate": 158.67205578298308,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.486465333029628,
      "bert_score": 0.9999998211860657,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.017496109008789
    },
    {
      "query": "What is another name for abstract expressionism?",
      "predicted_response": "the New York School",
      "method": "exact-match",
      "actual_response": "the New York School",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.3501983378082514e-05,
        "tree_retrieval": 5.361764932982624,
        "debate": 158.67205578298308,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.5070733750471845,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.004676103591919
    },
    {
      "query": "What was the impact of the civil war on Afghanistan's economy?",
      "predicted_response": "one of the poorest countries on earth",
      "method": "exact-match",
      "actual_response": "one of the poorest countries on earth",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.440301563590765e-05,
        "tree_retrieval": 5.350922921032179,
        "debate": 158.67205578298308,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.418597458046861,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9735245108604431
    },
    {
      "query": "Who were the two other renowned artists in the Super Bowl 50 halftime show?",
      "predicted_response": "Luke Bryan and Keith Urban also performed at the Super Bowl 50 halftime show along with Bruno Mars. They entertained the audience with a blend of country music and various musical styles.",
      "method": "debate",
      "actual_response": "Beyonc\u00e9 and Bruno Mars",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.500199479982257e-05,
        "tree_retrieval": 5.52691334602423,
        "debate": 91.92001184797846,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 97.53881670802366,
      "bert_score": 0.872065544128418,
      "f1_score": 0.18181818181818182,
      "rouge_score": 0.16666666666666666,
      "bleurt_score": -1.093446135520935
    },
    {
      "query": "Who was the top pick in the 1998 NFL draft?",
      "predicted_response": "Manning",
      "method": "exact-match",
      "actual_response": "Manning",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7202011551707983e-05,
        "tree_retrieval": 5.627238784043584,
        "debate": 91.92001184797846,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.76029020792339,
      "bert_score": 1.0000003576278687,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0194296836853027
    },
    {
      "query": "In which year was Tem\u00fcjin, later known as Genghis Khan, probably born?",
      "predicted_response": "1162",
      "method": "debate",
      "actual_response": "1162",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.5401997845619917e-05,
        "tree_retrieval": 5.603688500006683,
        "debate": 44.93488125799922,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 50.61027133394964,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0366435050964355
    },
    {
      "query": "What is the third type of content available at Home besides free and won?",
      "predicted_response": "premium",
      "method": "exact-match",
      "actual_response": "premium",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.3900996893644333e-05,
        "tree_retrieval": 5.562733509985264,
        "debate": 44.93488125799922,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.643381249974482,
      "bert_score": 0.9999997019767761,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.043946385383606
    },
    {
      "query": "Who held the Arena Football League championship title previously?",
      "predicted_response": "San Jose SaberCats",
      "method": "exact-match",
      "actual_response": "San Jose SaberCats",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6102003175765276e-05,
        "tree_retrieval": 5.280792351986747,
        "debate": 44.93488125799922,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.348394375061616,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9985144734382629
    },
    {
      "query": "To which civilization did the pottery belong?",
      "predicted_response": "Timucua",
      "method": "exact-match",
      "actual_response": "Timucua",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7202011551707983e-05,
        "tree_retrieval": 5.466978065029252,
        "debate": 44.93488125799922,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.53696004091762,
      "bert_score": 1.0000003576278687,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0183701515197754
    },
    {
      "query": "In which neighborhood is the Marriott Starr Pass Resort & Spa located?",
      "predicted_response": "The Marriott Starr Pass Resort & Spa lies in the Starr Pass neighborhood.",
      "method": "debate",
      "actual_response": "Starr Pass",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.5201006792485714e-05,
        "tree_retrieval": 5.309560260968283,
        "debate": 72.42879169801017,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 77.81651445792522,
      "bert_score": 0.8350159525871277,
      "f1_score": 0.33333333333333337,
      "rouge_score": 0.2857142857142857,
      "bleurt_score": -0.9181695580482483
    },
    {
      "query": "Besides obtaining money, what is another purpose of using bribes against others?",
      "predicted_response": "harm",
      "method": "exact-match",
      "actual_response": "harm",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.5201006792485714e-05,
        "tree_retrieval": 5.626502652012277,
        "debate": 72.42879169801017,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.694716792088002,
      "bert_score": 0.9999998211860657,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0262795686721802
    },
    {
      "query": "For how long was a king elected in the Roman Kingdom?",
      "predicted_response": "life term",
      "method": "exact-match",
      "actual_response": "life term",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6102003175765276e-05,
        "tree_retrieval": 5.4119532209588215,
        "debate": 72.42879169801017,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.477410790976137,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.030747413635254
    },
    {
      "query": "Who was the prominent political leader who criticized The Times in 1944 for not backing the British Army?",
      "predicted_response": "Winston Churchill",
      "method": "exact-match",
      "actual_response": "Winston Churchill",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.680195029824972e-05,
        "tree_retrieval": 5.9957614919985645,
        "debate": 72.42879169801017,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 6.093318165978417,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0072424411773682
    },
    {
      "query": "In what year did The Russian Revolution occur?",
      "predicted_response": "1917",
      "method": "exact-match",
      "actual_response": "1917",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.500199479982257e-05,
        "tree_retrieval": 5.28695402899757,
        "debate": 72.42879169801017,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.369479791959748,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0376330614089966
    },
    {
      "query": "What was the number of Chinese troops participating in the rescue efforts?",
      "predicted_response": "135,000",
      "method": "exact-match",
      "actual_response": "135,000",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.820196095854044e-05,
        "tree_retrieval": 5.286523104994558,
        "debate": 72.42879169801017,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.358741290983744,
      "bert_score": 1.0000004768371582,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.05194091796875
    },
    {
      "query": "What is the US's total area in miles, as stated by the Encyclopedia Britannica?",
      "predicted_response": "3,676,486 sq mi",
      "method": "exact-match",
      "actual_response": "3,676,486 sq mi",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.4401000700891018e-05,
        "tree_retrieval": 5.27844255900709,
        "debate": 72.42879169801017,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.35205683298409,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.055823802947998
    },
    {
      "query": "Where in France was Cluny Abbey established?",
      "predicted_response": "M\u00e2con",
      "method": "exact-match",
      "actual_response": "M\u00e2con",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.730201231315732e-05,
        "tree_retrieval": 5.3696985569549724,
        "debate": 72.42879169801017,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.436810582992621,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9606187343597412
    },
    {
      "query": "What did Josel request the city of Strasbourg to ban from being sold?",
      "predicted_response": "Luther's anti-Jewish works",
      "method": "exact-match",
      "actual_response": "Luther's anti-Jewish works",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.760201459750533e-05,
        "tree_retrieval": 5.369688201986719,
        "debate": 72.42879169801017,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.45248112489935,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0229628086090088
    },
    {
      "query": "Who moderated the initial debate between Bush and Kerry?",
      "predicted_response": "Jim Lehrer",
      "method": "exact-match",
      "actual_response": "Jim Lehrer",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.830201992765069e-05,
        "tree_retrieval": 5.920669021026697,
        "debate": 72.42879169801017,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 6.017493415973149,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.982391893863678
    },
    {
      "query": "What is the status of the Catalan Countries?",
      "predicted_response": "no official status",
      "method": "exact-match",
      "actual_response": "no official status",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6402005460113287e-05,
        "tree_retrieval": 5.8456729460158385,
        "debate": 72.42879169801017,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.912012250046246,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0641777515411377
    },
    {
      "query": "When did the Persian Gulf War hostilities start?",
      "predicted_response": "The Persian Gulf War started on January 17, 1991, initiating Operation Desert Storm. This event marked the beginning of the conflict later known as the Persian Gulf War.",
      "method": "debate",
      "actual_response": "January 1991",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.700201002880931e-05,
        "tree_retrieval": 5.428543303976767,
        "debate": 61.347743160964455,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 66.84310879092664,
      "bert_score": 0.8358446955680847,
      "f1_score": 0.15384615384615385,
      "rouge_score": 0.13333333333333333,
      "bleurt_score": -1.630543828010559
    },
    {
      "query": "What became the student body's objective?",
      "predicted_response": "winning the big game",
      "method": "exact-match",
      "actual_response": "winning the big game",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.1501968149095774e-05,
        "tree_retrieval": 5.62989739299519,
        "debate": 61.347743160964455,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.692151167080738,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0339573621749878
    },
    {
      "query": "In what year was the Mann Act enacted?",
      "predicted_response": "June 25, 1910",
      "method": "exact-match",
      "actual_response": "June 25, 1910",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.430198946967721e-05,
        "tree_retrieval": 5.314734696992673,
        "debate": 61.347743160964455,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.407163916039281,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0455974340438843
    },
    {
      "query": "What is the process for appointing judges?",
      "predicted_response": "elected by the voters",
      "method": "exact-match",
      "actual_response": "elected by the voters",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.100298974663019e-05,
        "tree_retrieval": 5.5166046529775485,
        "debate": 61.347743160964455,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.583831999916583,
      "bert_score": 1.0000003576278687,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9948026537895203
    },
    {
      "query": "Who informed Mary that she would give birth to the Messiah?",
      "predicted_response": "Gabriel",
      "method": "exact-match",
      "actual_response": "Gabriel",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7102010790258646e-05,
        "tree_retrieval": 5.402159954013769,
        "debate": 61.347743160964455,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.493495416012593,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.987234890460968
    },
    {
      "query": "Who established groups such as the FA?",
      "predicted_response": "Former public schoolboys",
      "method": "exact-match",
      "actual_response": "Former public schoolboys",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.270098775625229e-05,
        "tree_retrieval": 5.37276409601327,
        "debate": 61.347743160964455,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.498343875049613,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.026863694190979
    },
    {
      "query": "Which British king or queen is depicted above the frame surrounding the arches and entrance?",
      "predicted_response": "Queen Victoria",
      "method": "exact-match",
      "actual_response": "Queen Victoria",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.330300725996494e-05,
        "tree_retrieval": 5.680514501989819,
        "debate": 61.347743160964455,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.745393166085705,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9973084330558777
    },
    {
      "query": "What kind of philosopher was Hobbes?",
      "predicted_response": "empiricist",
      "method": "exact-match",
      "actual_response": "empiricist",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.5401997845619917e-05,
        "tree_retrieval": 5.249595718982164,
        "debate": 61.347743160964455,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.314780583023094,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0790537595748901
    },
    {
      "query": "What is the current number of artworks housed in the National Museum?",
      "predicted_response": "Refer to official museum records or announcements from the National Museum for an accurate count of their artworks. The number can change due to new acquisitions, donations, and exhibition alterations, so consult their latest updates for the current holdings.",
      "method": "debate",
      "actual_response": "200,000",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8402020689100027e-05,
        "tree_retrieval": 5.354689503030386,
        "debate": 94.84060287196189,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 100.27264916698914,
      "bert_score": 0.8087330460548401,
      "f1_score": 0,
      "rouge_score": 0.0,
      "bleurt_score": -1.6504403352737427
    },
    {
      "query": "Where is the Holiday Festival of Lights held?",
      "predicted_response": "James Island County Park",
      "method": "exact-match",
      "actual_response": "James Island County Park",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.570296732708812e-05,
        "tree_retrieval": 5.400127926026471,
        "debate": 94.84060287196189,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.467514249961823,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9901626706123352
    },
    {
      "query": "During which war was Buckingham Palace left undamaged?",
      "predicted_response": "World War I",
      "method": "exact-match",
      "actual_response": "World War I",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8902024496346712e-05,
        "tree_retrieval": 5.2580303120194,
        "debate": 94.84060287196189,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.327734417049214,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0292874574661255
    },
    {
      "query": "What was IBM's 2004 operating margin?",
      "predicted_response": "16.8%",
      "method": "exact-match",
      "actual_response": "16.8%",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6502006221562624e-05,
        "tree_retrieval": 5.336286831006873,
        "debate": 94.84060287196189,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.449464334058575,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0468753576278687
    },
    {
      "query": "Can you name a well-known restaurant in Little Havana?",
      "predicted_response": "Versailles",
      "method": "exact-match",
      "actual_response": "Versailles",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.730201231315732e-05,
        "tree_retrieval": 5.274577350995969,
        "debate": 94.84060287196189,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.365313249989413,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9721988439559937
    },
    {
      "query": "In which year did the Tenth Doctor feature in the Sarah Jane series?",
      "predicted_response": "2009",
      "method": "exact-match",
      "actual_response": "2009",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.900398198515177e-05,
        "tree_retrieval": 5.19690634397557,
        "debate": 94.84060287196189,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.26423087506555,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0155655145645142
    },
    {
      "query": "When did Blair declare his resignation?",
      "predicted_response": "September 2006",
      "method": "exact-match",
      "actual_response": "September 2006",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.3500993847846985e-05,
        "tree_retrieval": 5.671195096976589,
        "debate": 94.84060287196189,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.736730459029786,
      "bert_score": 0.9999996423721313,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0989651679992676
    },
    {
      "query": "Who became famous for his poor rendition of \"She Bangs\" during an audition?",
      "predicted_response": "William Hung gained notoriety for his subpar performance of \"She Bangs\" in an audition, a moment that received global attention and media coverage.",
      "method": "debate",
      "actual_response": "William Hung",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.3901055101305246e-05,
        "tree_retrieval": 5.192153288051486,
        "debate": 74.89989445899846,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 80.18539741705172,
      "bert_score": 0.8480132818222046,
      "f1_score": 0.17391304347826084,
      "rouge_score": 0.16,
      "bleurt_score": -1.8686368465423584
    },
    {
      "query": "What sustains the tree?",
      "predicted_response": "sapling-sized sprouts",
      "method": "exact-match",
      "actual_response": "sapling-sized sprouts",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.170299507677555e-05,
        "tree_retrieval": 5.809588045987766,
        "debate": 74.89989445899846,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.907720459043048,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.085878610610962
    },
    {
      "query": "What is the English translation of Zugunruhe?",
      "predicted_response": "migratory restlessness",
      "method": "exact-match",
      "actual_response": "migratory restlessness",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6402005460113287e-05,
        "tree_retrieval": 5.292180460004602,
        "debate": 74.89989445899846,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.372967040981166,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.1166541576385498
    },
    {
      "query": "What occurred following Kanye's controversial remark?",
      "predicted_response": "Rick Kaplan cut off the microphone and then cut away to Chris Tucker",
      "method": "exact-match",
      "actual_response": "Rick Kaplan cut off the microphone and then cut away to Chris Tucker",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.760201459750533e-05,
        "tree_retrieval": 5.156813312030863,
        "debate": 74.89989445899846,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.221551125054248,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0105632543563843
    },
    {
      "query": "When did the Kievan state divide into twelve distinct principalities?",
      "predicted_response": "12th century,",
      "method": "exact-match",
      "actual_response": "12th century,",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7102010790258646e-05,
        "tree_retrieval": 5.4206099509610794,
        "debate": 74.89989445899846,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.595943749998696,
      "bert_score": 0.9999997019767761,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0509536266326904
    },
    {
      "query": "What are Nigeria's religions apart from the two most prominent ones?",
      "predicted_response": "Approximately 10% of Nigeria's population follow traditional beliefs and animistic practices, constituting a significant minority. These local or indigenous religious groups play an essential role in the country's diverse religious landscape.",
      "method": "debate",
      "actual_response": "a tiny minority of adherents of Animism and other religions",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6202003937214613e-05,
        "tree_retrieval": 5.271344381966628,
        "debate": 69.9101563039585,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 75.28422187501565,
      "bert_score": 0.8677324056625366,
      "f1_score": 0.16216216216216214,
      "rouge_score": 0.0930232558139535,
      "bleurt_score": -0.1715109944343567
    },
    {
      "query": "What does the government of Saint Helena produce independently?",
      "predicted_response": "coinage and banknotes",
      "method": "exact-match",
      "actual_response": "coinage and banknotes",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.5902001652866602e-05,
        "tree_retrieval": 5.941581194987521,
        "debate": 69.9101563039585,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 6.011230458971113,
      "bert_score": 0.9999999403953552,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0163499116897583
    },
    {
      "query": "In which location can the lake in Atenco be found?",
      "predicted_response": "Federal District",
      "method": "exact-match",
      "actual_response": "Federal District",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.460199175402522e-05,
        "tree_retrieval": 5.231001734966412,
        "debate": 69.9101563039585,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.3294184589758515,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9974325299263
    },
    {
      "query": "To which other large agency's formation was the DHS likened?",
      "predicted_response": "the Department of Defense",
      "method": "exact-match",
      "actual_response": "the Department of Defense",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6802008505910635e-05,
        "tree_retrieval": 5.358263355970848,
        "debate": 69.9101563039585,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.447972833062522,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9312778115272522
    },
    {
      "query": "What is the function of the Sieve of Eratosthenes?",
      "predicted_response": "compute primes",
      "method": "exact-match",
      "actual_response": "compute primes",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.8802023734897375e-05,
        "tree_retrieval": 5.201703250000719,
        "debate": 69.9101563039585,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.299426499987021,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.106893539428711
    },
    {
      "query": "Who is the creator of Minecraft?",
      "predicted_response": "Markus \"Notch\" Persson",
      "method": "exact-match",
      "actual_response": "Markus \"Notch\" Persson",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.700201002880931e-05,
        "tree_retrieval": 5.226809834013693,
        "debate": 69.9101563039585,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.293293833034113,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.9886862635612488
    },
    {
      "query": "On whose likeness was the ship on wheels modeled?",
      "predicted_response": "Nerthus or Freyr",
      "method": "exact-match",
      "actual_response": "Nerthus or Freyr",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.130198456346989e-05,
        "tree_retrieval": 5.298361334018409,
        "debate": 69.9101563039585,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.369750416954048,
      "bert_score": 0.9999997615814209,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0328171253204346
    },
    {
      "query": "In which location do Eleonora's falcons nest?",
      "predicted_response": "Mediterranean islands",
      "method": "exact-match",
      "actual_response": "Mediterranean islands",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6802008505910635e-05,
        "tree_retrieval": 5.271722675010096,
        "debate": 69.9101563039585,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.36390491691418,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0075713396072388
    },
    {
      "query": "What features do certain dictionaries include?",
      "predicted_response": "sub-index",
      "method": "exact-match",
      "actual_response": "sub-index",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.630200469866395e-05,
        "tree_retrieval": 5.725977764988784,
        "debate": 69.9101563039585,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.7932295829523355,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0598304271697998
    },
    {
      "query": "How is designing defined in the context of built environments?",
      "predicted_response": "architectural lighting design",
      "method": "exact-match",
      "actual_response": "architectural lighting design",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.4101987946778536e-05,
        "tree_retrieval": 5.2374469980131835,
        "debate": 69.9101563039585,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.329919875017367,
      "bert_score": 1.0000003576278687,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.068164587020874
    },
    {
      "query": "What is the name of the LGBT newspaper?",
      "predicted_response": "Seattle Gay News",
      "method": "exact-match",
      "actual_response": "Seattle Gay News",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6102003175765276e-05,
        "tree_retrieval": 5.2592508280067705,
        "debate": 69.9101563039585,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.3527573340106755,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0995430946350098
    },
    {
      "query": "What changes have occurred in the local dialects of French-speaking areas?",
      "predicted_response": "have almost disappeared",
      "method": "exact-match",
      "actual_response": "have almost disappeared",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.670297494158149e-05,
        "tree_retrieval": 5.232681258989032,
        "debate": 69.9101563039585,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.3015789579367265,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0872348546981812
    },
    {
      "query": "What makes this specific rule contentious?",
      "predicted_response": "the Fish and Wildlife Service (FWS) loses much ability to further protect a species if the mitigation measures by the landowner prove insufficient.",
      "method": "exact-match",
      "actual_response": "the Fish and Wildlife Service (FWS) loses much ability to further protect a species if the mitigation measures by the landowner prove insufficient.",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 4.21029981225729e-05,
        "tree_retrieval": 5.434857036045287,
        "debate": 69.9101563039585,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.500503082992509,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.8755412697792053
    },
    {
      "query": "In which four years did England exit the UEFA European Championship at the group stage?",
      "predicted_response": "1980, 1988, 1992 and 2000",
      "method": "exact-match",
      "actual_response": "1980, 1988, 1992 and 2000",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.630200469866395e-05,
        "tree_retrieval": 5.230437477992382,
        "debate": 69.9101563039585,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.393843457917683,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0549662113189697
    },
    {
      "query": "Who was interred in the abbey on April 4, 1727?",
      "predicted_response": "Isaac Newton",
      "method": "exact-match",
      "actual_response": "Isaac Newton",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.7001951821148396e-05,
        "tree_retrieval": 5.35781680198852,
        "debate": 69.9101563039585,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.4269384579965845,
      "bert_score": 1.0000001192092896,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 0.989962637424469
    },
    {
      "query": "What vanished and was later reconstructed to play period music?",
      "predicted_response": "authentic instruments",
      "method": "exact-match",
      "actual_response": "authentic instruments",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 3.800197737291455e-05,
        "tree_retrieval": 5.726949509000406,
        "debate": 69.9101563039585,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.795537041965872,
      "bert_score": 1.0000007152557373,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0351018905639648
    },
    {
      "query": "Which is one of the most challenging derbies held in Egypt?",
      "predicted_response": "The Cairo Derby",
      "method": "exact-match",
      "actual_response": "The Cairo Derby",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.700201002880931e-05,
        "tree_retrieval": 5.389069694967475,
        "debate": 69.9101563039585,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.492714459076524,
      "bert_score": 1.000000238418579,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0223233699798584
    },
    {
      "query": "In which article by Donnelly and Embrechts did warnings and research on CDOs first appear?",
      "predicted_response": "2006",
      "method": "exact-match",
      "actual_response": "2006",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.680195029824972e-05,
        "tree_retrieval": 5.260540115996264,
        "debate": 69.9101563039585,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.330342291039415,
      "bert_score": 0.9999998807907104,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0723955631256104
    },
    {
      "query": "Who is the focal point of all mystical experiences?",
      "predicted_response": "Christ",
      "method": "exact-match",
      "actual_response": "Christ",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.6402005460113287e-05,
        "tree_retrieval": 5.3527095089666545,
        "debate": 69.9101563039585,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.446386291994713,
      "bert_score": 0.9999995827674866,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0628763437271118
    },
    {
      "query": "What are the components of gastric juice?",
      "predicted_response": "hydrochloric acid and pepsin",
      "method": "exact-match",
      "actual_response": "hydrochloric acid and pepsin",
      "source": "rajpurkar/squad",
      "latency": {
        "retrieval_table": 2.5802000891417265e-05,
        "tree_retrieval": 5.3138989270082675,
        "debate": 69.9101563039585,
        "global_llm": 1.320415276044514
      },
      "overall_latency": 5.407200834015384,
      "bert_score": 1.0,
      "f1_score": 1.0,
      "rouge_score": 1.0,
      "bleurt_score": 1.0152158737182617
    }
  ]
}